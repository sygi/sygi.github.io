<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"
    xmlns:dc="http://purl.org/dc/elements/1.1/">
    <channel>
        <title>Jakub Sygnowski blog</title>
        <link>https://sygnowski.ml</link>
        <description><![CDATA[Posts on games, programming, and art.]]></description>
        <atom:link href="https://sygnowski.ml/rss.xml" rel="self"
                   type="application/rss+xml" />
        <lastBuildDate>Wed, 23 Mar 2022 00:00:00 UT</lastBuildDate>
        <item>
    <title>Bézier curves and 3D modelling</title>
    <link>https://sygnowski.ml/posts/2022-03-23-bezier.html</link>
    <description><![CDATA[<div class="info">
    Posted on March 23, 2022
    
</div>
<h1>Bézier curves and 3D modelling</h1>
<p>To improve my 3d modeling skills in Blender, I made models for a set of chess pieces. Given their rotational symmetry, I constructed them as traces that Bézier curves leave when they are rotated around an axis.</p>
<h2 id="chess-pieces-and-rotational-symmetry">Chess pieces and rotational symmetry</h2>
<p>I chose to make the chess pieces despite not being a great chess fan. They seem to be of an appropriate difficulty level: posing a number of challenges, but well-defined and not too hard.</p>
<p>As the pieces are mostly rotationally symmetric (i.e. look the same when you rotate them by an angle around some axis), I planned to construct them as a curve that’s rotated around an axis. I hoped that repeating this workflow a couple of times with different pieces with an increasing amount of additional work will be a good learning experience.</p>
<p>I started by looking for how people create a rotationally symmetric mesh out of a curve. I found a number of tutorials (<a href="https://www.youtube.com/watch?v=a81YWJEy5Vk">1</a>, <a href="https://www.youtube.com/watch?v=gCt-RJY2pRg">2</a>, <a href="https://www.youtube.com/watch?v=-GdiOhqi_XY">3</a>), all of which did only a partial symmetry, i.e. rotated a given mesh by a fixed angle a couple of times.</p>
<p>While technically, as the final model has a fixed number of vertices, its mesh could be created by rotating something by a fixed angle and merging everything, it isn’t the most convenient way to construct it. For example, to change the number of vertices on the circle, I would need to both modify the number of repetitions and the rotation angle.</p>
<p>In the end, I found the <a href="https://docs.blender.org/manual/en/latest/modeling/modifiers/generate/screw.html">screw modifier</a>, which, when set up correctly, was accomplishing exactly what I wanted.</p>
<h2 id="bezier-curves">Bezier curves</h2>
<p>Once I knew how to rotate a curve to transform it into a surface, I had to learn how to create the curves in the first place.</p>
<p>I used a <a href="https://en.wikipedia.org/wiki/B%C3%A9zier_curve">Bézier curve</a> as a mathematical model for the curve. It is a popular way to express smooth curves in computer graphics.</p>
<h3 id="in-theory">In theory</h3>
<p>A Bezier curve consists of several segments. A single segment is constructed using a number of (at least two) control points. The first and the last control point lie at the ends of the curve, while the remaining ones describe the path the curve takes between them.</p>
<p>Let’s consider a quadratic Bezier curve, i.e. having three (one, apart from the starting and ending ones) control points: <span class="math inline">\(P_A\)</span>, <span class="math inline">\(P_B\)</span>, <span class="math inline">\(P_C\)</span>.</p>
<figure>
<img src="../images/bezier/bezier.png" style="width:90.0%" alt="Construction of a quadratic Bezier curve." />
<figcaption aria-hidden="true">Construction of a quadratic Bezier curve.</figcaption>
</figure>
<p>It is natural to express a Bezier curve using a function <span class="math inline">\(Bez(t)\)</span>, which maps <span class="math inline">\(t \in [0, 1]\)</span> to a point on the curve. It explains how the curve is continuously drawn, starting from the first control point at <span class="math inline">\(Bez(0) = P_A\)</span> and ending at the last one for <span class="math inline">\(Bez(1) = P_B\)</span>.</p>
<p>The intermediate points are defined as follows:</p>
<ol type="1">
<li>Let’s take a point <span class="math inline">\(P_{AB}\)</span>, which lies after <span class="math inline">\(t\)</span> portion of the way from <span class="math inline">\(P_A\)</span> to <span class="math inline">\(P_B\)</span>, i.e. <span class="math inline">\(P_{AB} = tP_A + (1-t)P_B\)</span>.</li>
<li>Analogously, define <span class="math inline">\(P_{BC}\)</span> on between <span class="math inline">\(P_B\)</span> and <span class="math inline">\(P_C\)</span> as: <span class="math inline">\(P_{BC} = tP_B + (1-t)P_C\)</span>.</li>
<li>Let the point on the curve, <span class="math inline">\(Bez(t)\)</span> be after <span class="math inline">\(t\)</span> portion of way between <span class="math inline">\(P_{AB}\)</span> and <span class="math inline">\(P_{BC}\)</span>, i.e. <span class="math inline">\(Bez(t) = tP_{AB} + (1-t)P_{BC}\)</span>.</li>
</ol>
<p>If one substituted the definitions of the intermediate points <span class="math inline">\(P_{AB}\)</span> and <span class="math inline">\(P_{BC}\)</span>, it would become clear that the Bezier curve with three control points is indeed a quadratic polynomial, explaining its name.</p>
<p>The most popular Bezier curves are the cubic ones, that is having four control points. They are constructed iteratively, by first placing points <span class="math inline">\(P_{AB}\)</span>, <span class="math inline">\(P_{BC}\)</span>, and <span class="math inline">\(P_{CD}\)</span> on the corresponding segments, and then repeating the construction above with the new three points as the control points.</p>
<figure>
<img src="../images/bezier/cubic_bezier.png" style="width:80.0%" alt="Construction of the cubic Bezier curve." />
<figcaption aria-hidden="true">Construction of the cubic Bezier curve.</figcaption>
</figure>
<p>For a deeper (and more interactive) introduction to Bezier curves, I recommend <a href="https://ciechanow.ski/curves-and-surfaces/">this article</a> by Bartosz Ciechanowski, from where the visualizations above are taken.</p>
<h3 id="in-practice">In practice</h3>
<p>The mathematical formulas above give little intuition on how to place the control points to get a given curve.</p>
<p>To grow this intuition, I played a <a href="https://bezier.method.ac">bezier curve game</a>. One is asked there to create a Bezier curve with as few points as possible to draw more and more complex shapes.</p>
<p>Once back in Blender, after a recommendation <a href="https://www.youtube.com/watch?v=YI8vXzbtehg">from a video</a>, I tried using an add-on for drawing Bezier curves. As I didn’t like its interface, I went back to <a href="https://docs.blender.org/manual/en/2.79/modeling/curves/introduction.html">the default curve-drawing tool</a>.</p>
<h2 id="process">Process</h2>
<p>As a reference for modeling the chess pieces, I used this photo of a standard chess set used at tournaments:</p>
<figure>
<img src="../images/bezier/JaquesCookStaunton.jpg" style="width:70.0%" alt="Photo made by Frank A. Camaratta, Jr.; The House of Staunton, Inc." />
<figcaption aria-hidden="true">Photo made by Frank A. Camaratta, Jr.; The House of Staunton, Inc.</figcaption>
</figure>
<p>The process for designing various pieces was largely similar:</p>
<ol type="1">
<li>I drew the outline of the piece as a Bezier curve. While doing so, <code>ctrl-RMB</code> was a useful shortcut for placing new points and <code>vf</code> for making non-smooth connections.</li>
<li>I increased the width of the line to make it more visible at <a href="https://docs.blender.org/manual/en/latest/modeling/curves/properties/geometry.html#bevel"><code>Object properties &gt; bevel &gt; depth</code></a>.</li>
<li>I placed an empty on the future rotational axis and parented it to the curve to make moving around easier.</li>
<li>I used the screw modifier around the empty to create the surface.</li>
<li>By choosing <code>overlays &gt; face orientation</code> I checked whether the normals are pointing outwards (blue), and chose <code>normals &gt; flip</code> if not.</li>
<li>I chose <code>Object &gt; convert &gt; mesh</code>, as I couldn’t use <a href="https://docs.blender.org/manual/en/latest/modeling/modifiers/generate/booleans.html">boolean modifiers</a> otherwise.</li>
<li>I set the bevel depth back to zero</li>
<li>I cut all the necessary pieces with the boolean modifiers.</li>
<li>In the end, I checked the geometry for duplicate nodes to merge and applied a subdivision modifier for a more smooth look.</li>
</ol>
<h3 id="pawn">Pawn</h3>
<p>I started with the simplest piece to model, a pawn. While making it was just the matter of drawing the curve and rotating it, I was also establishing the process I mentioned above.</p>
<figure>
<a href="../images/bezier/pawn.png"><img src="../images/bezier/pawn-small.png" style="width:40.0%" alt="pawn model" /></a>
<figcaption>
Final model of the pawn.
</figcaption>
</figure>
<h3 id="bishop">Bishop</h3>
<p>The next piece I made was a bishop. After doing the rotationally symmetric part, I needed to cut out the hole using a boolean modifier.</p>
<p>Initially, I struggled a little with that, as having the normals the wrong way made the boolean modifier go crazy.</p>
<figure>
<a href="../images/bezier/bishop.png"><img src="../images/bezier/bishop-small.png" style="width:40.0%" alt="bishop model" /></a>
<figcaption>
Final model of the bishop.
</figcaption>
</figure>
<p>In the final model, you can see edges around the hole that have been smoothed too much. This is an effect of applying the subdivision modifier too early. As I have learned too late, it should be used at the very end, and one should mark the edges which should stay sharp as <a href="https://docs.blender.org/manual/en/latest/modeling/modifiers/generate/booleans.html">creases</a>.</p>
<h3 id="rook">Rook</h3>
<p>When I got to the rook, I could use the knowledge from the “partial symmetry with a fixed angle” tutorials I mentioned above: it was easy to make the “holes in the wall”<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>.</p>
<figure>
<a href="../images/bezier/rook.png"><img src="../images/bezier/rook-small.png" style="width:40.0%" alt="rook model" /></a>
<figcaption>
Final model of the rook.
</figcaption>
</figure>
<h4 id="object-offset-in-array-modifier">Object offset in Array modifier</h4>
<p>One can use the <a href="https://docs.blender.org/manual/en/latest/modeling/modifiers/generate/array.html#object-offset">array modifier</a> to do the partial symmetry.
To know how to use it with the object offset setting, one needs to understand two types of transformations: in the object mode and in the edit mode.</p>
<p>In the object mode, we are moving/rotating/scaling objects as a whole; every change is reflected in the origin of the object. The transformations are represented (for the whole object) in the <code>Transform</code> panel (and can be later applied to bake in the changes).</p>
<p>In the edit mode, the transformations are performed at the level of vertices, not objects. While it is possible to perform the same transformations as in the object mode, the <code>Transform</code> panel only shows the effect of the location transformation, as, at the vertex level, rotation and scale don’t make sense.</p>
<p>As the transformations are only applied to the particular vertices
(and not the whole object), the origin of the object stays put when vertices are edited<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>.</p>
<p>The transformation that is repeatedly applied to the copied object in the array modifier is the difference between the (object-level) transformation of the offset object and the one being copied.</p>
<p>So, to perform the partial symmetry with the hole in the wall and the empty describing the rotation axis, their transformations need to satisfy:</p>
<ol type="1">
<li>location (of the origin) should be the same, as there shouldn’t be any translation between consecutive copies,</li>
<li>scale should be the same (ideally: 1) too,</li>
<li>rotation needs to differ by 360 degrees/number of holes (60 degrees, in my case).</li>
</ol>
<p>On one hand, I wanted the rotation axis to be outside of the box which represents the hole, to avoid intersecting different holes. On the other, the location of the box needs to be the same as the axis for the consecutive boxes to not move relative to each other.</p>
<p>This is the point where I used the two transformation modes: I kept the box <em>object</em> in the same location as the axis but moved (in edit mode) all the boxes’ <em>vertices</em> away from the axis, to avoid intersections.</p>
<figure>
<a href="../images/bezier/rook-holes.png"><img src="../images/bezier/rook-holes.png" style="width:50.0%" alt="rook with wall holes" /></a>
<figcaption>
The rook model with the boxes representing the holes in the wall to be cut out.
</figcaption>
</figure>
<h3 id="queen">Queen</h3>
<p>The queen was more complex than the previous pieces but didn’t involve any new method.</p>
<figure>
<a href="../images/bezier/queen.png"><img src="../images/bezier/queen-small.png" style="width:40.0%" alt="queen model" /></a>
<figcaption>
Final model of the queen.
</figcaption>
</figure>
<h3 id="king">King</h3>
<p>For the king, the novel part was the cross on top of its “crown”.</p>
<p>To make it, I drew a separate Bezier curve for its outline, but instead of rotating it, I only mirrored it across the Z-axis.</p>
<p>Then, I converted the curve to a mesh, filled it, and extruded it in the Y direction.</p>
<figure>
<a href="../images/bezier/king.png"><img src="../images/bezier/king-small.png" style="width:40.0%" alt="king model" /></a>
<figcaption>
Final model of the king.
</figcaption>
</figure>
<h3 id="knight">Knight</h3>
<p>To make the knight, I needed to follow a completely different process. As it isn’t possible to create it through unions and differences of simple shapes, I borrowed a drawing tablet from my girlfriend and decided to give sculpting a go.</p>
<figure>
<a href="../images/bezier/knight1.png"><img src="../images/bezier/knight1-small.png" style="width:40.0%" alt="first knight model" /></a>
<a href="../images/bezier/knight2.png"><img src="../images/bezier/knight2-small.png" style="width:40.0%" alt="second knight model" /></a>
<figcaption>
First two models of the knight.
</figcaption>
</figure>
<p>As the first attempt, as you see, was not very successful, I went through a follow-along tutorial on youtube before trying again. The second model was visibly better but still left much to be desired.</p>
<h4 id="new-knight-reference">New knight reference</h4>
<p>As trying to reproduce that original Stanton knight proved to be prohibitively difficult, I decided to make a different, simpler knight.</p>
<figure>
<img src="../images/bezier/knight-ref.jpg" style="width:30.0%" alt="New, simpler, knight reference." />
<figcaption aria-hidden="true">New, simpler, knight reference.</figcaption>
</figure>
<p>I started with making a shape that would smoothly transition from a circle (bottom) to a square (top) with <a href="https://docs.blender.org/manual/en/latest/modeling/meshes/editing/edge/bridge_edge_loops.html">bridge edge loops</a>.</p>
<p>Then, I squished the top to a thin rectangle, while making a <a href="https://docs.blender.org/manual/en/latest/editors/3dview/controls/proportional_editing.html">proportional edit</a><a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> in a non-uniform way, to give the knight the smooth shape of something to be picked by the fingers.</p>
<p>I then proceeded to draw the outline of the knight with a Bezier curve. Similarly to the cross of the king’s crown, I filled and extruded the outline and intersected this horse-like shape with the pawn structure.</p>
<p>Given the number of sharp edges that the final piece has, I made heavy use of creases to avoid them being smoothed out while subdividing.</p>
<p>I then proceeded to cut the holes in the side of the knight. For the eye, I used yet another Bezier curve (now with a bigger bevel) and a boolean modifier.</p>
<p>For the barely-visible cuts marking the horse’s mane, I wanted the curves to taper at their ends. There is <a href="https://docs.blender.org/manual/en/latest/modeling/curves/properties/geometry.html">a setting</a> for that, which requires passing another object which controls the amount of tapering.</p>
<p>After some trial and error, I established that one needs to draw a graph describing the width of the taper as a taper object. To do that, it was convenient to visualize the local axis of the taper curve by choosing <code>object properties &gt; viewport display &gt; axis</code>.</p>
<figure>
<a href="../images/bezier/knight3.png"><img src="../images/bezier/knight3-small.png" style="width:50.0%" alt="final knight model" /></a>
<figcaption>
Final model of the knight.
</figcaption>
</figure>
<h2 id="board">Board</h2>
<p>As a final element to the scene, I made a board to place the pieces on.</p>
<p>I generated the checkerboard pattern with <a href="https://docs.blender.org/manual/en/latest/render/shader_nodes/textures/checker.html">checker texture</a>. To apply it to a part of the model, I made a UV map covering the inner part of the board and applied this texture only there using <a href="https://docs.blender.org/manual/en/latest/render/shader_nodes/input/texture_coordinate.html">Texture Coordinate node</a>.</p>
<h2 id="final-render">Final render</h2>
<figure>
<a href="../images/bezier/full.png"><img src="../images/bezier/full-small.png" style="width:80.0%" alt="final full render" /></a>
</figure>
<section class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>Which, as I’ve learned now, are called <code>crenel</code>s.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>Of course, it can later be reset to the center of geometry.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p>not a great name here<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
]]></description>
    <pubDate>Wed, 23 Mar 2022 00:00:00 UT</pubDate>
    <guid>https://sygnowski.ml/posts/2022-03-23-bezier.html</guid>
    <dc:creator>Jakub Sygnowski</dc:creator>
</item>
<item>
    <title>Historia jednej paczki</title>
    <link>https://sygnowski.ml/posts/2022-03-13-customs.html</link>
    <description><![CDATA[<div class="info">
    Posted on March 13, 2022
    
</div>
<h1>Historia jednej paczki</h1>
<p>Here goes a story of a package with private items that I sent before moving from Switzerland to Poland. One may be surprised that this mundane topic is granted a blogpost, but the story is filled with absurdities. As I am afraid they would not be understandable if I tried to express the drama in English, this post is exceptionally written in Polish.</p>
<p>Kiedy wyjechałem kilka lat temu z Polski, normalnym było dla mnie, że sprawy w instytucjach państwowych są zaczynane dopiero po wyczerpaniu drogi reklamacyjnej, gdy nie ma już żadnych sposobów na uniknięcie zajęcia się sprawą. Prowadziło to do śmiesznych (lub nie) sytuacji, na przykład gdy po złożeniu skargi brytyjski urząd podatkowy pytał, czy 40 funtów (i wystawienie nowego dokumentu) jest wystarczającą rekompensatą za literówkę w moim imieniu w jakimś dokumencie.</p>
<figure>
<a href="../images/customs/hmrc_cheque.jpg"><img src="../images/customs/hmrc_cheque.jpg" style="width:80.0%" alt="Czek od HMRC" /></a>
<figcaption>
Czek z rekompensatą od HMRC.
</figcaption>
</figure>
<p>Przed niedawną przeprowadzką ze Szwajcarii wysłaliśmy część rzeczy do Polski pocztą, i nawet nie pomyślałem zawczasu, że paczka będzie przetwarzana przez Pocztę Polską. A nawet gdybym pomyślał, to pewnie bym uznał, że to najwyżej będzie iść trochę dłużej. Rzeczywistość jednak przerosła moją wyobraźnię.</p>
<p>Po 3 dniach podróży paczki wewnątrz Szwajcarii, ślad po niej zaginął. Kilka tygodni później poprosiłem szwajcarską pocztę o zlokalizowanie paczki. Po ich interwencji, paczka pojawiła się w systemie do monitoringu w Warszawie. Wiadomo: dopóki nikt się nie upomni, nie ma co przekazywać paczki, a nuż odbiorca zapomni?</p>
<p>Dzień później paczka przeniosła się do Działu Obsługi Zagranicznej (dalej: DOZ) w Lublinie, a Poczta przysłała zawiadomienie wnioskujące o podanie informacji na temat zawartości paczki, żeby móc złożyć zeznanie celne.</p>
<figure>
<img src="../images/customs/map_ann.png" style="width:80.0%" alt="Nie wnikajmy, czemu paczka została przeniesiona do Lublina skoro docelowy adres jest bliżej Warszawy, w mazowieckim." />
<figcaption aria-hidden="true">Nie wnikajmy, czemu paczka została przeniesiona do Lublina skoro docelowy adres jest bliżej Warszawy, w mazowieckim.</figcaption>
</figure>
<h2 id="zawiadomienie">Zawiadomienie</h2>
<p>Pomińmy kwestię tego, że te informacje zostały podane przy wysyłce paczki w Szwajcarii (Poczta Polska łaskawie dołączyła załączony do paczki list przewozowy): wiadomo, pracownik poczty wypełniający deklaracje celne nie musi umieć przetłumaczyć nazw towarów na język polski.</p>
<p>Zawiadomienie zawiera informację, że “niedopełnienie [w ciągu 14 dni] formalności skutkuje odmową przyjęcia wniosku zgodnie z art. 12 rozporządzenia (UE) 2015/2447 oraz udzielenie Poczcie Polskiej zgody na zwrot przesyłki do nadawcy”. O ile można pominąć fakt, że wspomniany artykuł nie wspomina nic o jakimkolwiek zwracaniu przesyłek, to ciekawe jest, że nikt nie zastanowił się, że w przypadku tej paczki nadawca ma ten sam adres, co adres docelowy.</p>
<figure>
<a href="../images/customs/nadawca_odbiorca.png"><img src="../images/customs/nadawca_odbiorca.png" style="width:75.0%" alt="Adresy w liście przewozowym" /></a>
<figcaption>
Adres nadawcy i odbiorcy na liście przewozowym.
</figcaption>
</figure>
<p>Postanowiłem nie testować, czy w przypadku braku informacji z mojej strony paczka zostanie przesłana bez dalszych opłat (normalnie jest opłata 8.50zł za złożenie deklaracji celnej<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>); spodziewałem się, że wypełnię oświadczenie, poczta złoży deklarację celną i dostarczy paczkę za kilka dni. Jak można się domyślić z długości posta, tak się jednak nie stało.</p>
<p>Zawiadomienie od poczty zawiera listę sytuacji, w których poczta nie złoży deklaracji celnej, i odbiorca musi to zrobić “we własnym zakresie” bądź skorzystać z agencji celnej. Część z tych sytuacji, np. gdy wartość towaru przekracza 1000 euro wynika z przepisów, a część (np. że jest to towar powracający, tzn. używany wcześniej w Polsce), to bezpodstawna twórczość pracowników poczty.</p>
<p>Jako, że w moim przypadku żadna z nich nie miała miejsca, nie spodziewałem się problemów: wypełniłem oświadczenie i czekałem na nadejście przesyłki.</p>
<p>Nie dostałem żadnej informacji zwrotnej z poczty w ciągu kilku dni po wypełnieniu oświadczenia. Próbowałem dzwonić na załączone (dwa) numery telefonów: jednego nikt nie odbierał, a na drugim włączała się poczta głosowa.</p>
<p>W kontakcie z centralą poczty dostałem informację, że poczta dokłada wszelkich starań, żeby szybko składać te deklaracje i że opóźnienia na pewno są z winy Urzędu Celnego. Kilka minut później DOZ odpisał na moje oświadczenie sprzed tygodnia: wiadomo, nie ma co przetwarzać oświadczenia, dopóki odbiorca się nie upomni.</p>
<h2 id="pierwsza-odmowa">Pierwsza odmowa</h2>
<p>Niestety, DOZ odmówił złożenia deklaracji na podstawie nowo-wymyślonej zasady: poczta nie składa deklaracji celnych w przypadku mienia przesiedleńczego. Mienie przesiedleńcze to towary, których osoba używała w obcym kraju, i chce je przywieźć do Polski, bo przeprowadza się tutaj na stałe. W przypadku mienia przesiedleńczego Urząd Celny aplikuje specjalne zwolnienie z cła.</p>
<p>O ile zgodnie z prawem poczta ma obowiązek wypełniać deklaracje celne z mieniem przesiedleńczym w imieniu odbiorców, to byłoby zrozumiałe, że poczta mogłaby nie chcieć aplikować o jakieś specjalne zwolnienia. W moim przypadku nie chciałem z niego korzystać, głównie dlatego, że tak czy inaczej wszystkie<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> towary ze Szwajcarii są zwolnione z cła, podobnie jak towary o małej wartości, jakie moja paczka zawierała.</p>
<p>Żeby było śmieszniej, nawet gdybym chciał, to nie mogę zgodnie z prawem skorzystać z tej ulgi, bo przysługuje ona tylko dla osób, które przenoszą się do Polski na stałe<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>, co nie było prawdą w moim przypadku.</p>
<h2 id="druga-odmowa">Druga odmowa</h2>
<p>Odpisałem więc DOZ, że paczka nie zawiera mienia przesiedleńczego, i że mogą spokojnie składać deklarację.</p>
<p>Po tygodniu poczta odpisała, że może to nie jest mienie przesiedleńcze, ale jako że jest ten sam nadawca i odbiorca, to oni traktują to jako “mienie osobiste” i “zgodnie z przepisami” poczta nie może odprawiać tego typu paczek.</p>
<p>Jednocześnie w mailu pracownik poczty dołączył link do <a href="https://www.poczta-polska.pl/hermes/uploads/2021/06/Zasady-wykonywania-przez-PP-zg%C5%82osze%C5%84-celnych-w-imporcie-2021-08-06-final.pdf">zasad składania deklaracji celnych</a>, z których w żaden sposób nie wynika, że poczta nie składa deklaracji rzeczy osobistych. Po zwróceniu uwagi, że zasady mówią co innego, i zapytaniu w ogóle o jakie przepisy chodzi, pani z poczty uznała, że nie warto już ze mną dyskutować i przestała odpisywać.</p>
<h2 id="reklamacja">Reklamacja</h2>
<p>W międzyczasie złożyłem reklamację. Jako instytucja publiczna, Poczta Polska ma proces reklamacyjny zdefiniowany ustawą<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>, z 30-ma dniami na odpowiedź, i obowiązkiem podania dalszej drogi reklamacyjnej, podobnie jak banki.</p>
<p>Poczta realizuje obowiązek odbierana reklamacji przez <a href="https://ereklamacje.poczta-polska.pl">stronę</a>. Strona, bardzo sprytnie, umożliwia zarówno złożenie reklamacji, jak i skargi, przy której (jak mniemam) ustawowe wymagania odpowiedzi na reklamację nie obowiązują.</p>
<p>Dodatkowo, przy składaniu reklamacji, aby załączyć skan maila od poczty, musiałem złożyć oświadczenie, że “wszystkie załączone dokumenty są prawdziwe i nie zawierają wirusów”. Mam nadzieję, że nie okaże się, że pdf emaila nie spełnia założenia “prawdziwości” dokumentu.</p>
<figure>
<img src="../images/customs/oswiadczenie_reklamacja.png" style="width:100.0%" alt="Oświadczenie, które trzeba kliknąć przed wysłaniem reklamacji do Poczty Polskiej." />
<figcaption aria-hidden="true">Oświadczenie, które trzeba kliknąć przed wysłaniem reklamacji do Poczty Polskiej.</figcaption>
</figure>
<p>Odpowiedź na reklamację otrzymałem już po 27 dniach. W odpowiedzi (która sugeruje, że złożyłem “skargę” a nie “reklamację”) pracownik podsumowuje zasady składania deklaracji celnych i stwierdza:</p>
<blockquote>
<p>Niezależnie od powyższego, za wszelkie utrudnienia i negatywne odczucia związane ze zgłoszoną sprawą uprzejmie przepraszamy</p>
</blockquote>
<p>zupełnie nie odnosząc się do tego, dlaczego poczta nie złożyła deklaracji, skoro zasady, które wspomniał mówią, że powinna.</p>
<p>W odpowiedzi brak jest jakiejkolwiek metody kontaktu: jak odpowiedź mi nie odpowiada, mogę przecież złożyć drugą reklamację i dostać kolejną odpowiedź (za kolejne 25+ dni). To też zrobiłem, ale jednocześnie zacząłem próbować odzyskać paczkę innymi kanałami.</p>
<h2 id="kontakt-ze-szwajcarską-pocztą">Kontakt ze Szwajcarską pocztą</h2>
<p>W międzyczasie Szwajcarzy napisali, że znaleźli paczkę i że czeka na zwolenienie przez celników na poczcie. Pewnie się zdziwili, jak im odpisałem, że wygląda na to, że poczta nie złożyła (i nie zamierza) jeszcze deklaracji celnej, bo przesyłka ma tego samego nadawcę, co odbiorcę.</p>
<p>Z tego, co pisali, próbowali potem ustalić z Pocztą Polską o co w ogóle chodzi, ale poczta im też przestała odpisywać.</p>
<p>Jako że nie bardzo byli w stanie cokolwiek pomóc z paczkę, zaproponowali 70SFr. odszkodowania, cytując:</p>
<blockquote>
<p>We hope that this compensation will meet with your satisfaction. We want satisfied customers and hope that we can regain your trust with our courtesy.</p>
</blockquote>
<h2 id="kis-i-puesc">KIS i PUESC</h2>
<p>Jako że poczta nie chciała złożyć deklaracji, zadzwoniłem do <a href="https://www.kis.gov.pl/zalatwianie-spraw/udzielanie-informacji">Krajowej Informacji Skarbowej</a>, aby dowiedzieć się, w jaki sposób można złożyć deklarację “we własnym zakresie”, jak sugerowała poczta.</p>
<p>Po drugiej stronie była nawet miła pani, która głównie przekonywała mnie, że to poczta powinna złożyć deklarację (tak, jakbym tego nie wiedział) i że tak będzie najprościej, bo oni składają uproszczony formularz, a normalni ludzie muszą wypełniać kody towarów i w ogóle jest to bardzo skomplikowane.</p>
<p>Po poszkaniu online, jak składać deklaracje celne, trafiłem na rządową platformę <a href="https://puesc.gov.pl">PUESC</a>, która ma służyć do wszelkich rzeczy związanych z cłem. Wygląda na to, że platforma ma <a href="https://www-2.puesc.gov.pl/web/puesc/strona-glowna">starą wersję</a>, która wspiera składanie deklaracji celnych (ta funkcjonalność nie została jeszcze zmigrowana).</p>
<p>Po załóżeniu konta na platformie, trafiłem do sekcji składania formularzy, ale jedynym, co dało się tam wysłać to pliki XML. Nie było dla mnie jasne w jaki sposób taki plik skonstruować.</p>
<p>Po zapytaniu o to na help desku PUESCu dowiedziałem się, że platforma zakłada, że użytkownik ma oprogramowanie, które generuje poprawne XMLe. Szukałem chwilę darmowego oprogramowania online do tego, ale znalazłem tylko płatne, z cenami rzędu 1000zł/stanowisko. Tego typu programów używają agencje celne.</p>
<h2 id="papierowy-sad">Papierowy SAD</h2>
<p>Przeglądając zasady składania wniosków o <a href="https://www.podatki.gov.pl/clo/informacje-dla-przedsiebiorcow/procedury-celne/procedura-dopuszczenia-do-obrotu/">dopuszczenie towarów do obrotu</a>, trafiłem na informację, że deklarację można złożyć nie tylko elektronicznie, ale także na papierowym <a href="https://ec.europa.eu/taxation_customs/single-administrative-document-sad_en">formularzu SAD</a>.</p>
<p>Formularz zawiera te same informacje, co pełne deklaracje celne składane przez importerów/agencje celne, więc <a href="https://www-2.puesc.gov.pl/documents/10180/111775799/Instrukcja+wype%C5%82niania+zg%C5%82osze%C5%84%20celnych+AIS_IMPORT+AES_ECS2+NCTS2+wersja+2.3.pdf/b0c80b60-bfad-430d-8883-54027e3465dd">instrukcja</a>, jak go wypełnić ma kilkaset stron (plus lista z 12 tysiącami pozycji z kodami towarów).</p>
<p>Na początku wydawało się, że wypełnienie tego formularza będzie bardzo trudne, ale, czytając instrukcje pole po polu, było tylko kilka takich, w których miałem wątpliwości. Stwierdziłem, że bardziej mi się opłaca wypełnić je jakkolwiek i poprawić po dostaniu odmowy z urzędu niż próbować dochodzić, czym różni się “kontyngent taryfowy” od “plafonu taryfowego”.</p>
<p>Główna trudność (poza przypominaniem sobie jakie dokładnie towary znajdują się w paczce i ile ważą) formularza znajdowała się w części wspólnej dla wszystkich towarów: przy wymienianianiu towarów większość danych się powtarzała, a pozostałe były proste do wypełnienia (waga, kod towaru, wartość, kraj pochodzenia).</p>
<p>Wypełniony formularz SAD zapisałem jako pdf i wysłałem za pomocą ePUAP jako załącznik do wniosku do Urzędu Celnego w Białej Podlaskiej (która to zarządza oddziałem w Lublinie, gdzie znajdowała się paczka).</p>
<h2 id="komunikat-zc215">Komunikat ZC215</h2>
<p>W międzyczasie, na stronie PUESC, trafiłem na <a href="https://www-2.puesc.gov.pl/documents/10180/111161805/xsd+-+AIS-IMP.zip/773564a6-16dd-477f-bf70-4eaf66d72958">dokumentację</a> w formacie <a href="https://en.wikipedia.org/wiki/XML_Schema_(W3C)">XML Schema</a>, opisującą różne wnioski (komunikaty), które można wysyłać przez PUESC. Po zrozumieniu, w jaki sposób opisują one komunikaty do wysłania i który komunikat (Zgłoszczenie Celne: dopuszczenie do obrotu, o kodzie ZC215) chcę złożyć, pomyślałem, że mógłbym wczytać plik XSD ze schemą, wygenerować pusty komunikat i powypełniać pola ręcznie.</p>
<p>Na szczęście, nie musiałem tego robić, bo w międzyczasie znalazłem <a href="https://www-2.puesc.gov.pl/documents/10180/111161805/AIS_IMP_SXML_PL_w_3_14.pdf/84481207-b5c9-4904-9689-8f14020c55a3">specyfikację XML</a> opisującą które pola są obowiązkowe w których komunikatach i co one znaczą w języku naturalnym.</p>
<figure>
<img src="../images/customs/itemizowanie.png" style="width:35.0%" alt="Sposób opisu listy parametów w oficjalnej specyfikacji XML." />
<figcaption aria-hidden="true">Sposób opisu listy parametów w oficjalnej specyfikacji XML.</figcaption>
</figure>
<p>Pliki XSD wciąż się przydały, bo pozwalają na automatyczną walidację, czy komunikat zgadza się ze schemą. Pythonowa biblioteka <a href="https://pypi.org/project/xmlschema/">xmlschema</a> robiła to bardzo wygodnie, pokazując, których elementów i atrybutów gdzie brakuje.</p>
<h2 id="reguły">Reguły</h2>
<p>Kiedy komunikat był już—jak się spodziewałem—skończony, trzeba było go podpisać przed złożeniem w PUESC.</p>
<p>Niestety, stary portal wspiera tylko podpis fizycznym podpisem kwalifikowanym (którego nie mam), albo “certyfikatem celnym”, do uzyskania którego, wyglądało że trzeba fizycznie pójść do urzędu (co, z punktu widzenia bezpieczeństwa, miałoby jakiś sens).</p>
<p>Postanowiłem spróbować podpisać dokument korzystając z zewnętrznego, rządowego <a href="https://www.gov.pl/web/gov/podpisz-dokument-elektronicznie-wykorzystaj-podpis-zaufany">portalu</a> do podpisywania dowolnych dokumentów. Korzystając z tej strony byłem w stanie łatwo podpisać komunikat uwierzytelniając się przez ePUAP.</p>
<p>Po wysłaniu tak podpisanego komunikatu, dostałem jednak informację zwrotną z odrzuceniem komunikatu. Informacja (sformatowana w postaci XMLa, którego czytało się trochę niewygodnie) zawierała zestaw reguł, których mój komunikat nie spełniał.</p>
<p>Większość z nich była zdefiniowana w formie założeń typu “jeśli pole X występuje i nie jest = 42, wtedy pole Y też musi występować”, choć niektóre z nich po prostu opisywały fakt, że jakieś pole jest obowiązkowe (co dałoby się wyrazić za pomocą scheme’y).</p>
<p>Tak, czy inaczej, automatyczne generowanie informacji zwrotnej po kilku minutach po kilku minutach było bardzo wygodne, bo pozwalało mi szybko zweryfikować i poprawiać komunikat.</p>
<p>Niestety, jedna z błędnych reguł informowała, że format podpisu jest nieprawidłowy: wygląda na to, że podpis z rządowej strony do podpisywania dokumentów nie jest zgodny z wymaganiami PUESC.</p>
<h2 id="podpis-elektroniczny">Podpis elektroniczny</h2>
<p>Zauważyłem, że w nowym serwisie PUESC pojawiają się komunikaty, które wysyłałem przez stary. Dodatkowo, nowy PUESC pozwala na podpisanie komunikatów korzystając z ePUAPU jako trzecia opcja (oprócz podpisu kwalifikowanego i certyfikatu celnego). Niestety, nowy PUESC nie pozwala na wysyłanie komunikatów celnych (które są na razie wspierane tylko przez stary), a aby “wysłać dokument” w starym EPUESC (co spowoduje jego migrację do nowego), musi on być już podpisany. Nie można więc przesłać komunikatu ze starego do nowego PUESCu po to, aby go podpisać i potem wysłać starym systemem.</p>
<p>Na szczęście, okazało się, że generowanie certyfikatów celnych, które w starym PUESC wymagało pójścia do urzędu, w przypadku nowego PUESCu da się zrobić przez stronę. Tu znowu miejsce na duży plus dla rządowych systemów informatycznych, bo program pozwalający na wygenerowanie certyfikatu: <a href="https://puesc.gov.pl/pki/resource/Instrukcja_CertSign.pdf">CertSign</a> da się uruchomić nawet na linuxie (wygląda, że jest napisany jako jakiś applet javowy). Z drugiej strony nie do końca rozumiem w jaki sposób działa uwierzytelnienie potrzebne do wygenenerowania certyfikatu, skoro konto w PUESC zakładałem (jeśli dobrze pamiętam) tylko na email, i w żadnym momencie nie byłem poproszony o zautoryzowanie przez ePUAP.</p>
<p>W każdym razie, po rozwiązaniu drobnych problemów wymagających dodania certyfikatów publicznych rządowych stron w przeglądarce, byłem w stanie podpisywać XMLe korzystając z własnego certyfikatu celnego.</p>
<h2 id="rozmowa-z-urzędem-celnym">Rozmowa z Urzędem Celnym</h2>
<p>Odpowiedź z odrzuceniem wniosku dostałem już po kilkudziesięciu minutach, co było szokiem, bo musiał na to zgłoszenie spojrzeć urzędnik, a co więcej komunikat wysłałem ok. godziny 17 w sobotę.</p>
<p>Co więcej, zwrotka zawierała numer kontaktowy! Telefon odebrała stosunkowo miła pani, która, po upewnieniu się, że jestem pełnoletni (xD)<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>, zaczęła wymieniać, co było źle we wniosku, który złożyłem.</p>
<p>Pani twierdziła, że tam jest dużo błędów, i że prościej mi będzie skorzystać z agencji celnej, ale jednak była bardzo pomocna i wymieniła wszystko, co chciała, żebym zmienił we wniosku. Wbrew temu, co mówiła, były to dosyć proste rzeczy do poprawienia, typu: w tym polu chcą jeszcze PESEL (mimo, że XML schema tego nie wymagała), a w polu “miejsce dostawy” chcą tylko miejscowość, a nie “26-400 Przysucha, Polska”.</p>
<p>Po około 30-to minutowej rozmowie, pani stwierdziła, żebym jeszcze skontaktował się z Panem Zbyszkiem<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>, który zajmuje się moją paczką i który próbował napisać do mnie maila.</p>
<p>Po stwierdzeniu, że nie dostałem wspomnianego maila, pani przełączyła mnie na telefon z nim.</p>
<h2 id="pan-zbyszek">Pan Zbyszek</h2>
<p>Podczas rozmowy Pan Zbyszek okazał się być super (jeszcze bardziej niż poprzednia pani, która też była całkiem ok).</p>
<p>Stwierdził on (a rozmowa miała miejsce jakąś godzinę po tym, jak wysłałem formularz przez PUESC, w sobotę wieczorem), że zdążył już porozmawiać z ludźmi na poczcie, i że oni się zgodzili złożyć tę deklarację, tylko żebym im napisał oświadczenie, że w paczce nie ma mienia przesiedleńczego. I że on wie, że ja już im to pisałem, i że właściwie to oni powinni tę deklarację złożyć tak, czy inaczej, ale jak mu napiszę to oświadczenie i wyślę w poniedziałek (bo nie pracuje w niedzielę), to on pójdzie na pocztę i wszystko z nimi załatwi.</p>
<p>Oczywiście bardzo mu podziękowałem, ale też zapytałem, jak to jest, że oni są tacy spoko w tym urzędzie, a na poczcie już od dwóch miesięcy próbuję tę paczkę odzyskać i nic nie da się załatwić. Odpowiedział, że nie ma sprawy i, że im też się nie chce, cytując, pierdzielić z odrzucaniem i poprawianiem tych formularzy, kiedy poczta może złożyć dużo prostszą deklarację, a wiadomo, że to nie są jakieś towary o dużej wartości, na które im by się w ogóle chciało patrzeć.</p>
<p>Dodatkowo, Pan Zbyszek był ciekawy jak udało mi się wysłać ten komunikat przez PUESC, bo mówił, że to był pierwszy raz, kiedy widział, żeby go wypełniła osoba fizyczna, a nie firma/agencja celna ^^.</p>
<h2 id="koniec-historii">Koniec historii</h2>
<p>Po tym, jak Pan Zbyszek “ogarnął sprawę” w poniedziałek wieczorem, w paczka została dostarczona do Przysuchy w środę.</p>
<p>Dostałem później (tym razem po 22 dniach) jeszcze odpowiedź na drugą reklamację, w której urzędnik PP stwierdza:</p>
<blockquote>
<p>Z uwagi na wprowadzone na terenie całej Unii Europejskiej nowe regulacje (…), czas potrzebny na obsługę przesyłek z krajów spoza UE uległ wydłużeniu z przyczyn niezależnych od Poczty Polskiej S.A.</p>
</blockquote>
<p>Jak można się domyślać, postępowanie poczty nie przysparza im zbyt dobrych ocen, w szczególności na Google Maps. Na koniec chcę się podzielić jedną z niewielu pozytywnych opinii, które DOZ w Lublinie tam dostał:</p>
<figure>
<img src="../images/customs/komentarz_pp.png" style="width:60.0%" alt="Komentarz do opinii o poczcie." />
<figcaption aria-hidden="true">Komentarz do opinii o poczcie.</figcaption>
</figure>
<section class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>która, jak później się dowiedziałem, łamie postanowienia Międzynarodowej Unii Pocztowej, konkretnie art. 20.4 <a href="https://www.upu.int/UPU/media/upu/files/UPU/aboutUpu/acts/actsOfCurrentCycle/actsActsOfThe26ThCongressIstanbul2016En.pdf">Decyzji z międzynarodowego kongresu pocztowego ze Stanbułu</a><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>z jakimiś specjalnymi wyjątkami<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p>patrz: art. 47 <a href="http://isap.sejm.gov.pl/isap.nsf/download.xsp/WDU20040540535/U/D20040535Lj.pdf">ustawy z dnia 11 marca 2004 r. o podatku od towarów i usług</a><a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4" role="doc-endnote"><p>Art. 92 <a href="http://isap.sejm.gov.pl/isap.nsf/download.xsp/WDU20120001529/U/D20121529Lj.pdf">Dz.U.2020.1041</a><a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5" role="doc-endnote"><p>pozwoliłem sobie nie skomentować, że zmuszanie dzieci do wypełniania tych formularzy byłoby znęcaniem się nad nieletnimi<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6" role="doc-endnote"><p>imię zmienione, nie wiem, czy chciałby zostać boheterem tej historii<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
]]></description>
    <pubDate>Sun, 13 Mar 2022 00:00:00 UT</pubDate>
    <guid>https://sygnowski.ml/posts/2022-03-13-customs.html</guid>
    <dc:creator>Jakub Sygnowski</dc:creator>
</item>
<item>
    <title>Physically-based rendering: materials</title>
    <link>https://sygnowski.ml/posts/2021-12-01-pbr-materials.html</link>
    <description><![CDATA[<div class="info">
    Posted on December  1, 2021
    
</div>
<h1>Physically-based rendering: materials</h1>
<p>3D scenes consist of objects, each of which has a shape and a material. The shape is defined through a mesh: a collection of vertices, edges, and faces living in a 3D world. The material describes the color of the object and how it reflects and transmits light. In this post, I inspect a popular way of expressing the material: a principled BSDF to see how its numerous parameters influence the look of an object.</p>
<h2 id="principled-bsdf">Principled BSDF</h2>
<p>As a quick reminder from <a href="../posts/2021-11-28-pbr-101.html">the previous post</a>, the way the light bounces of objects is described as a function called BSDF to a computer.</p>
<p>That representation is not natural for people: it’s based on the interpretation of light as a multi-dimensional probability density function. To make it easier for artists to define the materials, Disney <a href="https://static1.squarespace.com/static/58586fa5ebbd1a60e7d76d3e/t/593a3afa46c3c4a376d779f6/1496988449807/s2012_pbs_disney_brdf_notes_v2.pdf">came up with a shader</a><a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> called Principled BSDF whose inputs are physically-based properties like “roughness” or “metallic” which get translated to a BSDF.</p>
<figure>
<img src="https://docs.blender.org/manual/en/latest/_images/render_shader-nodes_shader_principled_node.png" style="width:35.0%" alt="A node containing all the settings to describe a material using Principled BSDF in Blender." />
<figcaption aria-hidden="true">A node containing all the settings to describe a material using Principled BSDF in Blender.</figcaption>
</figure>
<p>Principled BSDF has become the industry standard for describing materials. Despite its principle to be as intuitive as possible, it has a lot of parameters one can change, whose meaning is not always obvious, making using the shader overwhelming at first.</p>
<p>Below, I summarize my understanding of various properties the shader has.</p>
<h2 id="base-color">Base color</h2>
<p>Color is the most basic property a material can have.</p>
<p>The standard BSDF formalism doesn’t capture the color of objects. To do so, we need to attach an extra property to the light ray containing the color of the light (described as a 3-element RGB vector). Whenever the ray hits an object, its color coordinates get multiplied by the RBG coordinates of the object, decreasing the intensity of the corresponding channels.</p>
<p>Let’s consider the ray going in the direction from the sun to the camera:</p>
<ol type="1">
<li>The ray starts perfectly white, with color represented as <code>(1., 1., 1.)</code></li>
<li>The ray hits a perfectly red object, i.e. having color (1., 0., 0.). The color of the ray changes to <code>(1., 0., 0.)</code>.</li>
<li>The ray goes further and hits a grey object of color <code>(0.5, 0.5, 0.5)</code>. The ray’s color gets multiplied to <code>(0.5, 0., 0.)</code>, which corresponds to dark red.</li>
<li>The ray hits the camera where it’s averaged with other incoming rays, creating an (RGB) pixel.</li>
</ol>
<figure>
<img src="../images/pbr_materials/color.png" style="width:70.0%" alt="Diagram showing a change in the light ray color when being reflected from colorful surfaces." />
<figcaption aria-hidden="true">Diagram showing a change in the light ray color when being reflected from colorful surfaces.</figcaption>
</figure>
<h2 id="specular-vs-diffuse-reflections">Specular vs diffuse reflections</h2>
<p>The specular reflection model assumes the light rays are reflected under the same angle to the normal as the incoming light<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>.</p>
<p>There is another model of reflection, which assumes that the incident light gets scattered in all directions with equal probability, called <a href="https://en.wikipedia.org/wiki/Diffuse_reflection">diffuse reflection</a>.</p>
<p>The physical mechanism behind the diffuse reflection is like this:</p>
<ol type="1">
<li>The light ray crosses the surface of the material.</li>
<li>Inside, the material is mostly empty, except for a small number of particles.</li>
<li>When the ray hits a particle, it gets reflected in a random direction.</li>
</ol>
<figure>
<img src="../images/pbr_materials/diffuse.gif" style="width:70.0%" alt="Comparison of specular and diffuse reflection by GianniG46 (link)." />
<figcaption aria-hidden="true">Comparison of specular and diffuse reflection by GianniG46 (<a href="https://commons.wikimedia.org/wiki/File:Lambert2.gif">link</a>).</figcaption>
</figure>
<p>Principled BSDF handles these two reflection models by first calculating the light intensity corresponding to each of them separately, and then simply summing them up.</p>
<h3 id="subsurface-scattering">Subsurface scattering</h3>
<p>The behavior of light rays reflecting from the material particles is called <em>subsurface scattering</em>. Apart from being reflected in a random direction, the light rays change color during scattering. As the internals of the material may have a different color than the surface (e.g., skin is white-pink and the internal parts of a body are red), the color of the reflected ray may depend on how deep it entered material.</p>
<figure>
<img src="../images/pbr_materials/subsurface_res.jpg" style="width:40.0%" alt="Subsurface scattering in the human hand. Scaled-down photo by Davepoo2014." />
<figcaption aria-hidden="true">Subsurface scattering in the human hand. Scaled-down <a href="https://commons.wikimedia.org/wiki/File:Skin_Subsurface_Scattering.jpg">photo by Davepoo2014</a>.</figcaption>
</figure>
<p>The Principled BSDF in Blender has three parameters for controlling the subsurface scattering:</p>
<ol type="1">
<li>Subsurface, controlling overall strength of subsurface scattering. It is a multiplier to the radius.</li>
<li>Subsurface color, defining the color of the material internals.</li>
<li>Subsurface radius, defining the average depth the light rays enter the material. It is provided as a 3-dimensional vector: one for RGB colors, to allow materials to, e.g., only allow the red color to enter.</li>
</ol>
<h2 id="metallic">Metallic</h2>
<p>There are two types of materials: <a href="https://en.wikipedia.org/wiki/Electrical_conductor">conductors</a> (often called metallic by the artistic community) and <a href="https://en.wikipedia.org/wiki/Dielectric">dielectrics</a>.</p>
<p>For conductors, the light enters the material to a much smaller depth than for dielectrics. Visually it manifests itself by:</p>
<ol type="1">
<li>Conductors not permitting passing any light through the material. In other words, only dielectrics are transmissive.</li>
<li>Both types of materials cause specular reflection (occurring at the surface of the material), but only dielectrics induce a diffuse one (needing to enter the material to a certain depth).</li>
<li>The specular reflection itself is of the same color as the incoming light in the case of dielectrics but tinted by the color of the material for conductors.</li>
</ol>
<p>Physically, materials are either conductors or dielectrics (not anything in between). The parameter in the shader allows for in-between values, which correspond to non-photorealistic blends between the two.</p>
<p>As its alternative name suggests, conductors are basically only metals, whereas dielectrics are all other real-life materials.</p>
<figure>
<img src="../images/pbr_materials/metallic.png" style="width:40.0%" alt="Render of a metallic material" />
<img src="../images/pbr_materials/dielectric.png" style="width:40.0%" alt="Render of a dielectric material" />
<figcaption>
Conductor (left) vs dielectric (right). Dielectric has a bigger proportion of diffuse reflection (flat pink color). The specular reflection is tinted towards pink in case of metallic and white for the dielectric.
</figcaption>
</figure>
<h2 id="transmission">Transmission</h2>
<p>For dielectrics, a part of the light shining on the surface is reflected in a specular way, and a part of it enters the material.</p>
<p>For the light that enters the material, a part of it randomly hits the material particles and gets reflected in a diffuse reflection, and the rest passes through the material as transmitted light.</p>
<p><em>Transmission</em> parameter controls the proportion of light that leaves the object on the other side. Value of 1 means that there is no diffuse reflection: the specular parameter decides the strength of the specular reflection, and all the remaining light passes through the object.</p>
<p>The color of the light ray is still multiplied by the base color of the object, allowing the creation of materials like colored glass, which transmit some colors but absorb others.</p>
<figure>
<img src="../images/pbr_materials/transmission.png" style="width:40.0%" alt="Fully transmissive (glass-like) material, with a red base color." />
<figcaption aria-hidden="true">Fully transmissive (glass-like) material, with a red base color.</figcaption>
</figure>
<h2 id="index-of-refraction">Index of refraction</h2>
<p>Light travels at a different speed in different mediums. It’s fastest in a vacuum, with a speed of nearly 300.000 km/s. The ratio between the speed of light in a vacuum to the speed of light in a given material is called its <em>index of refraction</em> (IOR). For example, the IOR of glass is 1.33, meaning the light is slower in glass by around 1/3.</p>
<p>The wave-like nature of light can explain an effect called refraction, where the direction of light changes when moving between two mediums with different indices of refraction.</p>
<figure>
<img src="../images/pbr_materials/snells_law.png" style="width:35.0%" alt="Illustration of Snell’s law, which states that the relation between angles of refraction \theta and the indices of refraction n satisfies n_1\sin \theta_1 = n_2 \sin \theta_2." />
<figcaption aria-hidden="true">Illustration of Snell’s law, which states that the relation between angles of refraction <span class="math inline">\(\theta\)</span> and the indices of refraction <span class="math inline">\(n\)</span> satisfies <span class="math inline">\(n_1\sin \theta_1 = n_2 \sin \theta_2\)</span>.</figcaption>
</figure>
<p>The IOR parameter found in the Principled BSDF controls the refraction angle, allowing the artist to match materials with a known index of refraction.</p>
<h2 id="specular">Specular</h2>
<p>Fresnel reflectance defines that part of the light that is reflected off the surface in a specular way when the light shines on it from the normal direction (i.e., perpendicularly to the surface) for a dielectric.</p>
<p>Physically, it <a href="https://en.wikipedia.org/wiki/Fresnel_equations#Power_(intensity)_reflection_and_transmission_coefficients">can be calculated from IOR</a> of the material<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>.</p>
<p><span class="math display">\[f_0 = \frac{(IOR - 1)^2}{(IOR+1)^2}\]</span></p>
<p>As the resulting values would be between 0 and 8% for most of the materials (although there are dielectrics with higher specular reflection, for example, a diamond specularly reflects 17% of the incoming light), Fresnel reflectance is expressed through a normalized specular parameter as:</p>
<p><span class="math display">\[S = f_0/0.08\]</span></p>
<p>On that scale, a typical dielectric with <span class="math inline">\(f_0 = 0.04\)</span> will have a specular parameter of <span class="math inline">\(0.5\)</span>.</p>
<p>The Fresnel reflectance <span class="math inline">\(f_0\)</span> is also used to calculate the Fresnel effect that causes a higher reflection at grazing (far from the normal) angles</p>
<figure>
<img src="../images/pbr_materials/no_fresnel.png" style="width:40.0%" alt="Render of a material with no Fresnel effect" />
<img src="../images/pbr_materials/fresnel.png" style="width:40.0%" alt="Render of a material with Fresnel effect" />
<figcaption>
Illustration of a fresnel effect. Notice faint, light highlights at the borders of the sphere on the right (with specular &gt;0).
</figcaption>
</figure>
<p>The split between specular and diffuse reflection applies only to the dielectrics. For conductors, all reflection happens in a specular way, so controlling the specular parameter doesn’t make physical sense<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>.</p>
<h3 id="specular-tint">Specular tint</h3>
<p>As we said before, the specular reflection for dielectrics doesn’t change the color of the light ray. To achieve an artistic (non-physically accurate) effect of specular reflection tinting the light towards the base color of the material, Principled BSDF offers the <em>specular tint</em> parameter.</p>
<h2 id="microfacet-model-and-roughness">Microfacet model and roughness</h2>
<p>One can divide the reflected light can into:</p>
<ul>
<li>specular reflection, reflecting the light in the direction under the same angle to the normal as the incident light, and</li>
<li>diffuse reflection, reflecting the light in all directions, present only for dielectrics.</li>
</ul>
<p>The surface of the materials is rarely perfectly flat: it often has some general surface and a lot of microscopic variations to it, called microfacets. When a ray of light shines onto a surface with these bumps, it often hits an area whose normal is not the same as the global normal of the surface.</p>
<figure>
<img src="https://google.github.io/filament/images/diagram_macrosurface.png" style="width:80.0%" alt="A figure (from Filament documentation) explaining the idea of microfacets." />
<figcaption aria-hidden="true">A figure (from <a href="https://google.github.io/filament/Filament.html#figure_microfacets">Filament documentation</a>) explaining the idea of microfacets.</figcaption>
</figure>
<p>Because of that, the light reflected through a specular reflection doesn’t take a single direction but rather a range of angles around it.</p>
<p>Instead of modeling the actual microscopic structure of the material (which would be tedious and computationally inefficient), the renderers are using statistical models<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> to decide in which direction the rays will (specularly) reflect.</p>
<p>The shader has a parameter called <em>roughness</em>, controlling how wide the distribution of reflected rays will be. Materials with lower roughness will appear shinier as more highlights will appear on their surface.</p>
<figure>
<img src="https://google.github.io/filament/images/diagram_roughness.png" style="width:95.0%" alt="Illustration of the roughness parameter: higher roughness (left), corresponding to a more bumpy surface, causes a wider range of directions of reflected rays. Comes from Filament docs." />
<figcaption aria-hidden="true">Illustration of the roughness parameter: higher roughness (left), corresponding to a more bumpy surface, causes a wider range of directions of reflected rays. Comes from <a href="https://google.github.io/filament/Filament.html#figure_roughness">Filament docs</a>.</figcaption>
</figure>
<h2 id="anisotropic">Anisotropic</h2>
<p>For typical materials, the variation in the surface is random, and the specular reflection, on average, goes in the same direction as if the object was flat. High roughness can increase the variance of reflection but doesn’t change the mean direction.</p>
<p>For some types of materials, the assumption about the randomness of microfacets is not correct. For example, a surface made of sanded metal, seen on the bottom of a pan, has a regular pattern that systematically changes reflections.</p>
<p>To allow accounting for this effect, Principled BSDF has a parameter called <em>anisotropy</em> that modifies the direction of the reflections. The <em>tangent</em> setting controls the angles of the reflections, and anisotropy the strength of the effect.</p>
<figure>
<img src="../images/pbr_materials/isotropic.png" style="width:40.0%" alt="Render of an isotropic material" />
<img src="../images/pbr_materials/anisotropic.png" style="width:40.0%" alt="Render of an anisotropic material" />
<figcaption>
Increasing anisotropy (right) allows changing the direction of the specular reflection.
</figcaption>
</figure>
<h2 id="clearcoat">Clearcoat</h2>
<p>The microfacet model assumes that there is a single parameter: roughness describing the “bumpiness” of the material’s surface, which doesn’t change with the depth of the material.</p>
<p>It makes it hard to capture materials that by themselves are rough but are covered with a coat of shiny paint, like lacquered wood or colorful aluminum cans.</p>
<p>To simplify defining such materials, there is a <em>clearcoat</em> parameter that adds a thin layer of colorless paint, with the roughness that we can control separately, on top of the given material.</p>
<figure>
<img src="../images/pbr_materials/no_clearcoat.png" style="width:40.0%" alt="Render of material without clearcoat" />
<img src="../images/pbr_materials/clearcoat.png" style="width:40.0%" alt="Render of a material with clearcoatl" />
<figcaption>
A can with a high clearcoat (right) has a more shiny look to it.
</figcaption>
</figure>
<h2 id="sheen">Sheen</h2>
<p>Another material property that creates a layer on top of the regular material is <em>sheen</em>. It is used to describe an effect causing materials like fleece or velvet to be slightly lighter near the borders.</p>
<h2 id="principled-bsdf-in-blender">Principled BSDF in Blender</h2>
<p>I looked at how different shader parameters influence the final look of the materials in Blender. It was intimidating at first, but in the end, only 3-4 of them (roughness, metallic, specular, transmission) express most of the variability in the materials.</p>
<p>While I looked specifically at the Blender (Cycles) implementation of the Principled BSDF, other renderers use the same terminology, e.g. <a href="https://mitsuba2.readthedocs.io/en/latest/generated/plugins.html#bsdfs">Mitsuba</a>, or <a href="https://google.github.io/filament/Materials.html">Filament</a>,
so the understanding of these parameters is useful independent of the actual render used.</p>
<section class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>a program for drawing pixels on a screen<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>See section <a href="../posts/2021-11-28-pbr-101.html#specular-reflection-model">Specular reflection model</a> in the previous post.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p>However, it looks like changing this parameter in Blender does influence the level of Fresnel effect applied.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4" role="doc-endnote"><p>However, it looks like changing this parameter in Blender does influence the level of Fresnel effect applied.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5" role="doc-endnote"><p>Blender allows to choose between two versions of <a href="https://www.cs.cornell.edu/~srm/publications/EGSR07-btdf.pdf">GGX</a>.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
]]></description>
    <pubDate>Wed, 01 Dec 2021 00:00:00 UT</pubDate>
    <guid>https://sygnowski.ml/posts/2021-12-01-pbr-materials.html</guid>
    <dc:creator>Jakub Sygnowski</dc:creator>
</item>
<item>
    <title>Physically-based rendering 101</title>
    <link>https://sygnowski.ml/posts/2021-11-28-pbr-101.html</link>
    <description><![CDATA[<div class="info">
    Posted on November 28, 2021
    
</div>
<h1>Physically-based rendering 101</h1>
<p>After watching one of the <a href="https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg">Two minute papers</a> videos: a short video summary of a research paper made by ‪Károly Zsolnai-Fehér‬, YouTube algorithm suggested I should watch <a href="https://users.cg.tuwien.ac.at/zsolnai/gfx/rendering-course/">the rendering course</a> of the same author next. My physically-based rendering knowledge is very fragmented: I was immersed in the lingo without understanding it through friends who are doing PhDs in related areas, and I tried and failed to read <a href="https://pbr-book.org">PBRT</a>. I watched Károly’s course to connect the dots and, hopefully, to understand Blender better.</p>
<h2 id="ray-tracing">Ray tracing</h2>
<p>At the heart of physically-based rendering is the ray tracing algorithm. It describes how rays of light travel through a 3D scene:</p>
<ol type="1">
<li>light starts at a light source,</li>
<li>it goes in a straight line to objects, and bounces off them, until</li>
<li>it hits the camera, in front of which there is a canvas on which the image will appear.</li>
</ol>
<figure>
<img src="../images/pbr/ray_tracing.png" style="width:70.0%" alt="Figure representing ray tracing, from “Predicting acoustic emission attenuation in solids using ray-tracing within a 3D solid model” by Mohamed El-Shaib." />
<figcaption aria-hidden="true">Figure representing ray tracing, from “Predicting acoustic emission attenuation in solids using ray-tracing within a 3D solid model” by Mohamed El-Shaib.</figcaption>
</figure>
<h2 id="specular-reflection-model">Specular reflection model</h2>
<p>To formalize the above algorithm, one needs to define a model deciding in which direction a light ray reflects from a surface it hits.</p>
<p>One simple reflection model is the specular reflection. It assumes the reflected ray angle to the normal of the surface is the same as the angle between the shining ray and the normal.</p>
<figure>
<img src="../images/pbr/specular_reflection_back.png" style="width:70.0%" alt="Specular reflection model. Angle MNP is the same as PNO." />
<figcaption aria-hidden="true">Specular reflection model. Angle MNP is the same as PNO.</figcaption>
</figure>
<p>If one knows the shapes and locations of objects, lights, and the camera and assumes the reflections follow the specular model, it is easy to calculate the paths of light rays in the scene.</p>
<p>The resulting images would assume that every surface behaves as an ideal mirror, though.</p>
<h2 id="various-materials">Various materials</h2>
<p>The reality is more complex, and there are multitudes of different materials with varying behaviors, like:</p>
<ul>
<li>reflecting the light of different colors that makes us see colorful images,</li>
<li>reflecting rays not in a single direction, but in a range of directions (<a href="https://en.wikipedia.org/wiki/Diffuse_reflection">diffuse reflection</a>),</li>
<li>allowing (a part of) light to pass through the material after being <a href="https://en.wikipedia.org/wiki/Refraction">refracted</a> at the boundary of mediums (e.g. in glass).</li>
</ul>
<h3 id="bxdf-formalism">BxDF formalism</h3>
<p>To formally describe the distribution of the reflected light, scientists use a function called Bidirectional Reflectance Distribution Function (BRDF). It is a non-negative function of three parameters</p>
<p><span class="math display">\[f_r(p, \omega_i, \omega_o): R^3 \times [-\pi, \pi]^2 \times [-\pi, \pi]^2 \rightarrow R_+ \cup \{0\}\]</span></p>
<p>where <span class="math inline">\(p\)</span> is the point on the surface hit by the ray, <span class="math inline">\(\omega_i\)</span> is the angle (on the hemisphere) between the incoming ray and the plane tangent to the object in <span class="math inline">\(p\)</span>, and <span class="math inline">\(\omega_o\)</span> is the angle on the same hemisphere between the outgoing ray and the tangent.</p>
<p>When the value of BRDF is high, a lot of the energy of the ray coming from the direction <span class="math inline">\(\omega_i\)</span> passes to the ray outgoing in the direction <span class="math inline">\(\omega_o\)</span>; if it is 0, that ray is not reflecting in this direction at all.</p>
<p>BRDF is normalized to behave like a probability density function:</p>
<p><span class="math display">\[\forall_p \int f_r(p, \omega_i, \omega_o) \text{d}\omega_i \text{d}\omega_o \le 1\]</span></p>
<p>where the integral can be less than 1 to account for the energy loss.</p>
<p>Apart from the light reflected from the surface of an object, we would like to model light passing (transmitted) through partially transparent materials. To do so, we use an analogical function called Bidirectional Transmittance Distribution Function (BTDF) <span class="math inline">\(f_t(p, \omega_i, \omega_o)\)</span> which describes the amount of energy preserved when light ray coming from direction <span class="math inline">\(\omega_i\)</span> hits point <span class="math inline">\(p\)</span> and gets refracted to a direction <span class="math inline">\(\omega_o\)</span> on the hemisphere on the other side of the surface.</p>
<p>The sum of these functions, describing the two effects together, is called a Bidirectional Scattering Distribution Function (BSDF, <span class="math inline">\(f\)</span>):</p>
<p><span class="math display">\[f(p, \omega_i, \omega_o) = f_r(p, \omega_i, \omega_o) + f_t(p, \omega_i, \omega_o)\]</span></p>
<figure>
<img src="../images/pbr/bsdf.png" style="width:70.0%" alt="A diagram showing the split of BSDF into transmittance and reflectance parts." />
<figcaption aria-hidden="true">A diagram showing the split of BSDF into transmittance and reflectance parts.</figcaption>
</figure>
<h4 id="color">Color</h4>
<p>Note that I never mentioned the color in the definitions above. Under this model, we assume that the BSDF (distribution of directions of the outgoing light) is independent of the color (wavelength) of the ray. It is a simplifying assumption that makes it impossible to model situations like dispersion in a prism (where different colors are refracted by a different angle, i.e. need different BSDFs). Nevertheless, it doesn’t affect most of the scenes, and is used in Blender, so we’ll stick with it.</p>
<p>It doesn’t mean that the objects themselves don’t have color, but as handling it is independent from how the light ray travels through the scene, it will be skipped here<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>.</p>
<figure>
<img src="../images/pbr/prism_crop.jpg" style="width:50.0%" alt="A dispersion effect which requires different BSDFs for different colors. A photo made by Zátonyi Sándor shared on CC-SA license." />
<figcaption aria-hidden="true">A dispersion effect which requires different BSDFs for different colors. <a href="https://commons.wikimedia.org/wiki/File:Színszóródás_prizmán2.jpg">A photo made by Zátonyi Sándor</a> shared on CC-SA license.</figcaption>
</figure>
<h2 id="rendering-equation">Rendering equation</h2>
<p>How do we use the information about how the light is scattered on various surfaces to draw pixels on the screen?</p>
<p>Under <a href="https://en.wikipedia.org/wiki/Pinhole_camera">a standard camera model</a><a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>, the camera is a point, in front of which there is a canvas where the image will appear. The energy of a light ray determines the intensity of the pixel on the canvas that the light ray passed through before reaching the camera.</p>
<figure>
<img src="../images/pbr/pinhole_background.png" style="width:70.0%" alt="Diagram showing the canvas spread in front of the camera." />
<figcaption aria-hidden="true">Diagram showing the canvas spread in front of the camera.</figcaption>
</figure>
<p>To model the light shining from a particular point in space, apart from knowing how a single ray scatters locally (BSDFs), we need a way to sum up all the incoming light rays into a single outgoing one. To do this, we use a <em>rendering equation</em>:</p>
<p><span class="math display">\[L_o(p, \omega_o) = L_e(p, \omega_o) + \int f(p, \omega_i, \omega_o) L_i(p, \omega_i) \text{cos}(\sphericalangle(\omega_i, \omega_o)) \text{d} \omega_i\]</span></p>
<p>Where <span class="math inline">\(L_o\)</span> is the light outgoing from <span class="math inline">\(p\)</span> in the direction of <span class="math inline">\(\omega_o\)</span>, <span class="math inline">\(L_e\)</span> is the amount of the light emitted in that direction (i.e. when <span class="math inline">\(p\)</span> is a light source), and <span class="math inline">\(L_i\)</span> is the total light intensity incoming from direction <span class="math inline">\(\omega_i\)</span>.</p>
<p>Note that we can calculate <span class="math inline">\(L_i\)</span> recursively: to get the amount of light incoming to <span class="math inline">\(p\)</span> from direction <span class="math inline">\(\omega_i\)</span>, we can find <span class="math inline">\(q\)</span>: the point on the intersection of the incoming ray and the surface nearest to <span class="math inline">\(p\)</span>, and the light incoming to <span class="math inline">\(p\)</span> from <span class="math inline">\(q\)</span> will be the same as the light outgoing from <span class="math inline">\(q\)</span> in the direction of <span class="math inline">\(p\)</span>:
<span class="math display">\[L_i(p, \omega_i) = L_i(p, \overrightarrow{qp}) = L_o(q, \overrightarrow{pq})\]</span></p>
<figure>
<img src="../images/pbr/pq_rendering_background.png" style="width:40.0%" alt="Diagram illustrating how to calculate the light incoming to point p based on the light outgoing from q." />
<figcaption aria-hidden="true">Diagram illustrating how to calculate the light incoming to point <span class="math inline">\(p\)</span> based on the light outgoing from <span class="math inline">\(q\)</span>.</figcaption>
</figure>
<p>If we knew how to calculate the integral in the rendering equation, we could get the angle between a ground plane and the camera - pixel vector for each pixel on the canvas and calculate the light incoming to the camera from that direction:
<span class="math display">\[L_i(\text{camera}, \overrightarrow{(\text{pixel}, \text{canvas})})\]</span></p>
<p>using the rendering equation, which would define the intensity of the pixel.</p>
<h3 id="approximating-the-integral">Approximating the integral</h3>
<p>As anyone who ever calculated an integral knows, analytically working out an integral is difficult in all but the simplest of cases. Even if we restricted our BSDFs to very simple functions (limiting the properties of materials that we could express), we’d still need to do the calculation recursively over all (generally infinitely many) of the points the light ray can reach.</p>
<p>As the exact solution to the rendering equation is infeasible, we will be looking for an approximate one. The most popular method to do so is <em><a href="https://en.wikipedia.org/wiki/Monte_Carlo_integration">Monte Carlo integration</a></em>.</p>
<p>It is based on the interpretation of the integral as a mean value of the function of a random variable:
<span class="math display">\[\int_X f(x) \text{d}x = \mathbb{E}_{x \sim X} f(x)\]</span></p>
<p>Now, instead of calculating the function for every possible value of the random variable <span class="math inline">\(X\)</span>, we can sample it a couple of times according to its distribution and average the value of the function in these points:
<span class="math display">\[\mathbb{E}_X f(x) \approx µ_N = \frac{1}{N}\sum_{i=1}^N f(x_i), x_i \sim X\]</span></p>
<p>Of course, the more samples we take (the bigger N is), the closer (on average<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>) the resulting mean <span class="math inline">\(µ_N\)</span> will be to the real mean <span class="math inline">\(\mathbb{E}_{x \sim X} f(x)\)</span>. At the same time, the <span class="math inline">\(µ_N\)</span> estimator is unbiased, i.e. has no consistent skew to be bigger or lower than the true mean.</p>
<p>This leads to an algorithm we can implement. For a given pixel <span class="math inline">\(p\)</span>:</p>
<ol type="1">
<li>We find the first intersection point <span class="math inline">\(q\)</span> on the vector <span class="math inline">\(\overrightarrow{cp}\)</span> starting in the camera <span class="math inline">\(c\)</span> and passing through the pixel.</li>
<li>We sample the new incoming direction (corresponding to <span class="math inline">\(\omega_i\)</span> in the rendering equation) uniformly from the hemisphere centered in <span class="math inline">\(q\)</span>.</li>
<li>We recursively calculate <span class="math inline">\(L_i(q, \omega_i)\)</span>, stopping the recursion after a fixed number of bounces (the process would never end otherwise).</li>
<li>We set the pixel to <span class="math inline">\(L_e(q, \overrightarrow{pc})\)</span> (positive if <span class="math inline">\(q\)</span> is a light source) + <span class="math inline">\(f(q, \omega_i, \overrightarrow{pc})\)</span> (BSDF) times the value of recursively calculated incoming light (<span class="math inline">\(L_i(q, \omega_i)\)</span>) times the cosine of the angle between <span class="math inline">\(\omega_i\)</span> and <span class="math inline">\(\overrightarrow{pc}\)</span>.</li>
<li>As we repeat this process with sampling multiple directions <span class="math inline">\(\omega_i\)</span> and average the results, the pixel intensity approaches the correct value based on the rendering equation.</li>
</ol>
<h4 id="disclaimer-on-practice">Disclaimer on practice</h4>
<p>This process, while unbiased (except for the fact that we cut the recursion after a couple of bounces), can require a lot of samples to converge. In particular, for a punctual light, it is effectively impossible to hit it (we would reach it with probability 0).</p>
<p>There are some simple ways to make it more efficient, e.g. by shooting rays from both the camera and the light source at the same time or changing the distribution of rays to a different one than uniform over angles and correcting the resulting integral (<a href="https://en.wikipedia.org/wiki/Importance_sampling">importance sampling</a>).</p>
<p>Many PhDs have been written on how to speed up the rendering process. To keep things simple, I’ll assume the model described above, with rays shooting from the camera to approximate the integrals with vanilla Monte-Carlo, even if this is not how one implements a modern renderer in practice.</p>
<h2 id="artist-to-math-translation">Artist to math translation</h2>
<p>BSDF is a representation of the material that’s convenient for a computer, as it can easily sample outgoing ray directions and quantify the amount of lost energy from the bounce.</p>
<p>Expressing the material in terms of a BSDF is tricky for artists who create 3D models, though: it’s more natural for humans to think about the physical properties like color or glossiness than about the 7-dimensional BSDF.</p>
<p>To facilitate the process of constructing materials, 3D rendering software has nodes that can map physical parameters like “Transmission” or “Roughness” into a BSDF which will be later used by the renderer.</p>
<p>The most popular node in Blender that outputs a BSDF is called <a href="https://docs.blender.org/manual/en/latest/render/shader_nodes/shader/principled.html">Principled BSDF</a>. It is based on <a href="https://static1.squarespace.com/static/58586fa5ebbd1a60e7d76d3e/t/593a3afa46c3c4a376d779f6/1496988449807/s2012_pbs_disney_brdf_notes_v2.pdf">a seminal paper</a> from Disney, where they were investigating ways to express a BSDF using a small number of properties understandable by artists.</p>
<p>In the next blog post, I analyze its parameters to see how they influence the produced material.</p>
<section class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>I plan to do another blog post dealing specifically with various properties of materials, including color.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>Ignoring camera lenses and inversion of the image caused by the pinhole.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p>See <a href="https://en.wikipedia.org/wiki/Law_of_large_numbers">law of large numbers</a> for a formalization and proof of this statement.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
]]></description>
    <pubDate>Sun, 28 Nov 2021 00:00:00 UT</pubDate>
    <guid>https://sygnowski.ml/posts/2021-11-28-pbr-101.html</guid>
    <dc:creator>Jakub Sygnowski</dc:creator>
</item>
<item>
    <title>Easy-to-use text-to-speech</title>
    <link>https://sygnowski.ml/posts/2021-10-27-tts.html</link>
    <description><![CDATA[<div class="info">
    Posted on October 27, 2021
    
</div>
<h1>Easy-to-use text-to-speech</h1>
<p>Adventure games, the genre I like, tend to contain a lot of talking. Apart from writing the character statements on the screen, bigger game studios hire voice actors to record them and play the recordings. It is a lengthy and costly process. I decided to see how far text-to-speech is from being able to substitute human actors for indie adventure games.</p>
<h2 id="problem-definition">Problem definition</h2>
<p>I define the text-to-speech problem as:</p>
<ul>
<li>take as input a short English text prepared by the game writer and one of a couple of identities of the game characters</li>
<li>produce an audio wave where a character speaks the text.</li>
</ul>
<p>Text-to-speech is an active research domain, and one could easily spend weeks (or a whole career) looking for the best solution. I chose to limit myself to methods that:</p>
<ol type="1">
<li>are available for free,</li>
<li>don’t require me to do further research to make it work,</li>
<li>are easily available on the internet; in particular require no training neural networks on a high-end GPU for days.</li>
</ol>
<h2 id="high-level-state-of-research">High-level state of research</h2>
<p>To explain how the current models work, one needs to first define what a <em>mel-spectrogram</em> is.</p>
<p>According to <a href="https://en.wikipedia.org/wiki/Mel-frequency_cepstrum">Wikipedia</a>:</p>
<blockquote>
<p>Mel-frequency cepstrum (MFC) is a representation of the short-term power spectrum of a sound, based on a linear cosine transform of a log power spectrum on a nonlinear mel scale of frequency.</p>
</blockquote>
<p>Putting it in English, mel-spectrogram describes how much energy the part of the sound wave of a given frequency transports in time.</p>
<p>It’s a three-dimensional plot: the color conveys the amount of energy (lighter color: more energy), the Y-axis corresponds to frequency, and the X-axis to time.</p>
<figure>
<img src="../images/tts/mel-spectrogram.jpg" style="width:30.0%" alt="An example mel-spectrogram. Note a region of lower energy after around 130 time units, which corresponds to a pause in the sound wave." />
<figcaption aria-hidden="true">An example mel-spectrogram. Note a region of lower energy after around 130 time units, which corresponds to a pause in the sound wave.</figcaption>
</figure>
<p>An important property of mel-spectrograms is that they contain less information than the initial sound wave, i.e. there are multiple waves with the same mel-spectrogram.</p>
<h3 id="current-models">Current models</h3>
<p>To generate a single sound wave from a sentence of text, people currently apply two models sequentially:</p>
<ol type="1">
<li>An acoustic model, which transforms text to a mel-spectrogram, e.g. <a href="https://arxiv.org/abs/1712.05884">Tacotron 2</a>, <a href="https://arxiv.org/abs/2005.11129">Glow-TTS</a>, or <a href="https://arxiv.org/abs/2006.04558">FastSpeech 2</a>.</li>
<li>A vocoder, which maps the mel-spetrogram to the actual sound wave. The most popular vocoder is <a href="https://arxiv.org/abs/2010.05646">HiFi-GAN</a>, with other examples being <a href="https://arxiv.org/abs/1910.06711">MelGAN</a> and <a href="https://arxiv.org/abs/1811.00002">WaveGlow</a>.</li>
</ol>
<figure>
<img src="../images/tts/diagram.jpg" style="width:90.0%" alt="A high-level diagram describing architecture of the text-to-speech models." />
<figcaption aria-hidden="true">A high-level diagram describing architecture of the text-to-speech models.</figcaption>
</figure>
<p>There are some end-to-end models, like <a href="https://arxiv.org/abs/1609.03499">WaveNet</a> or <a href="https://arxiv.org/pdf/2006.04558.pdf">FastSpeech 2s</a> which are able to generate waves directly from text (without intermediate mel-spectrograms), but they don’t seem to be as popular these days.</p>
<h2 id="datasets">Datasets</h2>
<p>There are dozens of <a href="https://github.com/jim-schwoebel/voice_datasets">datasets</a> of varying quality containing recorded voice and the corresponding text. After skimming the papers above, I think these are the most popular ones:</p>
<ul>
<li><a href="https://keithito.com/LJ-Speech-Dataset/">LJ Speech</a>: a dataset with one female English speaker, reading 13100 short audio clips from 7 non-fiction books, consisting of approximately 24 hours in total.</li>
<li><a href="https://research.google/tools/datasets/libri-tts/">LibriTTS</a>: a multi-speaker English dataset containing 585 hours of speech by 2456 speakers (around 15min/speaker), reading Project Gutenberg books.</li>
<li><a href="https://datashare.ed.ac.uk/handle/10283/2950">VCTK</a>: 44 hours of mostly newspaper passages, read by 109 English speakers with different accents<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>, so 0.5h of speech/speaker.</li>
</ul>
<h2 id="analysis-of-the-models">Analysis of the models</h2>
<p>For the sake of this post, I tried out the three acoustic models mentioned above: <a href="https://github.com/NVIDIA/tacotron2">Tacotron 2</a>, <a href="https://github.com/jaywalnut310/glow-tts">Glow-TTS</a>, <a href="https://ming024.github.io/FastSpeech2/">FastSpeech 2</a>, whose implementations are available on Github. I combined them with a <a href="https://github.com/jik876/hifi-gan">Hifi-GAN</a> vocoder.</p>
<h3 id="using-pretrained-models">Using pretrained models</h3>
<p>While all of the papers claim they managed to train on either LJSpeech or one of VCTK and LibriTTS, for Glow-TTS and Tacotron 2 it was hard to find models pretrained on the multi-speaker datasets.</p>
<p>Here are the samples generated from the models based on LJSpeech:</p>
<figure>
<audio controls src="../data/tts/glowtts_hifigan_lj.wav">
Your browser does not support the <code>audio</code> element.
</audio>
<audio controls src="../data/tts/tacotron_hifigan_lj.wav">
Your browser does not support the <code>audio</code> element.
</audio>
<audio controls src="../data/tts/fastspeech2_hifigan_lj.wav">
Your browser does not support the <code>audio</code> element.
</audio>
<figcaption>
Samples generated from Glow-TTS, Tacotron, and FastSpeech2 with HiFi-GAN trained on LJ Speech (left to right).
</figcaption>
</figure>
<p>Both GlowTTS and tacotron sound great, with Glow-TTS maybe being a bit better. Fastspeech breaks a little and has problems with semicolons.</p>
<p>For a game with multiple characters, I need to generate a couple of different voices. While FastSpeech’s quality was not as good as the other models’, the authors of its Github repo provided a model pretrained on VCTK with an ability to condition on a speaker.</p>
<p>Here are samples I generated using FastSpeech2 with two different speakers:</p>
<figure>
<audio controls src="../data/tts/fastspeech2_hifigan_vctk.wav">
Your browser does not support the <code>audio</code> element.
</audio>
<audio controls src="../data/tts/fastspeech2_hifigan_vctk_another.wav">
Your browser does not support the <code>audio</code> element.
</audio>
<figcaption>
Samples from FastSpeech2 trained on VCTK, conditioned on speakers 270 and 277.
</figcaption>
</figure>
<p>Not surprisingly, the voice is even more robotic<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> (VCTK has more data overall, but less per speaker), but at least the model is easily accessible.</p>
<h4 id="modifying-the-vocoder">Modifying the vocoder</h4>
<p>I mentioned on which datasets the acoustic models above were trained, but what about the vocoder (HiFi-GAN)?</p>
<p>In <a href="https://github.com/jik876/hifi-gan">its repo</a>, authors provide checkpoints of HiFi-GAN trained on either: LJSpeech (in a version finetuned to the mel-spectrograms provided by Tacotron or not), VCTK, or LJSpeech, VCTK, and LibriTTS together. The last one (called universal model) was provided for easy finetuning to a particular acoustic model/another dataset.</p>
<p>To generate the previous samples, I used the vocoder model trained on the same dataset as acoustic model used (using the Tacotron-finetuned version for LJSpeech)<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>.</p>
<p>One may wonder: to what extent the choice of the dataset the vocoder was trained on is important? In other words: can we train a single vocoder and use it with different datasets?</p>
<p>Technically, the vocoder needs to choose one sound wave out of many that have the same mel-spectrogram as its input. However, it’s possible that the mel-spectrogram contains all the important information about the sound, and there is only one “reasonable” sound wave to choose, and the vocoder only needs to learn how to find it.</p>
<p>As the vocoder’s input is only the mel-spetrogram (no separate text), it has to include the information about <em>what</em> is said. Is the speaker’s identity encoded there too? If not (and it’s the vocoder that injects this information into the sound wave), we will be able to change the speaker by using a different HiFi-GAN checkpoint.</p>
<p>I tried doing just that: passed a mel-spectrogram generated by a model trained on LJ Speech to the vocoder trained on VCTK:</p>
<figure>
<audio controls src="../data/tts/glowtts_hifigan_vctk.wav">
Your browser does not support the <code>audio</code> element.
</audio>
<figcaption>
Sample of the Glow-TTS model trained on LJ Speech, using Hifi-GAN vocoder trained on VCTK.
</figcaption>
</figure>
<p>As you can hear, the original (LJ Speech) voice is still audible in the sound wave, meaning that mel-spectrogram contains speaker-specific features. At the same time, the voice is more shaken compared to when the correct checkpoint was used, meaning that the vocoder itself needs to be adjusted to the dataset, too.</p>
<h3 id="finetuning">Finetuning</h3>
<p>Everything I generated so far was either using the same female voice of LJ Speech, or was noisy. As I would like to get samples with multiple voices, I decided to try finetuning (adjusting the weights of) the models.</p>
<p>For the extra data to finetune the models on, I used all data available for speaker 270 in VCTK. It is a male voice (so easy to distinguish from the LJ Speech one, even for an amateur like me), and there is around half an hour of it available.</p>
<p>Out of the models I tested above, finetuning is easily possible only for Tacotron and HiFi-GAN, as:</p>
<ul>
<li>Fastspeech2 comes already trained on VCTK, so I can’t expect to get anything better than what’s already there <a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>.</li>
<li>the authors of the Glow-TTS repo provide a pretrained model for inference, but they don’t publish a part of the training model (one handling Data-dependent initialization) that is necessary for finetuning.</li>
</ul>
<h4 id="tacotron">Tacotron</h4>
<p>I finetuned the LJSpeech model for another 6000 steps. The validation loss was already at the lowest after 2000. As the training was taking around an hour for 1000 steps, two hours was enough to finetune Tacotron on the new speaker’s data.</p>
<p>Here are the samples from the model, generated with the VCTK version of Hifi-GAN:</p>
<figure>
<audio controls src="../data/tts/tacotron_vctk_2000.wav">
Your browser does not support the <code>audio</code> element.
</audio>
<audio controls src="../data/tts/tacotron_vctk_6000.wav">
Your browser does not support the <code>audio</code> element.
</audio>
<figcaption>
Sample of Tacotron finetuned after 2000 steps, and one after 6000 steps. Sound the same, with some robotic noise at the beginning.
</figcaption>
</figure>
<h4 id="hifi-gan">HiFi-GAN</h4>
<p>When Tacotron generates mel-spectrograms, it does so one timestep at a time, passing whatever was generated at the previous steps as an input for the current-step generation<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>.</p>
<p>It means if a model makes a mistake mid-sentence, it’s hard for it to get back on track. One can hear it easily early in the training when the model starts the sentence correctly but then breaks down into mumbling:</p>
<figure>
<audio controls src="../data/tts/tacotron_vctk_600.wav">
Your browser does not support the <code>audio</code> element.
</audio>
<figcaption>
Tacotron finetuned to a different actor after only 600 steps.
</figcaption>
</figure>
<p>To avoid being passed “wrong” mel-spectrograms while still adapting the vocoder to the ones produced by a particular acoustic model, HiFi-GAN uses Tacotron in a “teacher-forcing” mode when it is finetuned.</p>
<p>This mode is a method to change the inference behavior of Tacotron. When generating the <span class="math inline">\(i\)</span>-th slice of the mel-spectrogram, instead of conditioning the model on the previous output the model generated, it uses the ground-truth mel-spectrogram (produced out of a ground-truth sound wave) as the conditioning input.</p>
<p>This way, even if at some point the Tacotron model produces the wrong mel-spectrogram slice, the error will not get accumulated, as the future predictions won’t use the incorrect slice.</p>
<p>As such, the training data for HiFi-GAN consists of mel-spectrograms produced in the teacher-forcing mode by the already finetuned Tacotron and the sound waves. After generating it, I finetuned the vocoder using the “universal” checkpoint from the repo.</p>
<p>Here are samples I got without finetuning (using base universal checkpoint) HiFi-GAN (left), and after finetuning for around 10h (right).</p>
<figure>
<audio controls src="../data/tts/tacotron_universal_2000.wav">
Your browser does not support the <code>audio</code> element.
</audio>
<audio controls src="../data/tts/tacotron_hifi_finetuned_2000.wav">
Your browser does not support the <code>audio</code> element.
</audio>
<figcaption>
Sample of the finetuned (after 2000 steps) Tacotron, using a pretrained (left) and finetuned (right) HiFi-GAN.
</figcaption>
</figure>
<p>The finetuning helped a bit, but it didn’t manage to fully remove the metallic vibration when the actor would be taking a breath.</p>
<h4 id="compute">Compute</h4>
<p>Apart from learning the details of text-to-speech technology, this experiment was also an exercise in getting free access to the computing power.</p>
<p>Everyone knows that training the models on a CPU takes a long time. Furthermore, the code is written in a way that you need to modify the code to not use a GPU, which isn’t always easy.</p>
<p>I tried out two venues that offer free GPU for training deep learning models:</p>
<ol type="1">
<li><a href="https://colab.research.google.com">Google Colab</a></li>
<li><a href="https://gradient.run/free-gpu">Paperspace notebooks</a></li>
</ol>
<p>I found Colab easier to use overall: it was quicker to connect to a runtime, and the integration with Google Drive made moving data and the code between the local machine and the Colab easy.</p>
<p>Another benefit of Colab is that it’s more popular, so there are lots of instructions online on how to do even complex things<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>.</p>
<p>The top selling point of Paperspace is that they give a guaranteed uptime of up to 6h.</p>
<p>In the Colab, Google sends a pop-up to check whether you’re still there every couple of hours and disconnects you after some time anyway. In Paperspace, when you request a machine you may be asked to wait, but once you get it reserved, it’s yours for the period you requested it for.</p>
<p>Even more, you can close the browser window/turn off the computer and still expect the GPU to be crunching the matrices for you in the background, while the Colab disconnects you from the accelerator the moment you close the tab.</p>
<p>It makes Paperspace more convenient for training, as you can leave it for the night, not having to continuously monitor the process.</p>
<p>I also liked the interface of Paperspace a bit more, as it looked a bit more like a full VM with a disk and terminal (even though I didn’t manage to make tensorboard work), but that’s a minor difference.</p>
<p>Overall, I would say that Colab is better for doing quick inference, and Paperspace would be my choice for finetuning models. Full training (which would take around a week for the models I tested) would still be painful to do in 6h increments, but I didn’t want to do it anyway.</p>
<h2 id="prosodies">Prosodies</h2>
<p>If you hear a couple of sample sentences generated by these models, you’ll likely find them correctly pronounced but lifeless. The intonation, accent, and rhythm that conveys emotions (called <a href="https://en.wikipedia.org/wiki/Prosody_(linguistics)">prosody</a> by the linguists) are not there.</p>
<p>One could claim that understanding the text enough to express prosody is an insanely difficult problem, but I don’t think it’s crazy to expect it to be solved anytime soon. It sounds easier than the type of things <a href="https://openai.com/blog/openai-api/">GPT-3</a> does, although GPT-3 is an effort bigger by orders of magnitude (in terms of parameters, people, compute, etc.) than any of the models described above.</p>
<p>I haven’t seen TTS expressing emotion yet, though. I found <a href="https://apple.github.io/neural-tts-with-prosody-control/">one paper</a> that goes in the direction of controlling the prosody with some good samples, but unfortunately without available code.</p>
<section class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>The significant majority of the speakers come from the British Isles.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>quite appropriately to be fair, given that it’s generated by a machine<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p>For LibriTTS, even though the HiFi-GAN repo doesn’t contain the model trained only on LibriTTS, the FastSpeech2 code authors included a LibriTTS checkpoint directly in theirs.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4" role="doc-endnote"><p>Technically, multi-speaker training could be more difficult than finetuning on a single (new) voice. I still don’t expect anything better than the poor quality of LJ Speech, for which a lot of data is available.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5" role="doc-endnote"><p>the models that have this property are called <em>autoregressive</em><a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6" role="doc-endnote"><p>For example, for another project, it was easy to get Colab to record video from the webcam and play it back.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
]]></description>
    <pubDate>Wed, 27 Oct 2021 00:00:00 UT</pubDate>
    <guid>https://sygnowski.ml/posts/2021-10-27-tts.html</guid>
    <dc:creator>Jakub Sygnowski</dc:creator>
</item>
<item>
    <title>Learning Blender</title>
    <link>https://sygnowski.ml/posts/2021-10-22-blender.html</link>
    <description><![CDATA[<div class="info">
    Posted on October 22, 2021
    
</div>
<h1>Learning Blender</h1>
<p>When I was doing the graphics for the T.I.M.E stories expansion, I tried out various workflows for creating the content. Looking for inspiration on how professionals make art faster, I encountered a GDC talk titled <a href="https://www.youtube.com/watch?v=CYbYvImd7Bw&amp;ab_channel=GDC">Concept Art is Dead</a>. It claimed that no one is drawing things from scratch and everyone uses quick shortcuts like rendering a scene in Blender to get nice shadows or copying the texture from the internet instead. I tried it out to see how learning the basics of Blender may help me in creating 2D graphics.</p>
<h2 id="thoughts-on-3d-modeling">Thoughts on 3D modeling</h2>
<p>Having grown up playing games like The Sims (1!) or Syberia, I didn’t associate 3D graphics with high quality. Only when rediscovering games as an adult and seeing friends doing crazy realistic renders using <a href="https://en.wikipedia.org/wiki/Physically_based_rendering">physically based rendering</a> did I realize the progress computer graphics made in the last couple of years.</p>
<figure>
<img src="../images/blender/syberia.jpg" style="width:40.0%" alt="Screenshot from Syberia" />
<img src="../images/blender/render.jpg" style="width:45.0%" alt="Example high-quality 3D render" />
<figcaption>
Screenshot from the Syberia game (2002) vs <a href="https://www.artstation.com/artwork/bRa9r">Castle Duurstede</a> render (2017).
</figcaption>
</figure>
<p>While comparing PBR to the old graphics is not fair, as the former uses way more compute, recent improvements in hardware (like NVIDIA RTXs) narrowed the gap to the point where PBR technology becomes close to real-time.</p>
<p>Another part of the 3D workflow that starts to get hardware improvements is asset creation. An ability to scan a 3D object (<a href="https://en.wikipedia.org/wiki/Photogrammetry">photogrammetry</a>), which used to be pure science fiction a couple of years back, is being made possible with advances in AI and better cameras.
For example, the new iPhone includes a lidar camera, which captures not only RGB but also depth. It makes the automatic 3D model construction more accurate.</p>
<p>Having friends who regularly chat about computer graphics, I wanted to get more familiar with it for some time now. Making the graphics for the game turned out to generate enough motivation to actually do something.</p>
<h2 id="blender-guru-tutorials">Blender Guru tutorials</h2>
<p>I started learning Blender by following a classic <a href="https://www.youtube.com/watch?v=TPrnSACiTJ4&amp;t=9s&amp;ab_channel=BlenderGuru">Donut tutorial</a> by Andrew Price who also turned out to be the person behind <a href="https://www.youtube.com/watch?v=Qj1FK8n7WgY&amp;t=549s&amp;ab_channel=BlenderGuru">Understanding Color</a> and <a href="https://www.youtube.com/watch?v=vM39qhXle4g&amp;t=1s&amp;ab_channel=Blender">The Habits of Effective Artists</a>, which I highly enjoyed.
It’s a popular entry point for Blender, with the first video having been watched &gt;10M times. And for a good reason: the tutorial is very approachable, yet it shows lots of elements in a short time. It regularly reminds the keyboard shortcuts, which is very useful in a complex program like Blender.</p>
<figure>
<a href="../images/blender/donut.png"><img src="../images/blender/donut-res.png" style="width:45.0%" alt="Render of a donut" /></a>
<a href="../images/blender/cup.png"><img src="../images/blender/cup-res.png" style="width:45.0%" alt="Render of a cup" /></a>
<figcaption>
My final donut and cup renders. While I see a couple of things I could improve, I decided to leave them be and move forward to other challenges.
</figcaption>
</figure>
<p>After the Donut tutorial, I watched the <a href="https://www.youtube.com/watch?v=cg1K_ZWB0Uw&amp;list=RDCMUCOKHwx1VCdgnxwbjyb9Iu1g&amp;index=23&amp;ab_channel=BlenderGuru">Lightning for Beginners</a> but didn’t follow with any exercises.</p>
<h2 id="playing-on-my-own">Playing on my own</h2>
<p>Afterward, I went to model the barrack for the time stories expansion. I took a photo from an online museum, imported it to Blender, and tried to reconstruct the model based on it.
While making the whole building required some work, the modeling techniques presented in the donut tutorial were enough to create a reasonable shape.</p>
<figure>
<img src="../images/blender/barrack.png" style="width:60.0%" alt="Final render of the barrack used in the T.I.M.E stories expansion." />
<figcaption aria-hidden="true">Final render of the barrack used in the T.I.M.E stories expansion.</figcaption>
</figure>
<p>The nice part about creating the graphics in 3D was that I could reuse the single building in multiple places, creating various shots from inside, outside, between two buildings, etc. In the end, I used it in the creation of 5 scenes.</p>
<h2 id="legally-stealing-work-of-others">(Legally) stealing work of others</h2>
<p>Modeling the shape of the building is only the first part of the workflow. There was a couple of smaller assets that I needed, like: the window, railing, bunk beds, etc.
I wasn’t doing them on my own.</p>
<p>To my surprise, it’s possible to find lots of re-usable assets for free on the internet.
I used these two a lot:
- <a href="https://sketchfab.com/">sketchfab</a> for 3D assets. A lot of ones available there are licensed with CC attribution: one can use them for any reason (including commercial use) but need to attribute the author.
- <a href="https://polyhaven.com">Poly Haven</a> for “physically based” textures of various surfaces: grass, wall, brick, etc.
To make the materials look good, I needed to play with the lightning and the properties of the materials’ <a href="https://en.wikipedia.org/wiki/Bidirectional_reflectance_distribution_function">BRDF</a><a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. I think there is a big room for improvement here, but I accepted the good-enough version of the renders.</p>
<h3 id="rigid-bodies">Rigid bodies</h3>
<p>In one of the scenes in the game, I have a couple of people lying on the ground. While I was planning to not include people in the render and only paste them afterward, I considered it an opportunity to try the physics simulation that Blender has.</p>
<figure>
<video width="410" height="310" controls>
<source src="/images/blender/rigid.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
<figcaption>
An animation of cubes being hit by a chained ball from a <a href="https://www.youtube.com/watch?v=nHVYYMG3QVY">Blender guru tutorial</a>.
</figcaption>
</figure>
<p>I thought it would be as simple as downloading one of many character models available on the internet, turning the gravity on, and seeing the bodies being animated by the engine.</p>
<p>While finding a model of a person wasn’t hard<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>, I needed to learn (or re-learn) a bit more about how the physics simulator works to make it happen.
To make an object be affected by gravity, one needs to make it a <a href="https://docs.blender.org/manual/en/latest/physics/rigid_body/index.html">rigid body</a>. If done straightforwardly, it makes the character react to gravity but doesn’t cause it to deform when hitting other objects (it’s a <em>rigid</em> body after all).</p>
<h4 id="rigging">Rigging</h4>
<p>Expressing how exactly to deform a character mesh, which consists of thousands of vertices sounds very complex. At the same time, animators don’t need such granularity: they do more general things like: “raise a left arm” or “turn head to the right”, not move vertices in arbitrary directions.</p>
<p>To make controlling the characters easier, people create rigs. It’s a set of controls, called <em>bones</em>, which are connected to different parts of the character mesh in a way that if the bone moves, the corresponding vertices in the body (called “skin” in this context) are moving together with it.</p>
<p>This way, the animator has 10 or 20 controls that it can use, like “left arm”, and he makes the character move using them.
The good characters that you can find online come “rigged”, i.e. with the rig (also called the armature) created and ready to be used.</p>
<p>Using bones is not trivial in the context of rigid bodies. As they are attached to a mesh but are not a proper body on their own, you cannot make a bone a rigid body to be affected by gravity.
To overcome this problem, at first, I tried using the <a href="https://github.com/Pauan/blender-rigid-body-bones">rigid bones</a> blender add-on. It allows you to easily transform a set of bones into rigid bodies.</p>
<figure>
<img src="../images/blender/broken.gif" style="width:30.0%" alt="The effect of applying rigid bones to the skeleton character I used." />
<figcaption aria-hidden="true">The effect of applying rigid bones to the skeleton character I used.</figcaption>
</figure>
<p>I encountered lots of problems when using this setup, with the character spinning with increasing velocity, the bones moving away in random directions, or passing through the ground.</p>
<h4 id="parent-child-relationship">Parent-child relationship</h4>
<p>I tried changing some settings of the physics engine to prevent it from exploding but without success. A couple of tutorials and many hours of trial and error later, I managed to fix the problem.</p>
<p>A concept I understood was a parent-child relationship between the objects. In Blender, you can make one object (A) a parent of another (B), what makes A follow any transformation that B is performing.</p>
<p>It is a mechanism that makes the mesh follow the bones (as the armature is a parent of the object). The rigid bones add-on also uses it: it just creates a rigid-body cube around each bone and parent bone to the cube, such that the cube is affected by gravity, and bones are moving because their parents do.</p>
<p>Just adding the bones and the cubes around them doesn’t let the engine know that you want them to move together: Blender sees each cube as a separate rigid body and doesn’t try to keep them close to each other.
To fix this, one needs to use <a href="https://docs.blender.org/manual/en/latest/physics/rigid_body/constraints/introduction.html">rigid body constraint</a>s.
It creates an empty (an object without mass or volume) between two cubes that constrain the movement between them. For example, one can make moving the foot backward impossible using a constraint.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<p>When put together, the physics simulation moves the cubes, following the constraints, the cubes are moving the bones, and the bones are moving the mesh (skin).</p>
<figure>
<figure>
<img src="../images/blender/working.gif" style="width:30.0%" alt="Somewhat working rigid body character" />
<figcaption aria-hidden="true">Somewhat working rigid body character</figcaption>
</figure>
<figcaption>
Final character animation I got. There are still things that could be fixed (I didn’t set all of the constraints correctly), but I think I got the gist of how the physics engine works.
</figcaption>
</figure>
<h2 id="summary">Summary</h2>
<p>I found the process of learning Blender from Andrew’s tutorials really efficient, and Blender itself to be a good tool for creating the graphics that I was doing.
Even when the final product is a 2D image, the 3D model is a natural “intermediate representation” where you don’t have to worry about choosing perspective, lightning, or point of view, as you can always change them at the end.</p>
<p>I found the Blender a very effective tool for manipulating the models. It has a lot of parts that I only barely scratched the surface of, so I will surely be using it and learning more of it in the future.</p>
<section class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>Explaining various BRDF properties is a topic for another blog post :)<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>and while doing so, I discovered another great source of assets: <a href="mixamo.com">mixamo</a> with dozens of characters and their animations<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p><a href="https://www.youtube.com/watch?v=KUmIKWVrHZU&amp;ab_channel=BSWM">Here</a> is one tutorial that was nicely explaining creating gravity-aware characters.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
]]></description>
    <pubDate>Fri, 22 Oct 2021 00:00:00 UT</pubDate>
    <guid>https://sygnowski.ml/posts/2021-10-22-blender.html</guid>
    <dc:creator>Jakub Sygnowski</dc:creator>
</item>
<item>
    <title>T.I.M.E stories expansion graphic workflow</title>
    <link>https://sygnowski.ml/posts/2021-09-29-graphic-workflow.html</link>
    <description><![CDATA[<div class="info">
    Posted on September 29, 2021
    
</div>
<h1>T.I.M.E stories expansion graphic workflow</h1>
<p>At the end of last year, I made a fan <a href="https://sygnowski.ml/posts/2020-12-07-auschwitz.html">time stories scenario</a>. Concretely, only the “story” part was finished then, with all the graphics missing. This year, together with Michalina, I was working on and off on adding the custom images to the game. Being relatively new to graphic design, I tried a couple of different styles and workflows for creating the content.</p>
<h2 id="stylesworkflows">Styles/workflows</h2>
<h3 id="first-plans">First plans</h3>
<p>Initially, I planned to do a simple lineart drawing + flat colors to make it easy to create a lot of images. The first image I created used this technique and took around 8h to draw (+an hour or two for adding people on top).</p>
<figure>
<a href="../images/graphic_workflow/generic_gate.png"><img src="../images/graphic_workflow/generic_gate.png" style="width:30.0%" alt="Drawing of the gate camp" /></a>
<figcaption>
The image of the gate camp used in the game. Despite being simple, it still took a while to finish.
</figcaption>
</figure>
<h3 id="going-fancy">Going fancy</h3>
<p>Then, I decided to see how close I could get to being photorealistic. I planned to make the first location (Appellplatz) through a collage of drawing, applying textures, and manipulating other images from the internet. I hoped to spend more time on this location to get it really nice and choose a final workflow for other pictures based on how long different parts of this process took and how much they contributed to the final look.</p>
<p>I ended up using many images from <a href="collections.ushmm.org">United State Holocaust Museum</a> who have photos of concentration camp clothing available online, some textures from <a href="https://polyhaven.com">Poly Haven</a>, and lots of photo manipulation in Procreate. Initially, I was planning to apply <a href="https://en.wikipedia.org/wiki/Neural_Style_Transfer">style transfer</a> on top of the image to give it a more painterly look and hide the anatomy/perspective problems that I created (what ended up to be an overly optimistic goal).</p>
<figure>
<a href="../images/graphic_workflow/appellplatz_full.png"><img src="../images/graphic_workflow/appellplatz_full.png" style="width:95.0%" alt="Full panorama of the first location" /></a>
<figcaption>
As you can see, the results (before applying style transfer) aren’t great, despite taking 28h on this image alone.
</figcaption>
</figure>
<h3 id="style-transfer">Style transfer</h3>
<p>I played a bit with various methods for doing style transfer to fix the image that I created. As I didn’t want the stylization to spin into a 5-year research project,<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> I looked only at the tools that are easy to use (in particular don’t require training a neural network).</p>
<h4 id="neural-style-transfer">Neural Style Transfer</h4>
<p>The classic<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> method for doing style transfer involves optimizing a (content) image using gradient descent to keep late activations of some CNN the same while changing the early ones to match the ones of another (style) image.</p>
<p>The results were not great: I tried <a href="https://deepart.io">deepart.io</a> and <a href="https://reiinakano.com/arbitrary-image-stylization-tfjs/">Arbitrary Style Transfer in the Browser</a>, but the style application was too strong (even on the lowest setting), making it too hard to read the details of the images.</p>
<figure>
<img src="../images/graphic_workflow/style_transfer1.png" style="width:95.0%" alt="Result of applying neural style transfer" />
<img src="../images/graphic_workflow/style_transfer2.jpg" style="width:95.0%" alt="Another result of applying neural style transfer" />
<figcaption>
Results of applying style transfer.
</figcaption>
</figure>
<p>Another problem with them (and all the other browser-based tools that I tried) was that they support only small images, while I wanted to keep everything in high pixel density.</p>
<h4 id="nvidia-tools">NVIDIA tools</h4>
<p>NVIDIA has <a href="https://www.nvidia.com/en-us/research/ai-demos/">some of their research</a> available interactively through the browser. I tried two tools that could be useful for style manipulation:</p>
<ol type="1">
<li><a href="http://nvidia-research-mingyuliu.com/gaugan/">GauGAN</a>, where the user draws a rough segmentation map with different colors corresponding to different textures (eg. you can paint “trees” or “rock”), and the model generates an image based on this map and a reference landscape image. Low details, not the workflow I wanted, no people.</li>
<li><a href="https://www.nvidia.com/research/inpainting">Inpainting</a>, where the user deletes parts of the image, and the model tries to fill the gaps. I tried deleting parts of the image with broken anatomy, but the inpainted results blended to the background more than fixing the details.</li>
</ol>
<figure>
<img src="../images/graphic_workflow/inpainting_mask2.png" style="width:40.0%" alt="Mask to inpaint" />
<img src="../images/graphic_workflow/inpainting_result.png" style="width:40.0%" alt="Result of the inpainting" />
<figcaption>
An inpainted picture with hands masked out. The model doesn’t draw new hands but rather tries to smooth the surrounding background.
</figcaption>
</figure>
<h4 id="gmic">G’MIC</h4>
<p><a href="https://gmic.eu">G’MIC</a> is an extensive, open-source library for processing images. It is included in many popular graphics software (eg. GIMP, photoshop, Krita) as a plugin and also has <a href="https://gmicol.greyc.fr">an online version</a>.</p>
<p>Among others, it has many filters available for postprocessing of the images. I tried a couple of them.</p>
<figure>
<img src="../images/graphic_workflow/appellplatz_illustration.png" style="width:95.0%" alt="First panorama after applying an “illustration look” filter." />
<figcaption aria-hidden="true">First panorama after applying an “illustration look” filter.</figcaption>
</figure>
<p>The G’MIC filters deliver on their promise: they change the style of the images, with some of them making them more painterly. At the same time, as they are hard-coded (not learned) they can’t focus on fixing the places where the geometry was wrong.</p>
<h4 id="style-transfer-results">Style transfer results</h4>
<p>The tools I tried couldn’t help me in my workflow: they weren’t able to fix what I did wrong when creating the images.</p>
<p>I believe that the necessary research for this to happen has already been done: it should be possible to take a loss <span class="math inline">\(L(x)\)</span> describing “how likely a given image <span class="math inline">\(x\)</span> comes from a dataset <span class="math inline">\(D\)</span>” (taken from a GAN discriminator or a density estimation model) and update a given image using gradients <span class="math inline">\(\frac{\text{d}L}{\text{d}x}\)</span>.</p>
<p>A technique that is closest to this that I know of is <a href="https://en.wikipedia.org/wiki/DeepDream">DeepDream</a>. It optimizes the features of the image to facilitate classifying it with one of the classes instead of “matching the dataset” though, which causes a trippy, overly-objectified effect.</p>
<figure>
<a href="../images/graphic_workflow/deep_dream.jpg"><img src="../images/graphic_workflow/deep_dream.jpg" style="width:95.0%" alt="Result of the deep dream transform" /></a>
<figcaption>
An effect of applying DeepDream to my image.
</figcaption>
</figure>
<p>As I couldn’t find a successful style transfer tool, I went with the raw images in the end.</p>
<h3 id="going-3d">Going 3D</h3>
<p>After spending way too long on the Appellplatz image despite the results not being great, I made a step back and looked for different options. I decided to try to use Blender (a 3D-modelling tool) to aid with the image creation. I would be modeling the backgrounds in Blender, rendering them, and then drawing the people on top of the renders.</p>
<p>After learning the basics of Blender<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> with <a href="https://www.youtube.com/watch?v=TPrnSACiTJ4&amp;ab_channel=BlenderGuru">the donut tutorial</a>, it was relatively quick to prepare models that are good enough.</p>
<script type="module" src="https://unpkg.com/@google/model-viewer/dist/model-viewer.min.js"></script>
<figure>
<model-viewer style="width: 80%; height: 300px; margin-left: auto; margin-right: auto;"
  field-of-view='20deg'
camera-controls interaction-prompt="when-focused"
  alt="A 3D model of a barrack." src="../data/graphic_workflow/barrack.gltf">
</model-viewer>
<figcaption>
The 3D model of a camp barrack that I created with Blender (using some free assets).
</figcaption>
</figure>
<p>The existence of a huge library of free (mostly CC attribution) 3D assets on <a href="https://sketchfab.com">sketchfab</a> made modeling easier, but, most of the time, I was creating cubes and adding free textures (from PolyHaven again) on my own.</p>
<p>A great feature of the 3D workflow was that I didn’t need to fix the shot/perspective at the very beginning of a drawing. Instead, after the model was mostly done, I moved the camera/changed its focal length to put the characters in the correct part of the image.</p>
<p>While making the first barrack (and learning Blender) took some time, I could then make the next scene in only a couple of hours, which was a significant improvement over the hand-drawn/photo-manipulation workflows.</p>
<figure>
<img src="../images/graphic_workflow/exit.png" style="width:70.0%" alt="Creating this scene (using pre-made assets) took me only an hour or two!" />
<figcaption aria-hidden="true">Creating this scene (using pre-made assets) took me only an hour or two!</figcaption>
</figure>
<h2 id="graphics-workflow">Graphics workflow</h2>
<p>Apart from creating the graphics themselves, I needed a mechanism for adding them to the cards. Initially, I had each card available as an svg, which I manually exported to png via Inkscape’s UI. Then, I would use a script to order the cards and generate a pdf.</p>
<p>One problem with that workflow is that whenever one multi-card panorama changes, it is necessary to cut it into pieces again, add each of them to the corresponding Inkscape file (a.svg-h.svg), and then export them again.</p>
<p>What I ended up doing is, frankly, what I should have started with: keeping only a single svg file for a location and exporting it card by card using Inkscape’s command-line interface.</p>
<h3 id="export-script">Export script</h3>
<p>Here is a piece of the script that I ended up using:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># bash numbers tables starting from 0, but seq starts with 1</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="kw">`</span><span class="fu">seq</span> <span class="va">$((${</span><span class="op">#</span><span class="va">CARDS</span><span class="op">[@]</span><span class="va">}</span> <span class="op">-</span> <span class="dv">1</span><span class="va">))</span><span class="kw">`;</span> <span class="cf">do</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>  <span class="ex">inkscape</span> <span class="at">--export-filename</span><span class="op">=</span><span class="va">${CARDS</span><span class="op">[</span>i<span class="op">]</span><span class="va">}</span>.png <span class="dt">\</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">--export-area</span><span class="op">=</span><span class="va">$(($WIDTH</span><span class="op">*</span>(<span class="va">$i</span><span class="op">-</span><span class="dv">1</span>)<span class="va">))</span>:0:<span class="va">$(($WIDTH</span><span class="op">*</span><span class="va">$i))</span>:<span class="va">$HEIGHT</span> <span class="dt">\</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">--export-dpi</span><span class="op">=</span>60 full.svg</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="cf">done</span></span></code></pre></div>
<p>One problem with that was setting the correct <code>$WIDTH</code>: as my svg file unit was a millimeter, but the command-line interface uses “user unit” (which, as I learned later, corresponds to one pixel under 96dpi, independently of the chosen export-dpi), I eyeballed/binary searched the conversion rate.</p>
<h3 id="guides">Guides</h3>
<p>Guides are the axis-aligned lines that help to arrange the objects on the canvas. In Inkscape, after double-clicking a guide, one can set its position (including the origin, which is a point on the line) numerically.</p>
<p>I used them to accurately scale the external panoramas pasted to the Inkscape files: I would have two guides, with origins set to the opposite corners of the page, and scale the image while snapping to the guide origin.</p>
<h3 id="arranging-objects-on-the-cards">Arranging objects on the cards</h3>
<p>Another issue with multi-card Inkscape files I encountered was centering a text/object on each card separately.</p>
<figure>
<a href="../images/graphic_workflow/appellplatz_full.png"><img src="../images/graphic_workflow/appellplatz_full.png" style="width:95.0%" alt="Full panorama of the first location" /></a>
<figcaption>
I wanted each title (eg. “Appellplatz - C”) to be on the center of the respective card.
</figcaption>
</figure>
<p>Inkscape has an “Align and Distribute” tool to do transformations like this, but you can only either:</p>
<ul>
<li>Center all objects wrt. page, object, or selection, or</li>
<li>Distribute all objects equidistantly (in some sense) among themselves.</li>
</ul>
<p>As it isn’t possible to express the centering I wanted directly, I had to get creative. What I ended up doing is:</p>
<ol type="1">
<li>I created a thin line at both ends of the page. Guides helped with placing them precisely.</li>
<li>I placed the objects to be centered on each card on separate cards (inaccurately)</li>
<li>I added a thin line somewhere between each card.</li>
<li>I selected all lines and objects and turned on “Distribute centers equidistantly horizontally”.</li>
</ol>
<figure>
<a href="../images/graphic_workflow/inkscape_full.png"><img src="../images/graphic_workflow/inkscape_full.png" style="width:70.0%" alt="Screenshot from Inkscape" /></a>
<figcaption>
All objects are selected before distributing them.
</figcaption>
</figure>
<p>Note that using just the lines at the ends (to skip point 3 above) doesn’t work, as it would put the same distance between the end of the last text and the border of the page as between the texts themselves, where the latter one should be twice as big (as it covers space on two cards).</p>
<h3 id="flow-text">Flow text</h3>
<p>There are two types of objects for storing text in Inkscape:</p>
<ul>
<li>regular text,</li>
<li>flowed text.</li>
</ul>
<p>Flowed text, apart from paths describing the letters, contains a blue “bounding box” with the area that the text should fit into. It is more convenient to work with, as you don’t need to change newlines every time you modify the text.</p>
<p>I found two ways of getting a flowed text in Inkscape:</p>
<ol type="1">
<li>To select an area that will become the bounding box after choosing the text tool (as opposed to clicking on the canvas once, what creates regular text).</li>
<li>To select a (regular) text object and then another object with shift, and choose Text &gt; Flow into frame.</li>
</ol>
<p>Initially, I thought I will be using the second option with semi-transparent text boxes to improve visibility, but the Flow into frame option doesn’t allow to set margins between the frame and the text.</p>
<p>I found a workaround for adding the margins, involving creating another invisible box inside, but I found the first option with manually specifying the bounding box easier to use.</p>
<h3 id="simple-card-generation">Simple card generation</h3>
<p>There are many similar cards in a Time stories deck: item cards (differ only by the number), all “A” cards look basically the same, etc. Instead of creating them manually, one can create a template and generate multiple svgs from it, filling the template with the data.</p>
<p>There is a small library to do this: <a href="https://github.com/mbr/svglue">svglue</a>. To create a template, one adds a <code>template-id</code> attribute to a <code>tspan</code> element in xml editor in Inkscape where the text will be substituted. Then, it’s possible to do the following:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>template <span class="op">=</span> svglue.load(<span class="bu">file</span><span class="op">=</span><span class="st">&quot;template.svg&quot;</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>template.set_text(<span class="st">&quot;my_template_id&quot;</span>, <span class="st">&quot;new text&quot;</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>content <span class="op">=</span> template.<span class="fu">__str__</span>().decode(<span class="st">&#39;utf8&#39;</span>)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">&quot;output.svg&quot;</span>, <span class="st">&quot;w&quot;</span>) <span class="im">as</span> f:</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    f.write(content)</span></code></pre></div>
<p>The magic with <code>__str__()</code> is needed as the library is quite unsupported and doesn’t handle python3 strings well. Other than that (and finding a separate <code>set_flowtext</code> for flowed text), it worked really well and saved me lots of time.</p>
<figure>
<a href="../images/graphic_workflow/a.png"><img src="../images/graphic_workflow/a.png" style="width:30.0%" alt="Example A card" /></a>
<figcaption>
Example card generated using the template: other cards would only change the location name and the list of cards at the bottom.
</figcaption>
</figure>
<h1 id="final-words">Final words</h1>
<p>Using the workflow above, I did graphics for the back cards of all but two locations of the game. Michalina agreed to draw the remaining two. I then generated a “final” version of the cards and created a <a href="https://boardgamegeek.com/boardgameexpansion/348281/escape-auschwitz-fan-expansion-time-stories">BGG page for the expansion</a>. Ideally, I would cut pieces of the graphics (or even draw new ones) to put on the front sides as well, but after spending so much time on this, I’ll leave it be.</p>
<section class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>Obligatory reference to <a href="https://xkcd.com/1425/">the “Tasks” xkcd</a>.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>Created in 2015, so fairly old for the contemporary standards.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p>which deserves another blog post.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
]]></description>
    <pubDate>Wed, 29 Sep 2021 00:00:00 UT</pubDate>
    <guid>https://sygnowski.ml/posts/2021-09-29-graphic-workflow.html</guid>
    <dc:creator>Jakub Sygnowski</dc:creator>
</item>
<item>
    <title>Life after MIM UW analysis: making of</title>
    <link>https://sygnowski.ml/posts/2021-09-06-alma-survey.html</link>
    <description><![CDATA[<div class="info">
    Posted on September  6, 2021
    
</div>
<h1>Life after MIM UW analysis: making of</h1>
<p>I did a Masters in Computer Science at <a href="https://www.mimuw.edu.pl/en">MIM UW</a> in Warsaw, Poland. Many of my fellow alumni have successful careers, both in Poland and abroad. I wanted to know whether this was a statistical phenomenon or just a bias based on the group of my friends. To do so, I ran a survey among MIM graduates to know their fates better.</p>
<h2 id="my-anecdata">My anecdata</h2>
<p>When my parents (who live in a small town) heard I decided to study computer science, they expected me to become a school teacher or a shopkeeper.</p>
<p>It’s no wonder that I was surprised when seeing other students interning in big companies abroad or getting lucrative jobs after graduation. At the time, it seemed like everyone was getting these internships here or there, with most of the students moving to Silicon Valley for the summer.</p>
<p>During university, I once had a chat with a friend about why the career prospects of graduates aren’t advertised wider so that high-school students know what to expect from a degree. He suggested that, similarly to the surveys about classes we took, it would be enough to ask the alumni about their life after graduation in a survey.</p>
<p>Recently, in the Facebook group of the faculty, there was a discussion about the earnings of the alumni. It reminded me of the chat I had before, and I decided to run the survey we talked about to get to know the data better.</p>
<h2 id="ela">ELA</h2>
<p>It is in the best interest of the country/government to provide up-to-date information about different university degrees to allow students-to-be to make an informed decision about their future studies. Because of that, someone created a project called “Ekonomiczne losy absolwentów” (ELA), which provides statistics of the incomes of the alumni of various degrees taught in Poland.</p>
<p>The data comes from a governmental database registering social security contributions, so all (outside of grey economy) workers’ incomes are taken into account. One can see the information <a href="https://ela.nauka.gov.pl/en">here</a>.</p>
<p>There is a catch, though. As it’s a Polish database, it only includes incomes earned in Poland; data of someone who left the country after finishing the degree would not be captured there.</p>
<p>It will likely introduce a bias in the data: people working abroad usually earn higher salaries than those living in Poland. Looking at the data of my year (computer science, 2016), around half of the alumni are present in the data ELA uses.</p>
<h2 id="the-survey">The survey</h2>
<p>To get better coverage of the subjects, I decided to prepare a survey among the alumni. I created an anonymous form using Google Forms and sent the request to fill the survey on the Facebook group of the faculty and the alumni email group.</p>
<p>The survey accepted responses from 17th to 21st July. It asked questions about the salary and about other criteria like gender, size of the city where one was living in, or the number of hours one worked weekly.</p>
<h2 id="problems-with-the-analysis">Problems with the analysis</h2>
<h3 id="measuring-salaries">Measuring salaries</h3>
<p>The biggest problem I encountered while analyzing the data was how to compare the salaries earned in different countries.</p>
<p>The survey asked about the gross salary in the local currency of the country where the person lives. While converting it to a common currency isn’t that big of an issue, averaging the salaries between different countries may give an incomplete view due to varying living costs between countries.</p>
<p>My initial plan was to calculate, for each person, what percentage of the median salary in the given country it earns and then average these. I would get the information that, on average, MIM alumni get 175% of the median salary of their country (fabricated data).</p>
<p>I realized, though, that it’s surprisingly hard to find the data about the median income for lots of countries. The statistic that is more commonly available is the disposable (so net, aka after-tax) income. The salary is usually equivalised for a household (a fancy word meaning that the fact that costs don’t grow linearly with the size of the household is taken into account).</p>
<p>As using median incomes didn’t work out, I found a different concept to help me with cross-country income comparisons: <a href="https://en.wikipedia.org/wiki/Purchasing_power_parity">purchasing power parity</a>.</p>
<p>It is a way for modifying the exchange rates between currencies to take into account differing prices such that a unit of a currency can buy the same amount of goods (from a fixed basket). For example, if 2 Polish złotys can buy one bread in Poland, and 1 Swiss franc can buy one bread in Switzerland, the PPP exchange rate would be 2 PLN = 1 CHF, independently of the actual (FOREX) exchange rate between these currencies. In other words, it allows us to express the salaries as “the amount of standardized goods” that one could buy with them and compare these instead.</p>
<p>The exchange rates of national currencies with PPP are <a href="https://appsso.eurostat.ec.europa.eu/nui/submitViewTableAction.do">available from Eurostat</a> for a wide range of countries.</p>
<p>As with any economic model, PPP comes with assumptions. In our case, when we use PPP to help us average salaries, we assume that the worker spends the money in the same country as the one he earns it in. If a person earns in a high-income country but spends in a low-cost one (as is common with immigrants returning to Poland from Western countries), looking at the earned salary via the lens of goods available in the high-cost country doesn’t make sense, as the worker will be buying these goods cheaper than PPP would estimate.</p>
<h3 id="monthly-salary">Monthly salary</h3>
<p>In Poland, it’s common to express salary per month. As some of the salaries reported in the survey were significantly below the minimum salary, I suspect these were indeed monthly and not annual salaries.</p>
<p>On the other hand, some of the entries included even lower numbers, below a thousand PLN, suggesting that the salary was given in the thousands, not nominally.</p>
<p>Initially, I planned to allow only numbers &gt;= 10000 in the form to avoid these problems. It turned out, though, that:</p>
<ol type="a">
<li>it’s hard to make it work with any currency</li>
<li>when I set the limit for a short time, some people who don’t work still wanted to put a number here (even though the question was optional). To do so, they were putting the lowest possible number (10000). Using it in the analysis would introduce a bias.</li>
</ol>
<p>In the end, I allowed any numbers and filtered out the ones lower than the minimum salary.</p>
<h3 id="year-2019">Year 2019</h3>
<p>As I was afraid that the results for the year 2020 would be skewed due to the pandemic (and people, like myself, moving to a different country or getting to work from home), I asked about the salary information from 2019.</p>
<p>Apart from the salary, the survey asked many other statistical questions, like highest degree finished, which I later used to aggregate data.</p>
<figure>
<img src="../images/alma/education_comp.png" style="width:80.0%" alt="Aggregation of salary based on the highest degree received." />
<figcaption aria-hidden="true">Aggregation of salary based on the highest degree received.</figcaption>
</figure>
<p>Overall, the decision to use 2019 seems a reasonable one. I’m afraid though that people who finished a degree between 2019 and now answered with their salary from before graduation, which would confuse the aggregates a bit.</p>
<h3 id="distributing-the-survey">Distributing the survey</h3>
<p>I sent the survey to the student group on Facebook and an alumni email group on 17th July. While people started filling it using the Facebook link immediately, it turned out that the alumni group is censored, and the censor doesn’t work on the weekends, so the email reached the group only on Monday.</p>
<p>I decided to close the survey a couple of days later as I didn’t get as many responses anymore. Because of that, I got a complaint that the survey closed too soon and would be biased towards people who don’t have a life outside of the internet ;)</p>
<h2 id="final-words">Final words</h2>
<p>I was positively surprised by the number of responses I got: the survey recorded 203 entries.
I prepared the analysis of the results (in Polish) and made it available <a href="https://sygnowski.ml/data/alma/writeup.pdf">here</a>. The code I used to analyze the data is <a href="https://github.com/sygi/alma">here</a>.</p>
<p>Unsurprisingly, the data I gathered presented the view of the MIM UW alumni that’s somewhere in-between my biased anecdata (where most of the people move abroad and earn a lot) and the data from ELA. Overall, the data looks reasonable.</p>
<p>It may be worthwhile to repeat the survey in a couple of years.</p>
]]></description>
    <pubDate>Mon, 06 Sep 2021 00:00:00 UT</pubDate>
    <guid>https://sygnowski.ml/posts/2021-09-06-alma-survey.html</guid>
    <dc:creator>Jakub Sygnowski</dc:creator>
</item>
<item>
    <title>Tax residency for fun and profit</title>
    <link>https://sygnowski.ml/posts/2021-07-06-tax-residency.html</link>
    <description><![CDATA[<div class="info">
    Posted on July  6, 2021
    
</div>
<h1>Tax residency for fun and profit</h1>
<p>Tax rules, which describe what portion of one’s profit needs to pay to the government, are famously complex. If they involve multiple countries, they are even more confusing. Having moved to a different country for the 2020 pandemic, I studied the regulations in detail and decided to write down my observations for the future.</p>
<h2 id="rule-of-law-101">Rule of law 101</h2>
<p>One can summarize the general process of setting up tax law in a simple way (even though the rules themselves may be complex):</p>
<ol type="1">
<li>Parliament votes on legislative acts which decide on details of who, when, and how much tax should pay.</li>
<li>The act is published (“promulgated”) in an official bulletin and becomes law.</li>
<li>Courts resolve the disagreement between the taxpayer and tax authorities, and civil officers (bailiffs, police, etc.) enforce compliance.</li>
</ol>
<p><a href="https://en.wikipedia.org/wiki/Sovereign_state">Sovereignity</a> of the country guarantees that the law enacted in this way usually applies only on its territory. For example, it’s not possible for folks in Bundestag to decide on what taxes should residents of Sydney pay.</p>
<h2 id="double-taxation-agreements">Double taxation agreements</h2>
<p>Limiting the taxes to the territory of a state doesn’t solve all the international taxation issues.</p>
<p>For one, it shouldn’t be possible to evade taxes just by making repeated trips to a tax haven (area of low rates of taxation) on the day one’s salary is paid. On the other hand, a country doesn’t want to impose regular income taxes on tourists or attendees of a business meeting even if they happen to be physically present under its jurisdiction on their pay date.</p>
<p>As taxation systems (and the political power) differ significantly between countries, it feels impossible to set a single, precise set of tax rules that all countries could agree to. For this to happen, one would need to consider all possible taxes and exact situations when they should or shouldn’t be levied.</p>
<p>Instead of that, countries opt for bilateral treaties where pairs of countries (e.g. Germany and France) agree on which country has the right to tax different types of income. Such a treaty is called a <em>Double Taxation Agreement</em> (DTA).</p>
<h2 id="oecd-model-tax-convention">OECD model tax convention</h2>
<p>Even though getting all countries to agree on a common set of rules in a binding treaty feels like a lost cause, there are some general rules that countries want from their DTAs.</p>
<p>To simplify the process of writing the tax agreements with others, several Western countries, grouped in an intergovernmental organization OECD<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>, prepared a <a href="https://www.oecd.org/tax/treaties/1914467.pdf">model tax convention</a> which forms a common ground, helping them to start drafting the treaties.</p>
<figure>
<img src="../images/tax_residency/oecd.png" style="width:80.0%" alt="Map of OECD countries." />
<figcaption aria-hidden="true">Map of OECD countries.</figcaption>
</figure>
<p>The tax convention tries to formalize the idea of a person being liable for tax only in one country: the one where they live. While the details on the rates, procedures, or exemption methods differ between DTAs, the principles are the same in the DTAs of OECD countries. Let’s review them below.</p>
<h3 id="tax-residency">Tax residency</h3>
<p>The crucial concept defined in the model tax convention is <em>tax residency</em>. It’s meant to be a unique country where a given person generally pays taxes for their worldwide income.</p>
<p>The content of the article of the model convention defining tax residency:</p>
<details>
<summary>
Article 4: Residence
</summary>
<ol type="1">
<li><p>For the purposes of this Convention, the term “resident of a Contracting State” means any person who, under the laws of that State, is liable to tax therein by reason of his domicile, residence, place of management or any other criterion of a similar nature, and also includes that State and any political subdivision or local authority thereof. This term, however, does not include any person who is liable to tax in that State in respect only of income from sources in that State or capital situated therein.</p></li>
<li><p>Where by reason of the provisions of paragraph 1 an individual is a resident of both Contracting States, then his status shall be determined as follows:</p>
<ol type="a">
<li>he shall be deemed to be a resident only of the State in which he has a permanent home available to him; if he has a permanent home available to him in both States, he shall be deemed to be a resident only of the State with which his personal and economic relations are closer (centre of vital interests);</li>
<li>if the State in which he has his centre of vital interests cannot be determined, or if he has not a permanent home available to him in either State, he shall be deemed to be a resident only of the State in which he has an habitual abode;</li>
<li>if he has an habitual abode in both States or in neither of them, he shall be deemed to be a resident only of the State of which he is a national;</li>
<li>if he is a national of both States or of neither of them, the competent authorities of the Contracting States shall settle the question by mutual agreement.</li>
</ol></li>
<li><p>Where by reason of the provisions of paragraph 1 a person other than an individual is a resident of both Contracting States, then it shall be deemed to be a resident only of the State in which its place of effective management is situated.</p></li>
</ol>
</details>
<p>In simple words, if both signatories of the treaty want a given person to pay tax on its worldwide income, there is a set of criteria to review to determine in which (single) country the person will be considered a tax resident (likely giving it the right to tax their worldwide income).</p>
<h4 id="country-specific-tax-residency-criteria">Country-specific tax residency criteria</h4>
<p>Thanks to the criteria overriding national laws on tax residency, countries can set the rules freely in their local legislation. Even if the country-specific rules would overlap, making a single person “tax resident” in multiple countries based on their national law, the DTA will describe how to choose a single country for tax residency.</p>
<p>A commonly used criterium for tax residency says that a person who resides in a given country for more than 183 days in a given tax year is tax resident there. If every country used only this provision, there wouldn’t be any issues with multiple countries claiming overlapping tax residency. It’s often the case though that, apart from 183 days, there is an additional criterium saying that a person who has a house, family, or a “center of vital interests” in a country, would be considered tax resident there, even if they spent most of the year abroad.</p>
<p>There are also some countries where even a short stay triggers tax residency; for example, in Switzerland, a working individual becomes tax resident after 30 days of stay. The overlapping tax residency may thus happen, and the use of DTA criteria may be necessary to establish a single country of tax residency.</p>
<h4 id="no-tax-residency">No tax residency</h4>
<p>As countries decide on their tax residency rules without much regard to other countries’ regulations<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>, it may as well happen that a given person is not considered a tax resident in any particular country based on its laws.</p>
<p>For example, a person moving between 3 countries without his “center of vital interests” and spending less than 183 days in any of them due to frequent business visits may have no tax residency.</p>
<p>Does it mean such a person doesn’t have to pay any taxes? Of course not :) The countries often have rules saying that non-residents pay taxes on the portion of the income arising from that country. If a person is tax resident elsewhere, they may use the DTA to override the “non-resident paying local taxes” rule. If they are not resident in any country and are non-resident in multiple countries, they would be paying local taxes in each of them. They won’t be able to use DTAs in that case, as the DTA only clarifies the criteria for overlapping tax residency, i.e. multiple countries wanting to tax a person on the worldwide income.</p>
<h4 id="changes-within-the-tax-year">Changes within the (tax) year</h4>
<p>The country-specific rules for tax residency are often expressed in terms of the whole tax year, e.g. “a person is tax resident for the whole tax year if they spend at least 183 days during that year”.</p>
<p>What happens when someone’s situation changes in the middle of a year, by buying a home or permanently moving to another country in June?</p>
<p>The DTA solution is simple: it is not concerned about tax years at all (which may run differently in the two countries, e.g. in the UK the tax year starts April 6th). Instead, all of its provisions apply at any given moment; if someone’s circumstances change on June 15th, they may be considered a tax resident of one country until June 14th and the resident of another from June 15th onwards. It would give each country the right to tax only the portion of the income earned in the corresponding part of the year. Then, the countries typically apply their regular tax brackets only to the “home” part of the tax year.</p>
<h3 id="income-from-employment">Income from employment</h3>
<p>Most articles of the model tax convention explain which country possesses the taxation right for a particular type of income. Let’s take a look at article 15, describing income from employment.</p>
<details>
<summary>
Article 15: Income from employment
</summary>
<ol type="1">
<li>Subject to the provisions of Articles 16, 18 and 19, salaries, wages and other similar remuneration derived by a resident of a Contracting State in respect of an employment shall be taxable only in that State unless the employment is exercised in the other Contracting State. If the employment is so exercised, such remuneration as is derived therefrom may be taxed in that other State.</li>
<li>Notwithstanding the provisions of paragraph 1, remuneration derived by a resident of a Contracting State in respect of an employment exercised in the other Contracting State shall be taxable only in the first-mentioned State if:
<ol type="a">
<li>the recipient is present in the other State for a period or periods not exceeding in the aggregate 183 days in any twelve month period commencing or ending in the fiscal year concerned, and</li>
<li>the remuneration is paid by, or on behalf of, an employer who is not a resident of the other State, and</li>
<li>the remuneration is not borne by a permanent establishment which the employer has in the other State.</li>
</ol></li>
<li>Notwithstanding the preceding provisions of this Article, remuneration derived in respect of an employment exercised aboard a ship or aircraft operated in international traffic, or aboard a boat engaged in inland waterways transport, may be taxed in the Contracting State in which the place of effective management of the enterprise is situated.</li>
</ol>
</details>
<p>The article presents a general rule, saying that salary should be taxed in the country of tax residency unless the work is happening in the other country when the salary can be taxed by both countries.</p>
<p>It also adds an exception that even if the work happens in a country where the worker is non-resident (according to the treaty), if the worker is present there at most 183 days in any period of 12 months (and some other conditions are satisfied), only the residence country has the taxation rights to the salary.</p>
<p>This exception is important for people spending short amounts of time in other countries, for example attending conferences or business meetings: without it, every short-term visit to a different country would give taxation rights to the income earned while in the other country.</p>
<h3 id="methods-of-eliminating-double-taxation">Methods of eliminating double taxation</h3>
<p>What happens when this exception cannot be applied, and the salary can be taxed by both countries? It may occur when the employer sends the worker to supervise a project abroad for a year.</p>
<p>As allowing both countries to tax the same income would be harmful to the worker and discourage international collaboration, countries agree in a DTA a method for exempting the income taxed in the other country from taxation to some level.</p>
<p>Typically, the residence country (the one having the right to tax the worldwide income) offers the taxpayer an exemption to bring the total level of taxation somewhere close to the maximum level that the person would suffer in any given country.</p>
<p>It can be realized in a couple of different ways: using tax credits, exemption with progression, or proportional exemption.</p>
<p>Let’s assume the residence country taxes has two tax brackets: 10% for the income until 100 units and 20% afterward, and the non-residence country taxes everything at 15%, and see an example of how the different methods work out in practice.</p>
<h4 id="tax-credits">Tax credits</h4>
<p>With tax credits, the residence country decreases the tax due by the amount of tax the worker pays in the other country.</p>
<div class="table-wrapper">
<table>
<thead>
<tr class="header">
<th>Kind</th>
<th>Residence</th>
<th>Non-residence</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Income</td>
<td>100</td>
<td>20</td>
</tr>
<tr class="even">
<td>Tax</td>
<td>10 + 4</td>
<td>3</td>
</tr>
<tr class="odd">
<td>Credit</td>
<td>3</td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>The residence country taxes the worldwide income (120), with the first 100 taxed 10% (10), and the next 20 at 20% (4), and decreases the tax by the amount paid in non-residence country (3).</p>
<h4 id="exemption-with-progression">Exemption with progression</h4>
<p>When the country allows exemption with progression, instead of decreasing the tax directly, they only tax the income earned in the residence country, but the tax brackets take into account the income earned abroad. Let’s see the same example:</p>
<div class="table-wrapper">
<table>
<thead>
<tr class="header">
<th>Kind</th>
<th>Residence</th>
<th>Non-residence</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Income</td>
<td>100</td>
<td>20</td>
</tr>
<tr class="even">
<td>Tax</td>
<td>8 + 4</td>
<td>3</td>
</tr>
</tbody>
</table>
</div>
<p>The residence country taxes the first 80 in the first (10%) bracket (as 20 of the first-bracket allowance was used up by the foreign income). The remaining 20 is taxed in the second bracket (20%, 4).</p>
<h4 id="proportional-exemption">Proportional exemption</h4>
<p>The proportional exemption has some elements of the previous two methods of eliminating double taxation. Here, we first calculate the total tax and exemption as in the tax credit example. Then, we calculate the proportion of the paid tax that corresponds to the income earned abroad. If this proportion is higher than the tax credit, only that proportion is allowed as a credit.</p>
<div class="table-wrapper">
<table>
<thead>
<tr class="header">
<th>Kind</th>
<th>Residence</th>
<th>Non-residence</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Income</td>
<td>100</td>
<td>20</td>
</tr>
<tr class="even">
<td>Tax</td>
<td>10 + 4</td>
<td>3</td>
</tr>
<tr class="odd">
<td>Credit</td>
<td>2.33</td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>Proportion of the tax for the income abroad: 20/120 * 14 = 2.33. In this case, the tax due in the residence country will be 10 + 4 - 2.33.</p>
<p>One needs to remember that these three kinds of exemptions are just examples, and the countries are free to agree on any exemption method they find appropriate in their DTAs.</p>
<h3 id="commentary">Commentary</h3>
<p>The model tax convention (and any particular DTA) is relatively short: it has 20-something articles, around 1 A4 page each. It cannot possibly describe any possible situation in detail. As a complement to this document, OECD published a much lengthier, 380-page long, <a href="https://www.oecd.org/berlin/publikationen/43324465.pdf">Commentaries on the articles of the model tax convention</a>. It describes the tax convention articles and the reasoning behind them in more detail, giving practical examples. While it’s non-binding (as only the particular DTA is binding), various courts use it to help them interpret the articles of the DTAs.</p>
<h2 id="enforcing-the-agreement">Enforcing the agreement</h2>
<p>We know what to do when we think a country-specific law is violated: we go to the court, and it makes a judgment. But what to do if the two countries’ institutions have different interpretations of the DTA, e.g. when both countries claim “you are considered a tax resident of our country”?</p>
<h3 id="mutual-agreement-procedure">Mutual agreement procedure</h3>
<p>As we said earlier, referring an issue to a court in any particular country may not work, as their jurisdiction won’t cover the other state (after all, the other country’s courts may disagree!).</p>
<p>To give the taxpayer a mechanism to get a binding interpretation of the DTA, the countries agree to a special legal process, called Mutual Agreement Procedure (MAP).</p>
<p>It is initiated by the taxpayer who claims that the resolutions of the DTA are not followed by one of the countries. Both countries take part in the process and are trying to agree to a common interpretation. If they fail to do so within a time limit, the issue is referred to somewhat independent arbitration.</p>
<p>Given that MAP requires engaging both countries’ legal teams, it looks like a slow process one would prefer to avoid.</p>
<h2 id="final-words">Final words</h2>
<p>The international tax rules are complex, and the countries’ tax authorities often want you to pay all of the tax to their country. It’s worth knowing under what conditions you have to do so and when to claim taxation is not appropriate.</p>
<p>Understanding the rules may help you to pay less tax, as sometimes changing your decision (e.g. buying a house, selling stock, moving countries at a different time) may lead to your income being taxed in another country that has a lower tax rate.</p>
<h3 id="disclaimer">Disclaimer</h3>
<p>As I am not a tax professional, you should take the information above as a grain of salt.</p>
<section class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>Organisation for Economic Co-operation and Development, <a href="https://en.wikipedia.org/wiki/OECD">OECD</a>.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>Taking into account every other country’s constantly changing laws would be a lot of headache!<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
]]></description>
    <pubDate>Tue, 06 Jul 2021 00:00:00 UT</pubDate>
    <guid>https://sygnowski.ml/posts/2021-07-06-tax-residency.html</guid>
    <dc:creator>Jakub Sygnowski</dc:creator>
</item>
<item>
    <title>Compressed inserts for board games</title>
    <link>https://sygnowski.ml/posts/2021-06-20-board-game-inserts.html</link>
    <description><![CDATA[<div class="info">
    Posted on June 20, 2021
    
</div>
<h1>Compressed inserts for board games</h1>
<p>Board games are often sold in boxes that are bigger than necessary to fit the game elements. It makes sense from a marketing perspective: oversized shiny covers are better at grabbing the attention of buyers in a shop. Once bought, the big boxes are a pain: they are problematic when moving homes and take more shelf-size than needed. For this reason, at the end of 2018, we tried to design new, smaller boxes for board games with my <a href="https://micha7a.github.io">girlfriend</a>.</p>
<h2 id="technology-choice">Technology choice</h2>
<p>We decided to make the box by cutting it from plywood with a laser cutter.</p>
<p>To get started, we generated a generic box design using a <a href="https://boxdesigner.connectionlab.org">Box Designer website</a> (another generator is available at <a href="https://makeabox.io">makebox.io</a>).</p>
<figure>
<img src="/images/bg_inserts/blank_box.png" style="width:60.0%" alt="This is how a typical box looks like." />
<figcaption aria-hidden="true">This is how a typical box looks like.</figcaption>
</figure>
<h3 id="inkscape-workflow">Inkscape workflow</h3>
<p>We edited the box in Inkscape by adding and removing notches and adding separators. The goal was to create several small boxes to fit the elements of a Mice and Mystics game.</p>
<p>In the process of doing so, we improved our Inkscape skills. The features we learned and used the most were snapping, guides, and align-and-distribute. They helped to keep everything of perfect size (including keeping lines centered around where they should be).</p>
<h3 id="kerf">Kerf</h3>
<p>One reason we wanted to keep all the sizes exact is kerf. Kerf is the width of the cut that the laser cutter makes in the wood. It’s generally small (in our case we the kerf was around 0.1mm), but it’s still necessary to take it into account to make the box’ notches stick together.</p>
<p>Let’s imagine we create a design file with a notch of size 10mm and the corresponding hole to be 10mm, too. If we cut the file as-is, with 0.1mm kerf, we’ll end up with a notch of size around 9.9mm (we’ll lose half of the kerf on both sides of the notch) and a 10.1mm hole. The leeway of 0.2mm will make the connection loose and useless.</p>
<p>Instead, we should adjust their sizes, such that the notch can barely enter the hole: the wood can compress a little when joining the elements, and the pressure will keep them together.</p>
<p>I tried to find a way to achieve that in Inkscape automatically, but I didn’t find a satisfactory way of doing so. Some of the tutorials I found didn’t work with Paths instead of Objects or were only able to increase the size of the notch but not to make the hole smaller.</p>
<p>To get the effect I wanted, I wrote a small Inkscape extension (code <a href="https://github.com/sygi/inkscape_add_kerf">here</a>).</p>
<p>In the end, this is how one of the Mice and Mystics boxes turned out:</p>
<figure>
<img src="/images/bg_inserts/spider.svg" style="width:40.0%" alt="Design file" />
<img src="/images/bg_inserts/mnm_pajak.jpg" style="width:42.0%" alt="Box photo" />
<figcaption>
The design of the box and the its photo after assembly.
</figcaption>
</figure>
<h2 id="time-stories">Time stories</h2>
<p>After getting some experience with smaller boxes, we moved to make a single, complex box for T.I.M.E stories. The original box is notoriously oversized: all elements are relatively small (a couple of dices, pawns, and cardboard tokens), but the box is probably the biggest one I (used to :) own.</p>
<p>The overall process was the same as with the smaller boxes: measuring elements, choosing the compartments, and drawing them in Inkscape. One new feature I played with here was engraving: making a mark in the wood that isn’t deep enough to cut it through.</p>
<p>Here, I decided to put numbers next to the holes: they can be kept barely visible, yet they are very convenient when assembling a complex box with multiple inner compartments.</p>
<figure>
<img src="/images/bg_inserts/ts_numbers.png" style="width:40.0%" alt="A part of the design with holes numbered." />
<figcaption aria-hidden="true">A part of the design with holes numbered.</figcaption>
</figure>
<p>When kerf is chosen correctly, the notches are keeping together surprisingly sturdily. Because of that, I didn’t want to use the notches for the cover: it’s going to be opened and closed often, and disassembling the notched connection took a bit of effort. Instead, I decided to go for the sliding cover and a small engraving in the walls to fit the cover.</p>
<p>When printing, the cuts should be done starting with engraving and then moving from the inner cuts to the outer ones: the wood can otherwise move a bit when popping out of the sheet, and the future cuts will be made inaccurate.</p>
The size of the final box ended up to be 12.5cm x 16cm x 3.1cm, compared to 30cm x 30cm x 7.6 cm of the original box.
<figure>
<img src="/images/bg_inserts/ts_open_red.jpg" style="width:35.0%" alt="Final box" />
<img src="/images/bg_inserts/ts_comparison_red.jpg" style="width:35.0%" alt="Comparison with orignal box" />
<figcaption>
Picture of the box and a comparison to the original one.
</figcaption>
</figure>
<h3 id="board">Board</h3>
<p>The original Time Stories box is big but also fits a substantial game board. Michalina decided to draw a new, more compact board to print it later.</p>
<p>We wanted to fit the board in an A-something format because it’s easy to print and store. We managed to arrange the whole board on a sheet of A2,
but to do so, we had to evict the codex cards outside the board, leaving on a border only indicators on where they should go.</p>
<p>Michalina designed the functional elements in Inkscape and finished the look in (raster) Krita. She drew the background in Krita and traced the vector lines with a brush using a neat Krita feature (<code>Edit-&gt;Stroke selected shapes</code>).</p>
<figure>
<img src="/images/bg_inserts/ts_rasterized.png" style="width:60.0%" alt="Fancy raster lines generated from a vector-graphics, straight line." />
<figcaption aria-hidden="true">Fancy raster lines generated from a vector-graphics, straight line.</figcaption>
</figure>
<p>We printed the board on two A3 sheets of good quality and nicely covered it with a transparent film… and then forgot to take it with us on holiday, so we printed it again on regular A4 paper.</p>
<figure>
<a href="/images/bg_inserts/ts_board.png"><img src="/images/bg_inserts/ts_board_red.png" style="width:75.0%" alt="Board for the game" /></a>
<figcaption>
Design of the board by Michalina.
</figcaption>
</figure>
<h3 id="costs">Costs</h3>
<p>So, how much does making a box like this cost?</p>
<p>In terms of time, it took us probably around 10 hours to get comfortable enough with Inkscape, probably another 4 hours to design a new box and an hour or two to cut and assemble it.</p>
<p>In terms of the materials, the main cost would have been the amortization of a laser cutter: these devices tend to cost a couple of thousand dollars. We had access to one for free in the company we worked at, so no cost for us.</p>
<p>For a box like a time-stories one, we needed three A4 sheets of plywood (after arranging elements nicely). We bought a box of 20 sheets on Amazon for 2£, so the cost here was 30p per box (if not counting that we didn’t end up using all of the plywood sheets).</p>
<h3 id="final-words">Final words</h3>
<p>The files to cut the box and print the board are available <a href="../data/bg_inserts/ts_box.zip">here</a>. Overall, this project was a lot of fun and we’ll likely do more of these in the future.</p>
]]></description>
    <pubDate>Sun, 20 Jun 2021 00:00:00 UT</pubDate>
    <guid>https://sygnowski.ml/posts/2021-06-20-board-game-inserts.html</guid>
    <dc:creator>Jakub Sygnowski</dc:creator>
</item>
<item>
    <title>Digital painting exercises</title>
    <link>https://sygnowski.ml/posts/2021-05-09-digital-painting-exercises.html</link>
    <description><![CDATA[<div class="info">
    Posted on May  9, 2021
    
</div>
<h1>Digital painting exercises</h1>
<p>I think I am learning the quickest when doing a sequence of well-defined challenges of appropriate difficulty. As I didn’t find a course of digital drawing I’d like to follow, I was compiling separate exercises from various sources. They helped me to make small steps of progress in improving my digital painting skills.</p>
<h2 id="drawing-people">Drawing people</h2>
<p>For a start, I decided to revisit Proko lessons and learn to better draw people on a tablet.</p>
<h3 id="line-of-action-developing-workflow">Line of Action, developing workflow</h3>
<p>A popular exercise when learning to draw people is the timed drawing, where students draw a model in a short, predefined period of time. The goal is to be able to concentrate on capturing general gesture as opposed to details.</p>
<p>Many websites help to run a similar exercise online, where the student is drawing from a prepared picture which changes every couple of minutes.</p>
<p>I was using two websites for this: <a href="https://line-of-action.com">Line of Action</a> and <a href="https://www.youtube.com/channel/UCliUF1c8m7MUspaCykJljSg">New Masters Academy</a> channel on Youtube. They both have lots of content to draw.</p>
<p>You can see the effects of my experimentation below:</p>
<figure>
<a href="/images/digital_painting2/people/sketches/A.png" target="_blank"><img src="/images/digital_painting2/people/sketches/A.png" style="width:80.0%" alt="Human sketch" /><img data-bilderrahmen="sketches"/></a>
<figcaption>
Attempts at timed drawing of people.
</figcaption>
</figure>
<p>After doing these drawings, I have developed a workflow that seems to work well for me:</p>
<ol type="1">
<li>I start with a quick gesture line</li>
<li>Then, on another layer, I draw basic shapes on top to get all body parts in place.</li>
<li>Finally, I draw a contour line with a slightly smoothing pen on the final layer.</li>
</ol>
<figure>
<img src="/images/digital_painting2/people/workflow/gesture.png" style="width:30.3%" alt="Gesture drawing" />
<img src="/images/digital_painting2/people/workflow/shapes.png" style="width:29.95%" alt="Basic shapes" />
<img src="/images/digital_painting2/people/workflow/contour.png" style="width:30.0%" alt="Contour" />
<figcaption>
Stages of the drawing.
</figcaption>
</figure>
<p>Before moving on to the next thing, I made a couple of drawings following the workflow below:</p>
<figure>
<a href="/images/digital_painting2/people/final/A.png" target="_blank"><img src="/images/digital_painting2/people/final/A.png" style="width:50.0%" alt="Human drawing" /><img data-bilderrahmen="people"/></a>
<figcaption>
Final drawings made using this workflow.
</figcaption>
</figure>
<h3 id="portrait-painting">Portrait painting</h3>
<p>In the poses drawings, I omitted hands, feet, and faces, as they are harder than the overall shape of the body and a topic on their own. I decided to spend a bit of time learning to draw heads, though.</p>
<p>Initially, I did two sketches using the same reference page as before:</p>
<figure>
<img src="/images/digital_painting2/people/Heads.png" style="width:70.0%" alt="Sketches of a head." />
<figcaption aria-hidden="true">Sketches of a head.</figcaption>
</figure>
<p>I then moved to digital painting. I started by following <a href="www.youtube.com/watch?v=0CDd22s3jec">the walkthrough</a> on a Sam Does Arts youtube channel:</p>
<figure>
<img src="/images/digital_painting2/people/Sam.png" style="width:45.0%" alt="Portrait painting based on a tutorial." />
<figcaption aria-hidden="true">Portrait painting based on a tutorial.</figcaption>
</figure>
<p>While not everything here looks great, I think the overall shapes and colors went fine.</p>
<p>Then, I tried to paint my girlfriend from a photo. To decrease the number of things to think about, I allowed myself to trace the picture to establish the initial shapes. Somehow, they still didn’t end up as well as they should, which created an opportunity to learn the <a href="https://procreate.art/handbook/procreate/adjustments/adjustments-liquify/">liquify tool</a>, which allows to smoothly shrink and stretch elements of the picture.</p>
<p>Another element that didn’t work well in this picture is the saturation of colors. While one would usually see less saturation in strongly-lit areas, as the skin has red stuff beneath it, you can see more vibrant colors in lighted areas than in neutral light (see <a href="https://en.wikipedia.org/wiki/Subsurface_scattering">subsurface scattering</a>). The shadows also ended up too grey, looking too much like dirt.</p>
<figure>
<video width="450" height="480" controls>
<source src="/images/digital_painting2/people/misia-crop.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
<figcaption>
Recording of the painting.
</figcaption>
</figure>
<h2 id="art-studies">Art studies</h2>
<p>I found online a suggestion for improving one’s art: to analyze the paintings of others or movie frames. I decided to try two methods: a value and a color study.</p>
<h3 id="value-studies">Value studies</h3>
<p>Value (lightness) is a primary tool for creating a contrast in a picture. To think about how others use values in their art, I followed an exercise to draw using only three colors: white, black, and grey. Initially, the areas that light reaches are covered with white, then the half-tone ones are getting changed to grey. Arguably, this exercise helps to understand the shape of the objects in the pictures.</p>
<figure>
<img src="/images/digital_painting2/analysis/values.png" style="width:70.0%" alt="Value study of a couple of paintings and movie frames." />
<figcaption aria-hidden="true">Value study of a couple of paintings and movie frames.</figcaption>
</figure>
<h3 id="color-studies">Color studies</h3>
<p>The other exercise I tried was a more holistic analysis of the pictures of others. I looked over a couple of them and tried to understand the techniques used and reproduce them in a simplified form.</p>
<p>I used pictures of <a href="https://www.youtube.com/channel/UC0XHwYwoeWJC9d0ndtev5vw">AngryMikko</a> and <a href="https://www.youtube.com/channel/UCsDxB-CSMQ0Vu_hTag7-2UQ">Marco Bucci</a> for this, whose colors (despite having different styles) I enjoy.</p>
<figure>
<p><a href="/images/digital_painting2/analysis/mikko1.png"><img src="/images/digital_painting2/analysis/mikko1.png" style="width:45.0%" alt="House analysis" /></a>
<a href="/images/digital_painting2/analysis/mikko2.png"><img src="/images/digital_painting2/analysis/mikko2.png" style="width:45.0%" alt="Tree analysis" /></a></p>
<figcaption>
Analysis of “Maybe Tomorrow” (<a href="https://www.instagram.com/p/CADWmN4DbER/?hl=en">original</a>) and “Moon Chalice” (<a href="https://www.instagram.com/p/CHhdBFDDqfa/">original</a>) by AngryMikko.
</figcaption>
</figure>
<p>The main observation from Mikko was that, although his pictures feel colorful, he’s only using high saturation in a small portion of them: most of the time, he uses the saturation lower than 50%.</p>
<figure>
<a href="/images/digital_painting2/analysis/bucci.png"><img src="/images/digital_painting2/analysis/bucci.png" style="width:65.0%" alt="Tree character" /></a>
<figcaption>
Analysis of “GoodMorning” (<a href="https://www.marcobucci.com/personal-work?lightbox=dataItem-j7ali1xo1">original</a>) by Marco Bucci.
</figcaption>
</figure>
<p>On the other hand, Marco uses more highly-saturated areas. He also uses a bunch of different brushes compared to Mikko. It seems that, as the character was the main focus, its high value/saturation looks good even though it should be more in shadow behind the tree.</p>
<h2 id="exercises">Exercises</h2>
<p>Apart from studying others’ work, I also did a couple of custom exercises.</p>
<h3 id="lightning-a-cube">Lightning a cube</h3>
<p>First, after watching a couple of tutorials about color theory (<a href="https://www.youtube.com/watch?v=xcCJ2CU-bFw">Light and shadow</a>, <a href="https://www.youtube.com/watch?v=gwLQ0cDb4cE">Understanding shadow colors</a>, <a href="https://www.youtube.com/watch?v=botKNa3YGsI">Cavemen color theory</a>), I decided to summarize the general ideas behind natural colors of areas in light and shadow.</p>
<figure>
<a href="/images/digital_painting2/exercises/Lightning.png"><img src="/images/digital_painting2/exercises/Lightning.png" style="width:65.0%" alt="Cube lightning" /></a>
<figcaption>
Exercise/notes about lightning a cube.
</figcaption>
</figure>
<h3 id="rocks">Rocks</h3>
<p>Then, I decided to put it into practice a bit more and tried drawing rocks. I partially followed the tutorials (<a href="https://www.youtube.com/watch?v=b-7XzBg1wPM">1</a>, <a href="https://www.youtube.com/watch?v=puTpnQRKzUQ">2</a>, <a href="https://www.youtube.com/watch?v=bFB9FE18POY&amp;t=252s&amp;ab_channel=WalidFeghali">3</a>), and partially tried to imagine the shapes on my own and confirm my understanding of lightning on a rock.</p>
<figure>
<a href="/images/digital_painting2/exercises/Rocks.png"><img src="/images/digital_painting2/exercises/Rocks.png" style="width:65.0%" alt="Rocks drawings" /></a>
<figcaption>
Exercises in drawing a rock.
</figcaption>
</figure>
<h3 id="glowing-stuff">Glowing stuff</h3>
<p>Then, I tried to learn how to draw glowing things, as I find them to look cool. I decided to draw a picture of a turtle in a dark forest with some symbols glowing on its shell.</p>
<figure>
<a href="/images/digital_painting2/exercises/Turtle.png"><img src="/images/digital_painting2/exercises/Turtle.png" style="width:80.0%" alt="Turtle painting" /></a>
<figcaption>
Painting of a turtle.
</figcaption>
</figure>
<p>It turned out that the glowing letters were surprisingly simple to do; for this, I followed the advice of using multiple layers with more and more blurred images.</p>
<p>However, the overall image turned out so-so. I think these were the main problems:</p>
<p>I didn’t plan the composition/focal point/values upfront. The initial idea was to have a bigger view with a horizon line and fancy lights, but I decided to cut the complexity and concentrate on the turtle.</p>
<p>Even with a reduced view, I think there is too much area on the picture where nothing happens. I should have tried to put more contrasting elements around.</p>
<p>Apart from this, it seems I took Mikko’s example too much to heart and used saturated colors too little. I tried to fix it in the post-process, which only helped to some extent.</p>
<h2 id="summary">Summary</h2>
<p>I did a few exercises to improve my use of colors and ability to draw people. While I still envy colorful landscapes and would like to learn how to paint them, I think that I like more the style of strong lines and simple, close to flat colors (more <em>drawing</em> than <em>painting</em>).</p>
<p><a href="/images/digital_painting2/people/sketches/B.png" target="_blank"><img data-bilderrahmen="sketches"/></a><a href="/images/digital_painting2/people/sketches/C.png" target="_blank"><img data-bilderrahmen="sketches"/></a><a href="/images/digital_painting2/people/sketches/D.png" target="_blank"><img data-bilderrahmen="sketches"/></a><a href="/images/digital_painting2/people/sketches/E.png" target="_blank"><img data-bilderrahmen="sketches"/></a><a href="/images/digital_painting2/people/sketches/F.png" target="_blank"><img data-bilderrahmen="sketches"/></a><a href="/images/digital_painting2/people/sketches/G.png" target="_blank"><img data-bilderrahmen="sketches"/></a><a href="/images/digital_painting2/people/sketches/H.png" target="_blank"><img data-bilderrahmen="sketches"/></a><a href="/images/digital_painting2/people/sketches/I.png" target="_blank"><img data-bilderrahmen="sketches"/></a><a href="/images/digital_painting2/people/sketches/J.png" target="_blank"><img data-bilderrahmen="sketches"/></a></p>
<p><a href="/images/digital_painting2/people/final/B.png" target="_blank"><img data-bilderrahmen="people"/></a><a href="/images/digital_painting2/people/final/C.png" target="_blank"><img data-bilderrahmen="people"/></a><a href="/images/digital_painting2/people/final/D.png" target="_blank"><img data-bilderrahmen="people"/></a></p>
]]></description>
    <pubDate>Sun, 09 May 2021 00:00:00 UT</pubDate>
    <guid>https://sygnowski.ml/posts/2021-05-09-digital-painting-exercises.html</guid>
    <dc:creator>Jakub Sygnowski</dc:creator>
</item>
<item>
    <title>Choosing game engine</title>
    <link>https://sygnowski.ml/posts/2021-04-05-choosing-game-engine.html</link>
    <description><![CDATA[<div class="info">
    Posted on April  5, 2021
    
</div>
<h1>Choosing game engine</h1>
<p>The idea of telling stories interactively through video games is appealing to me. Some time ago, I started writing a scenario for a first, test game. In this post, I write down my experience of choosing a game engine.</p>
<h2 id="motivation">Motivation</h2>
<p>Between grade 6 and entering the workforce at around 25 years old, I haven’t played many games. Growing up playing Super Mario Bros, FIFA, or Ages of Empires II, I associated computer games more with cheap adrenaline rushes than discovering new worlds.</p>
<p>It changed dramatically after playing <a href="https://store.steampowered.com/app/582500/We_Were_Here/">We Were Here</a> and <a href="https://store.steampowered.com/app/230230/Divinity_Original_Sin_Classic/">Divinity Original Sin</a> with my girlfriend. These are cooperative games with no reflex elements and are more about exploring the game world than defeating opponents.</p>
<p>At the same time, we were a bit disappointed with We Were Here, as, even though we liked the walkie-talkie coop mechanic, the gameplay was centered around solving abstract puzzles: there’s a little story to discover there<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>.</p>
<p>I decided to fix this and try to make a game similar to We Were Here with a stronger focus on the story and learn some game development in the process.</p>
<h2 id="what-is-a-game-engine">What is a game engine</h2>
<p>Making a game involves a lot of complex elements: checking if game objects collide, networking, playing sounds, etc. Many of these can be done once and reused in many games: one doesn’t need to reinvent from scratch how the light bounces from objects or how to communicate player actions in a multi-player game.</p>
<p>The ability to separate some reusable solutions caused the birth of a concept of a <em>game engine</em>: a program made to simplify the process of game creation by providing common patterns, allowing the game designers to concentrate on the game’s content instead of low-level details.</p>
<p>The fixed solutions provided by the game engine may not always be appropriate for a given game. Bigger game studios producing AAA titles will effectively implement proprietary game engines on their own, which will allow them to optimize hardware usage and production workflow for their games.</p>
<p>For the smaller, independent game creators, game engines available on the market are a godsend, as they allow to start making games without investing thousands of manhours in solving ray-tracing issues or creating animation pipelines.</p>
<p>For my first game, I obviously want to reuse known solutions as much as possible, so I was trying out various game engines to see what they offer.</p>
<h2 id="choosing-game-engine">Choosing game engine</h2>
<p>When searching for the game engine over the internet, two popular choices come up often: Unity and Unreal.</p>
<h3 id="unreal">Unreal</h3>
<p>As I hope the mechanics in my game will be relatively simple (to concentrate on the story), the first feature of the game engine I was looking at was: the code language used to write the game. In the case of Unreal, it is C++, which I like, so I decided to try it out first.</p>
<p>For a start, I decided to create a 3D scene with a small city, a lake, a mountain, etc. While I learned lots of both Unreal and general 3D-modelling concepts, creating a 3D scene felt quite overwhelming and more effort than I’d like to put on the visuals alone. It would be exarberated if I needed to make some assets for the game: for the test scene I was taking them from the internet, which wouldn’t look good/wouldn’t be permitted by the license in a game.</p>
<p>A nice thing that I didn’t expect was the option to export the game to HTML5. When thinking about it, I’d certainly like to be able to embed the game on a website: it’ll be much easier to show the result of my work if one doesn’t need to install anything nor have any particular OS to do so.</p>
<p>Unfortunately, due to all the blows and whistles that Unreal comes with, the final export file was over 100MB, despite my attempts to make it as small as possible. Because of it, I wasn’t able to serve it on Github pages, and put it on S3 instead (<a href="https://sygi-blog.s3.eu-central-1.amazonaws.com/unreal/MovingUnrealPackaged-HTML5-Shipping.html">link</a>).</p>
<p>One idea that I quite liked in Unreal is <a href="https://docs.unrealengine.com/en-US/ProgrammingAndScripting/Blueprints/index.html">blueprints</a>. They are a visual programming language that allows people to connect various object properties without writing code. While writing code feels natural to me, the nicely rendered diagrams showing how the properties are changing look appealing.</p>
<figure>
<img src="../images/game_engines/unreal_blueprint.png" style="width:70.0%" alt="Blueprint in Unreal Engine." />
<figcaption aria-hidden="true">Blueprint in Unreal Engine.</figcaption>
</figure>
<p>Overall, I liked Unreal a lot for what it is: a professional-grade 3D game engine. At the same time, I realized that making a proper 3D game is a big undertaking, and I shouldn’t try it for the first game I ever made.</p>
<h3 id="unity">Unity</h3>
<p>After Unreal, I spent a couple of hours playing with its main competitor: Unity.</p>
<p>While it has better support for 2D, it had enough pain points to scare me off quickly.</p>
<p>For a start, while formally supported under Linux, the UI seemed to not look great under Cinnamon, and I remember having some problems with the installation. Linux is clearly not a prime target for Unity.</p>
<p>As the engine is as packed as Unreal, is it loaded slowly and took a lot of disk space. The scenes took a long time to build which would be a hinder to the debugging speed.</p>
<p>Furthermore, the game development language is C#, which I haven’t used so far. While it seems similar to Java, and I could learn it, I’d prefer it if I didn’t have to.</p>
<p>Finally, the business model of the engine put me off. While Unreal is open-source with a fee to pay when your sales exceed 100k USD/year (which won’t happen for me), Unity has a separate “pro” engine available for people who pay for it. While I certainly don’t need all the features of a full-fledged game engine, the feeling that further down the line, there may be things I’ll not be able to do because I didn’t pay was not a good first encounter.</p>
<h3 id="evennia">Evennia</h3>
<p>When limiting the scope after getting overwhelmed with 3D in Unreal, I played with the other end of the art-complexity spectrum and tried to make the game purely text-based. I discovered an old, unpopular “in my times” genre of games: <a href="https://en.wikipedia.org/wiki/MUD">multi-user dungeons (MUDs)</a>.</p>
<p>MUDs are similar to Roguelikes in being purely text-based, but MUDs are multi-player by definition, and the available commands are text-based, like <code>look around</code> or <code>go through the door</code> as opposed to the grid-based movement.</p>
<p>For MUDs, there are also game engines to simplify their building process, which, for example, take care of all the network-related stuff. The one I was playing with is called <a href="https://www.evennia.com">Evennia</a>. It’s written in (and uses as the game language) Python what is a plus for me.</p>
<p>As MUDs are inherently simpler (programming-wise) than fancy 3D games, the engine itself doesn’t have a lot of code, and I was able to find my way around it pretty quickly. I managed to add a way to display pictures, as I thought an occasional image here and there will make the puzzles more interesting.</p>
<p>In the end, I decided to try 2D graphics again, but as I liked the experience with Evennia I may go back to it some time (for a second game :).</p>
<h3 id="godot">Godot</h3>
<p>After my mixed experience with leading game engines, I looked around the internet more and found <a href="godotengine.org/">Godot</a>. It’s a free and open-source game engine that supports both 2D and 3D games. Despite being a powerful game engine, the editor is light (&lt;100 MB) and quick: building a scene takes &lt;1 s, and many changes are hot-reloaded while the game is running. It is important for me, as I left my desktop in London for the pandemic and I’m left with only a laptop with no graphic card now.</p>
<p>I was surprised that even though the engine developers and not able to sponsor the content, there is a lot of tutorials and guides online for using Godot. At the same time, the majority of these tutorials are targeted at beginner programmers/kids; finding higher-level design guides for building big games wasn’t easy.</p>
<h4 id="scene-tree">Scene tree</h4>
<p>The philosophy of game design in Godot revolves around building a <em>Scene</em> (which is vaguely equivalent to a <em>class</em> in Object-Oriented Programming) with a tree of <em>Nodes</em> (like fields, but you can arrange them hierarchically).</p>
<figure>
<img src="../images/game_engines/godot_tree.png" style="width:60.0%" alt="Example Scene tree in Godot." />
<figcaption aria-hidden="true">Example Scene tree in Godot.</figcaption>
</figure>
<p>Being able to combine various elements not only in code but also using the editor UI is surprisingly natural for someone who typically only writes code. It particularly helps for visual things like images, as dragging the image to a correct place is simpler than guessing the correct position in code.</p>
<p>This feature is available not only for the built-in classes but also for any other field marked <code>export</code> by the programmer. Thanks to that and the extensibility of the engine, I was able to make a plugin that sets a <code>horizon_line</code> field in a background image based on a click position. It will make it easy to scale the sprite for the character with the rules of the perspective.</p>
<p>While I like the nodes system overall, I also have a complaint about it. As the <code>script</code> (code) is just a text file that can be attached to any number of nodes (not only scenes but also its elements). Because of that, there are many ways to achieve inheritance (sharing of code in a more specific object): you can either use <em>inherited scenes</em> or <em>extend</em> the script itself.</p>
<p>As a result, it’s not clear which entities should be nodes and which should be scenes, and one needs to spend time making decisions that wouldn’t be needed if there was only one class-like concept, owning its code as in the OOP principles. To some extent, this problem can be solved by establishing conventions on when to use scenes vs nodes.</p>
<h4 id="language">Language</h4>
<p>Godot engine is written in C++, but the default game development language is GDScript, a custom language built for Godot. While writing a whole language for the game engine feels too idiosyncratic to me (seriously, is there not enough unpopular languages?), I somewhat buy <a href="https://docs.godotengine.org/en/stable/about/faq.html#what-were-the-motivations-behind-creating-gdscript">the arguments of developers</a>: they wanted to have a dynamically-typed language but struggled with GIL in Python.</p>
<p>As there is, technically, a way to integrate Godot with Python, I was initially planning to use it, but in the end, GDScript ended up being close enough to Python that I couldn’t care enough to spend time changing it.</p>
<p>Of course, it misses some syntactic-sugar features like f-strings or list comprehensions, but I didn’t find anything too painful for me language-wise.</p>
<p>For completeness’ sake, I should mention that Godot also offers a custom visual programming language (like Unreal’s blueprints), and the integration with C# and C++ seems to be easier than the one with python.</p>
<h2 id="final-words">Final words</h2>
<p>Game development is hard. I tried a couple of game engines which make the process much easier and ended up with Godot. My initial plans of making a story-rich online coop will probably be replaced with making a small point-and-click adventure game.</p>
<p>As of now, I have most of the story-independent code written and will move on to describe game scenes in more detail, likely followed by creating the assets. You can play the debug scene <a href="../data/game_engines/maduka.html">here</a>.</p>
<section class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>I read that the sequel, <a href="https://store.steampowered.com/app/865360/We_Were_Here_Together/">We Were Here Together</a>, has more story.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
]]></description>
    <pubDate>Mon, 05 Apr 2021 00:00:00 UT</pubDate>
    <guid>https://sygnowski.ml/posts/2021-04-05-choosing-game-engine.html</guid>
    <dc:creator>Jakub Sygnowski</dc:creator>
</item>
<item>
    <title>Starting with digital painting</title>
    <link>https://sygnowski.ml/posts/2021-01-30-digital-painting.html</link>
    <description><![CDATA[<div class="info">
    Posted on January 30, 2021
    
</div>
<h1>Starting with digital painting</h1>
<p>Having drawn a lot of black-and-white ink sketches, I have now moved to draw digitally. Last summer, I got an iPad Pro, and I’m learning the basics of digital painting now. It has unlocked a myriad (literally) of colors and new techniques to acquire, so I felt like wandering in the fog again.</p>
<p>To impose a structure on my journey and follow the advice of deliberate practice, I looked for a systematic course to guide me through the learning process.</p>
<h2 id="video-tutorials">Video tutorials</h2>
<p><a href="ctrlpaint.com/">Ctrl+Paint</a> sounded like that but, after taking a detailed look it felt focused on watching videos with theory (and teaching Photoshop’s interface, while I use Procreate) as opposed to particular exercises.</p>
<p>I thus turned to Youtube, which has dozens of videos on every topic. I started by following a couple of landscape tutorials by <a href="https://www.youtube.com/channel/UCTEy6Nok4pMzhCp2TS2DmdA">James Julier</a>. I particularly liked them, as the author uses only a simple round brush, which resulted in one less decision to make by me.</p>
<figure>
<a href="../images/painting/landscape1.png" target="_blank"><img src="../images/painting/landscape1.png" style="width:32.2%" alt="Snowy tree" /><img data-bilderrahmen="landscapes"/></a>
<a href="../images/painting/landscape2.png" target="_blank"><img src="../images/painting/landscape2.png" style="width:34.2%" alt="Mountain dark" /><img data-bilderrahmen="landscapes"/></a>
<a href="../images/painting/landscape3.png" target="_blank"><img src="../images/painting/landscape3.png" style="width:32.2%" alt="Mountain light" /><img data-bilderrahmen="landscapes"/></a>
<figcaption>
The landscape pictures I painted.
</figcaption>
</figure>
<p>Here are the pictures I created. As you see, I didn’t have the patience to finish all the details. I was also missing a planning/sketching phase, as James dives directly into painting but to me, it wasn’t always clear what he’s drawing at a given moment.</p>
<p>Other stuff I watched on youtube include <a href="https://www.youtube.com/channel/UC0XHwYwoeWJC9d0ndtev5vw">angrymikko</a>, even though his lessons seem a little too advanced for me yet, and <a href="https://www.youtube.com/channel/UCsDxB-CSMQ0Vu_hTag7-2UQ">Marco Bucci</a>. I also watched Marco’s digital drawing course on skillshare (using a trial). Overall, I didn’t find a lot of quality content on skillshare, though.</p>
<h3 id="brushes">Brushes</h3>
<p>Many of the Youtube videos I watched were of the form: “You can buy my brush set here. It’s easy to follow my tutorial with it”. It was rather disappointing, as I didn’t want to spend my precious motivation on learning someone’s vendor locked-in brush.</p>
<p>I thought that I like the texture of watercolors though, so I still found some <a href="https://procreate.brushes.work/watercolor-paint-brushes-procreate-5/">free watercolor brush set</a> and tried to follow <a href="https://www.youtube.com/watch?v=gryrZ7CXcG8&amp;ab_channel=CalvinatDrifterStudio">a watermelon tutorial</a>.</p>
<figure>
<a href="../images/painting/watermelon.png"><img src="../images/painting/watermelon.png" style="width:70.0%" alt="Watermelon" /></a>
</figure>
<p>While the effects were not terrible, I felt like I had to fight against the brushes a bit. Furthermore, having a lot of different brushes (with many of them being hard to use) slowed me down as I needed to spend time to choose among them.</p>
<p>In the end, after doing these tutorials I knew I should look for one “general” brush for everything and that I want something simple, namely, a brush with:</p>
<ul>
<li>~uniform opacity within a stamp</li>
<li>ability to cover an area with a flat color (potentially with multiple strokes)</li>
<li>no/little reaction to tilt (it’s way too hard for me to control)</li>
<li>pressure sensitivity shouldn’t change the size of the stroke (it’s hard to control edges of the lines otherwise)</li>
<li>no line smoothing, as it looks weird when the line doesn’t end up where I draw + the delay is annoying</li>
</ul>
<p>I tried finding a brush like this online. Most of them the ones I found were hard to use, with various effects happening with tilts and pressure. I found one, though, a <a href="https://procreate.brushes.work/copic-marker-brush/">copic marker brush</a> which (after some small modifications) I liked a lot. Apart from it, I made a couple of versions of the default round brush (with various shapes) as I liked the idea of them not being rotationally symmetric (so that you kind of can draw a rough edge without making the brush very small).</p>
<h2 id="other-exercises">Other exercises</h2>
<p>Apart from directly following full tutorials and semi-productively watching videos, I did a couple of shorter exercises:</p>
<ol type="1">
<li><a href="https://www.youtube.com/watch?v=MnQO-xxp3j4&amp;ab_channel=Let&#39;sDrawwithBeeJayDeL">Shading a crab</a> to make it look 3D. It was surprisingly simple to achieve that effect with the selection tool &amp; airbrush for the shading. One thing, however, that I missed in procreate, was the ability to <a href="https://folio.procreate.art/discussions/3/6/13529?page=2">disable touch during selection</a>. Without it, it’s currently easy to put extra selection points by mistake.</li>
</ol>
<figure>
<a href="../images/painting/crab.png"><img src="../images/painting/crab.png" style="width:80.0%" alt="Crab exercise" /></a>
<figcaption>
As I just wanted to see how the tools work, I didn’t particularly strive for the final look.
</figcaption>
</figure>
<ol start="2" type="1">
<li>Drawing <a href="https://www.reddit.com/r/learnart/comments/6rg53z/tutorial_on_how_to_paint_ghibli_style_trees/">Ghibli-style trees</a>. I tried doing it using the “watercolor” brushes but they made the edges smooth what didn’t look great. For the last version (bottom), I used an oval version of the simple round brush, which I think looks a bit better.</li>
</ol>
<figure>
<a href="../images/painting/trees.png"><img src="../images/painting/trees.png" style="width:80.0%" alt="Trees exercise" /></a>
</figure>
<ol start="3" type="1">
<li>I tried reproducing a <a href="https://www.reddit.com/r/learnart/comments/eiltgl/studio_ghibli_background_study_i_did_to_learn/">Ghibli background</a>.
It was a good lesson, e.g. on color choice and the amount of time to spend on various details (you can see I dropped some details mid-tree). </br>
For this picture, I used a “Painty Round” brush from Procreate <a href="https://procreate.art/all-star">all-star brush set</a>. While the brush itself was ok, I don’t feel like I used much of its texture, and, at the same time, the streamlining/line smoothing drove me mad.</li>
</ol>
<figure>
<a href="../images/painting/ghibli.png"><img src="../images/painting/ghibli.png" style="width:90.0%" alt="Ghibli tree" /></a>
<figcaption>
While the final image is far from perfect, it turned out way better than I expected.
</figcaption>
</figure>
<ol start="4" type="1">
<li>Still life: a standard exercise from the classical painting is also applicable for digital. It removes the complexity of deciding what exactly to draw; you’re merely replicating what you see and improving your craft.</li>
</ol>
<figure>
<a href="../images/painting/still-life.png"><img src="../images/painting/still-life.png" style="width:90.0%" alt="Watermelon" /></a>
<figcaption>
I used a simple scene here but will try with more complex one next.
</figcaption>
</figure>
<ol start="5" type="1">
<li>Painting with light: an exercise where you take a picture and try painting it with just one color. It seems like a useful thing to do at the beginning of the piece to decide where the light/shadow areas are. I haven’t tried it yet but will do it soon.</li>
</ol>
<h2 id="random-hints">Random hints</h2>
<p>Here are some of the hints repeated by various artists:</p>
<ol type="1">
<li>During digital painting, it’s worth it to flip the canvas horizontally or vertically from time to time to assess the picture from the new view. The final artwork should work whether it’s flipped or not.</li>
<li>On the other hand, you can disable colors on the tablet and draw in black&amp;white, using the color picker to choose colors. You will be painting in the full color range, but the areas of different value will be visible better.</li>
<li>Despite many artists selling custom Procreate brushes on their channels, they recommend learning a single brush and using it thorough the whole painting</li>
<li>The light should be on the opposite side of the hue range from the shadow. In other words, if the light is warm, the shadow should be cold.</li>
<li>Multiply blending mode is useful for shadows (as it always makes things darker)</li>
<li>To learn to match the colors in real-life and in the color palette, there is an exercise: we take a picture, select a part of it, and try to <a href="https://www.ctrlpaint.com/videos/guess-that-color">guess</a> the color by selecting it in the palette.</li>
</ol>
<p>With all of these lessons, tutorials, and exercises I have a lot of things to try. I’ll likely make a new post after I put them into practice some more.</p>
]]></description>
    <pubDate>Sat, 30 Jan 2021 00:00:00 UT</pubDate>
    <guid>https://sygnowski.ml/posts/2021-01-30-digital-painting.html</guid>
    <dc:creator>Jakub Sygnowski</dc:creator>
</item>
<item>
    <title>Escape from Auschwitz: a fan T.I.M.E stories scenario</title>
    <link>https://sygnowski.ml/posts/2020-12-07-auschwitz.html</link>
    <description><![CDATA[<div class="info">
    Posted on December  7, 2020
    
</div>
<h1>Escape from Auschwitz: a fan T.I.M.E stories scenario</h1>
<p>Ever since I got T.I.M.E stories for Christmas a couple of years ago, I was planning to make a fan scenario. I have finally found some time to develop the expansion recently.</p>
<p>T.I.M.E stories is a decksploration board game, about which I wrote a bit <a href="https://sygnowski.ml/posts/2020-06-19-time-stories.html">here</a>. The game itself defines mechanics (somewhat similarly to e.g. RPGs) and sells different scenarios (stories) as expansions. There’s a <a href="https://boardgamegeek.com/geeklist/225429/time-stories-print-and-play-scenarios-available-bg">long list</a> of fan-made scenarios, many of which are really high-quality<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>.</p>
<p>The high-level idea for the story (mild spoilers ahead) is the daring <a href="https://en.wikipedia.org/wiki/Kazimierz_Piechowski">escape</a> of Kazimierz Piechowski and his fellows from Auschwitz concentration camp.</p>
<h2 id="preparations">Preparations</h2>
<p>Initially, the story was meant to be about the escape of <a href="https://en.wikipedia.org/wiki/Witold_Pilecki">Witold Pilecki</a>, but after reading the details of his story I felt that the escape alone was not a worthy enough element of his biography. The escape of Piechowski seemed to fit the mechanics of time stories better.</p>
<p>To better understand the living conditions in the camp, I read <a href="https://en.wikipedia.org/wiki/Pilecki&#39;s_Report">Pilecki’s Report</a>: a document written by Pilecki and smuggled out by Stanisław Jaster.</p>
<p>I chose that obtaining this report will be the main topic of my scenario.</p>
<p>I also collected maps of Auschwitz I and, with the help of a historian friend, found <a href="http://lekcja.auschwitz.org/en_15_ucieczki/">the details of this particular escape</a>. I tried keeping the scenario close to the real event, although I made small changes to drive the story.</p>
<h2 id="technical">Technical</h2>
<p>I kept notes on the game setting in a markdown note. In there, I wrote there decisions on puzzles, the mechanism to leave and enter the inner camp, locations, the meaning of state tokens, and the like.</p>
<p>As the details of each location started to grow with the actual card texts, I moved them to a separate markdown file. I then wrote a short program that converted this file to a filetree with directories corresponding to locations and png files of each card with the rendered texts.</p>
<h2 id="inkscape">Inkscape</h2>
<p>After the list of locations and their contents was more or less decided, I started creating the actual cards. I was (still am) planning to create custom graphics for the locations, so I left the backs of the cards blank (apart from location names) and worked mostly on the fronts.</p>
<p>I made the cards in <a href="https://inkscape.org">Inkscape</a>: it was a relatively nice experience. For one type of cards (front, backs, A-cards, etc.) I pasted the background texture and locked it and was adding things on top of it.
When I was adding new elements like shields, tokens, but also choosing the design of text boxes, I was saving a new copy of the “template” card with the additional elements placed outside of the page. This way, I had a file with dozens of tokens, shields, and others ready to be drag and drop to the card page in the default svg template.</p>
<p>One particularly useful thing was the ability to create a card from a given subset of the image file, so I was able to create a couple of cards laying next to each other and easily cut them into separate cards. It was useful in creating the map and a multi-card puzzle.</p>
<h2 id="graphic-design">Graphic design</h2>
<p>All of the graphic elements I used so far weren’t created by me. I was surprised I was able to find free fonts (with a license allowing me to use them) on <a href="https://www.1001fonts.com">1001fonts.com</a>, and paper-like textures on <a href="https://www.myfreetextures.com">www.myfreetextures.com</a>. I used a small number of images for item cards <a href="https://www.pixabay.com">pixabay.com</a>.</p>
<p>I made a couple of smaller graphic-design decisions like adding the semi-transparent text boxes below texts to improve readability.</p>
<p>I also chose a font and size for every type of text, with most typical texts written with Lato 15. 15 was quite big, which made it necessary to cut some texts, but I think having less text made the cards better.</p>
<h2 id="playtests">Playtests</h2>
<p>I ran two playtests with friends. They discovered many important problems, e.g.:</p>
<ul>
<li>initially, I made a camp border a solid line, even around the camp gate and people didn’t try to go there because they assumed it wasn’t available</li>
<li>the mechanic for leaving the camp was not explained clearly enough</li>
<li>story was too linear</li>
</ul>
<p>I hopefully was able to fix those to a large degree.</p>
<h2 id="compiling">Compiling</h2>
<p>Once the cards were ready to play, I wrote a script that copied them in the correct order, as I was developing them in the directory tree based on the location name as above.</p>
<p>Then, I used a small subset of <a href="http://heresy.mrtrashcan.com/home/the-tools/">heresy tools</a> to join the cards together in a print-and-play pdf.</p>
<h2 id="remaining-problems">Remaining problems</h2>
<p>I think my workflow was ok-ish, but definitely could be improved.</p>
<p>In particular, the compiling phase for me included many steps: running the reordering script, moving the cards, substituting alpha with a light-grey (so that empty cards are visible in a pdf), and running heresy tools. This felt like unnecessary manual labor. Using heresy tools in full promise to help with it, but the need to manually enter the sizes and shapes is a no-deal for me. Having a single-step compilation could also allow me to generate a pdf with texts as vector graphics without converting to pngs first as I do now.</p>
<p>Apart from this, my handling of card texts could be improved. Currently, I stored them both in the markdown file and the final svg. Because of this, whenever something needed to be changed, I had to make the change in both places. Also, manually copying the texts to svg caused them to not be positioned in the same way.</p>
<p>Finally, when I figured out how to properly justify text in Inkscape, I already had the texts placed everywhere, so I didn’t bother to change them.</p>
<h2 id="final-words">Final words</h2>
<p>Without further ado, this is <a href="https://drive.google.com/file/d/1MzdS5eCEafiefC1Lmfx_WlrzFDWUweAh/view?usp=sharing">a link to the scenario</a> and its <a href="https://rpggeek.com/geeklist/206708/item/8006732#item8006732">BGG item page</a>.</p>
<p>I believe the whole game is finished, except for the custom graphics, which are not necessary for playing and will take a while to finish (it’s around 50 images to draw).</p>
<p>If you end up playing it, I’ll greatly appreciate <a href="https://forms.gle/Pf5pVWJfm1gusz4D6">your feedback</a>.</p>
<p>Apart from the images, I plan to do a Polish translation soon.</p>
<figure>
<img src="../images/auschwitz/cover.png" style="width:50.0%" alt="Game “cover”." />
<figcaption aria-hidden="true">Game “cover”.</figcaption>
</figure>
<section class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>I played <a href="https://boardgamegeek.com/boardgame/200443/switching-gears-fan-expansion-time-stories">Switching Gears</a> and <a href="https://boardgamegeek.com/boardgame/205847/pariah-missouri-fan-expansion-time-stories">Pariah Missouri</a>, and <a href="https://repository.rebel.pl/wydawnictwo/timestories/Varsovia_2100.pdf">Varsovia 2100</a>.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
]]></description>
    <pubDate>Mon, 07 Dec 2020 00:00:00 UT</pubDate>
    <guid>https://sygnowski.ml/posts/2020-12-07-auschwitz.html</guid>
    <dc:creator>Jakub Sygnowski</dc:creator>
</item>
<item>
    <title>Rushing through Proko courses</title>
    <link>https://sygnowski.ml/posts/2020-10-10-proko.html</link>
    <description><![CDATA[<div class="info">
    Posted on October 10, 2020
    
</div>
<h1>Rushing through Proko courses</h1>
<p>After finishing Drawabox, I tried to learn drawing the human figure with the <a href="https://proko.com">Proko</a> courses. I went through the free videos of the Figure and Head drawing courses and did a lot of exercises.</p>
<h2 id="general-views-on-proko">General views on Proko</h2>
<p>Proko courses concentrate on drawing a human figure. There is a couple of them, concentrating on the general figure, head, and specific parts of the anatomy. Each course consists of very detailed videos showing anatomical details and specific techniques for drawing them. The courses have paid and free versions; for the free versions, some of the videos are shorter or unavailable.</p>
<p>During the school closure due to pandemic, Proko offered an extension to his typically week-long free trial (which contains one full paid lesson from each course) to a month. I felt like the extra content of the paid lesson was not worth it, especially for a beginner like me, as the free videos are still quite long and detailed and there are literally hundreds of them available: more than I had time to watch. However, the trial came with lots of example drawings to the lesson, which I used while doing the other lessons.</p>
<p>In general, the Proko courses (especially the free videos) have less structure than the drawabox course. There is no “do this exercise 17 times and move over”, but rather “after watching this lesson on hands, draw some hands”. I approached the course with the deliberate practice approach, aiming to do lots of lame drawings and improving in the process.</p>
<p>I ended up following two courses from Proko: Figure Drawing and Portrait Drawing, as well as watching his Drawing Basics videos (which cover similar topics as drawabox on shapes, perspective, etc.).</p>
<h2 id="figure-drawing">Figure drawing</h2>
<p>Videos from the Figure drawing course are presented at the top of <a href="https://www.proko.com/library">Proko’s video library</a> and it is a recommended course to start from. This is where I started too.</p>
<h3 id="gesture">Gesture</h3>
<p>The first and most time-consuming lesson from the course is on drawing gesture. Gesture is the flow of the pose, the ability to describe what the person is doing without accurately depicting the anatomical details. The lesson stresses the importance of following the general flow, using simple lines, as opposed to tracing the contour of the figure.</p>
<p>The lesson proposes timed exercises to draw the gesture: to take a picture with the model, and plan to draw the gesture in 30 seconds or 2 minutes. Depending on the planned time-length, the gesture should have more or fewer details. As it was hard for me to plan such short drawings, I extended the timings to 1 and 5 minutes and drew the gestures slightly more detailed than on the presented examples.</p>
<p>The full version of this lesson was part of the trial, so it contained lots of example drawings, which I used for my exercises: I started with drawing them (in a timed manner) on my own, then looked at how Proko was drawing them, looking for mistakes in my drawings, and making yet another timed-exercise for a given photo, hopeful to fix the original issues.</p>
<p>I decided to draw along with all the presented example drawings, which there was around 20; this exercise definitely improved my feeling of human anatomy and the ability to express the pose in a small number of lines.</p>
<p><a href="../images/proko/gesture/simple/A.jpg" target="_blank">Pictures<img data-bilderrahmen="gesture"/></a></p>
<h3 id="beans">Beans</h3>
<p>After studying the gesture, Proko goes to introduce his simple models of the human torso: the bean and the robo-bean. These are
useful to attach the feeling of the 3-dimensionality to the flat gesture, without adding too many details at the same time.</p>
<p><a href="../images/proko/bean/A.jpg" target="_blank">Pictures<img data-bilderrahmen="beans"/></a></p>
<h3 id="mannequization">Mannequization</h3>
<p>One can introduce a similar model to simplify drawing other parts of the body. They are particularly helpful when drawing from imagination, when it’s easier (as it has a lower number of degrees of freedom) to introduce a box or a cylinder to denote a particular part of the body and check the proportions than try to introduce both the complex shape and proportions at the same time.</p>
<p><a href="../images/proko/mannequin/A.jpg" target="_blank">Pictures<img data-bilderrahmen="mannequin"/></a></p>
<h3 id="shading">Shading</h3>
<p>After these lessons, Proko has a very short one where he goes on how to shade a drawing with all the highlights, core shadow, and half-tones on an example of a generic muscle. Then, in the next lesson, he shows an example of a full-body, couple-of-hours human figure with full shading, inviting his students to draw along. This was way above what I felt I can do at a time, so I skipped this and moved to the portrait drawing course.</p>
<figure>
<img src="../images/proko/muscle.png" style="width:48.5%" alt="Presentation" />
<img src="../images/proko/yoni.jpg" style="width:50.37%" alt="Expectation" />
<figcaption>
Explained example shading (left) vs expectation for the next drawing (right).
</figcaption>
</figure>
<h2 id="markmaking">Markmaking</h2>
<p>In the figure drawing course, Proko uses a charcoal pencil. I used a 2B/3B graphite one, as I haven’t had anything else.</p>
<p>When I played with shading I was having two problems:
- shading big areas in the picture were taking a lot of time
- when shading, the lines I was making were creating a lined pattern as opposed to a nice gradient (ie. it was clear in which direction I put the lines and it wasn’t in any way related to the underlying 3d objects).</p>
<figure>
<img src="../images/proko/shading.jpg" style="width:40.0%" alt="Picture with straight-line effect visible." />
<figcaption aria-hidden="true">Picture with straight-line effect visible.</figcaption>
</figure>
<p>For the first problem, I was able to reduce it a bit by sharpening the pencil in a similar way to the one described in <a href="https://www.proko.com/drawing-supplies-i-use-in-my-videos/#.X2cYsXWYVTY">this video</a>. Even though I was using a graphite pencil, it still was relatively easy to do this.</p>
<p>For the lined pattern, I tried putting shading in a little bit more circular fashion which helped a bit. I think having a softer pencil (eg. 6B) would help more.</p>
<p>I thought using charcoal would also help here, so I bought a charcoal (and another carbon one, which apparently feels like something between charcoal and graphite) pencil.</p>
<figure>
<img src="../images/proko/charcoal.jpg" style="width:40.0%" alt="Nose picture drawn with charcoal." />
<figcaption aria-hidden="true">Nose picture drawn with charcoal.</figcaption>
</figure>
<p>As you can see in the picture above, the effects were not so great. I had problems controlling the value I put on the paper when using charcoal much more than with graphite. The texture of the paper was also creeping through the picture, leaving weird traces of white. I suspect this may go away with practice (and potentially softer charcoal), but I decided to go back to graphite.</p>
<h2 id="head-drawing">Head drawing</h2>
<p>After finishing (if one could ever “finish” a topic like figure drawing) the figure course, I proceeded to the portrait drawing course. I realized that it was actually created by Proko earlier, which may explain the assumption in the previous course to be able to deal with shading/head on one’s own for the full-body drawing.</p>
<p>Compared to the previous one, it’s more in the style of “this is how you draw a nose” vs giving general methods for photorealistic drawing. Nevertheless, it still gives detailed anatomical details and informative techniques that can be applied to any head.</p>
<p>I started the course by doing one picture with each of the body elements covered in the lessons: eye, nose, mouth, etc. While doing them, I was surprised at first that my pictures were actually looking quite professional (after adding the shading). However, the amount of time it was taking to draw simple body parts was discouraging and the fully-shaded style of pictures wasn’t as appealing to me as drawabox-style single-lines one. I thus decided to move on to more comics-like drawings despite doing a few exercises for this course.</p>
<p><a href="../images/proko/head/A.jpg" target="_blank">Pictures<img data-bilderrahmen="head"/></a></p>
<h2 id="overall">Overall</h2>
<p>I was doing the Proko courses from July until September. The two months are barely enough to touch the surface of the vast topics that I studied, so I feel like I rushed through the courses. At the same time, I feel like I got the basic idea in each of the lessons and while I would need a lot more practice to feel comfortable with all of it, I definitely improved a lot through the courses. I’m not sure what exactly I’ll do next, but I plan to play more with fine-tip pens again, likely drawing comic-style pictures.</p>
<p><a href="../images/proko/gesture/simple/B.jpg" target="_blank"><img data-bilderrahmen="gesture"/></a><a href="../images/proko/gesture/simple/C.jpg" target="_blank"><img data-bilderrahmen="gesture"/></a><a href="../images/proko/gesture/simple/D.jpg" target="_blank"><img data-bilderrahmen="gesture"/></a><a href="../images/proko/gesture/simple/E.jpg" target="_blank"><img data-bilderrahmen="gesture"/></a><a href="../images/proko/gesture/simple/F.jpg" target="_blank"><img data-bilderrahmen="gesture"/></a><a href="../images/proko/gesture/full/A.jpg" target="_blank"><img data-bilderrahmen="gesture"/></a><a href="../images/proko/gesture/full/B.jpg" target="_blank"><img data-bilderrahmen="gesture"/></a><a href="../images/proko/gesture/full/C.jpg" target="_blank"><img data-bilderrahmen="gesture"/></a><a href="../images/proko/gesture/full/D.jpg" target="_blank"><img data-bilderrahmen="gesture"/></a><a href="../images/proko/gesture/full/E.jpg" target="_blank"><img data-bilderrahmen="gesture"/></a><a href="../images/proko/gesture/full/F.jpg" target="_blank"><img data-bilderrahmen="gesture"/></a><a href="../images/proko/gesture/full/G.jpg" target="_blank"><img data-bilderrahmen="gesture"/></a><a href="../images/proko/gesture/full/H.jpg" target="_blank"><img data-bilderrahmen="gesture"/></a><a href="../images/proko/gesture/full/I.jpg" target="_blank"><img data-bilderrahmen="gesture"/></a><a href="../images/proko/gesture/full/J.jpg" target="_blank"><img data-bilderrahmen="gesture"/></a><a href="../images/proko/gesture/full/K.jpg" target="_blank"><img data-bilderrahmen="gesture"/></a><a href="../images/proko/gesture/full/L.jpg" target="_blank"><img data-bilderrahmen="gesture"/></a><a href="../images/proko/gesture/full/M.jpg" target="_blank"><img data-bilderrahmen="gesture"/></a><a href="../images/proko/gesture/full/N.jpg" target="_blank"><img data-bilderrahmen="gesture"/></a><a href="../images/proko/gesture/full/O.jpg" target="_blank"><img data-bilderrahmen="gesture"/></a><a href="../images/proko/gesture/full/P.jpg" target="_blank"><img data-bilderrahmen="gesture"/></a><a href="../images/proko/gesture/full/Q.jpg" target="_blank"><img data-bilderrahmen="gesture"/></a><a href="../images/proko/gesture/full/R.jpg" target="_blank"><img data-bilderrahmen="gesture"/></a><a href="../images/proko/gesture/full/S.jpg" target="_blank"><img data-bilderrahmen="gesture"/></a><a href="../images/proko/gesture/full/T.jpg" target="_blank"><img data-bilderrahmen="gesture"/></a><a href="../images/proko/gesture/full/U.jpg" target="_blank"><img data-bilderrahmen="gesture"/></a><a href="../images/proko/gesture/full/V.jpg" target="_blank"><img data-bilderrahmen="gesture"/></a><a href="../images/proko/gesture/full/W.jpg" target="_blank"><img data-bilderrahmen="gesture"/></a><a href="../images/proko/gesture/full/X.jpg" target="_blank"><img data-bilderrahmen="gesture"/></a><a href="../images/proko/gesture/full/Y.jpg" target="_blank"><img data-bilderrahmen="gesture"/></a><a href="../images/proko/gesture/full/Z.jpg" target="_blank"><img data-bilderrahmen="gesture"/></a><a href="../images/proko/gesture/full/Z.jpg" target="_blank"><img data-bilderrahmen="gesture"/></a><a href="../images/proko/gesture/full/Z1.jpg" target="_blank"><img data-bilderrahmen="gesture"/></a><a href="../images/proko/gesture/full/Z2.jpg" target="_blank"><img data-bilderrahmen="gesture"/></a><a href="../images/proko/gesture/full/Z3.jpg" target="_blank"><img data-bilderrahmen="gesture"/></a><a href="../images/proko/gesture/full/Z4.jpg" target="_blank"><img data-bilderrahmen="gesture"/></a><a href="../images/proko/gesture/full/Z5.jpg" target="_blank"><img data-bilderrahmen="gesture"/></a></p>
<p><a href="../images/proko/bean/B.jpg" target="_blank"><img data-bilderrahmen="beans"/></a><a href="../images/proko/bean/C.jpg" target="_blank"><img data-bilderrahmen="beans"/></a><a href="../images/proko/bean/D.jpg" target="_blank"><img data-bilderrahmen="beans"/></a><a href="../images/proko/bean/E.jpg" target="_blank"><img data-bilderrahmen="beans"/></a><a href="../images/proko/bean/F.jpg" target="_blank"><img data-bilderrahmen="beans"/></a><a href="../images/proko/robobean/A.jpg" target="_blank"><img data-bilderrahmen="beans"/></a><a href="../images/proko/robobean/B.jpg" target="_blank"><img data-bilderrahmen="beans"/></a><a href="../images/proko/robobean/C.jpg" target="_blank"><img data-bilderrahmen="beans"/></a><a href="../images/proko/robobean/D.jpg" target="_blank"><img data-bilderrahmen="beans"/></a><a href="../images/proko/mannequin/B.jpg" target="_blank"><img data-bilderrahmen="mannequin"/></a><a href="../images/proko/mannequin/C.jpg" target="_blank"><img data-bilderrahmen="mannequin"/></a><a href="../images/proko/mannequin/D.jpg" target="_blank"><img data-bilderrahmen="mannequin"/></a><a href="../images/proko/mannequin/E.jpg" target="_blank"><img data-bilderrahmen="mannequin"/></a><a href="../images/proko/mannequin/F.jpg" target="_blank"><img data-bilderrahmen="mannequin"/></a></p>
<p><a href="../images/proko/head/B.jpg" target="_blank"><img data-bilderrahmen="head"/></a><a href="../images/proko/head/C.jpg" target="_blank"><img data-bilderrahmen="head"/></a><a href="../images/proko/head/D.jpg" target="_blank"><img data-bilderrahmen="head"/></a><a href="../images/proko/head/E.jpg" target="_blank"><img data-bilderrahmen="head"/></a></p>
]]></description>
    <pubDate>Sat, 10 Oct 2020 00:00:00 UT</pubDate>
    <guid>https://sygnowski.ml/posts/2020-10-10-proko.html</guid>
    <dc:creator>Jakub Sygnowski</dc:creator>
</item>
<item>
    <title>Drawabox: 14 months later</title>
    <link>https://sygnowski.ml/posts/2020-07-11-drawabox.html</link>
    <description><![CDATA[<div class="info">
    Posted on July 11, 2020
    
</div>
<h1>Drawabox: 14 months later</h1>
<p>In May 2019, I decided to learn how to draw. To structure my learning, I was following a free online drawing course: <a href="https://drawabox.com">Drawabox</a>. 14 months later, I finished it and I am writing down my experience.</p>
<h2 id="my-background">My background</h2>
<p>In an attempt to increase my creativity and produce some artistic works I am trying to learn various artistic media. I recently played with world-building in Unreal, was making Android apps, and was painting board game figures. Now was the time for drawing.</p>
<p>I drew nothing since middle school and I only have a vague recollection of any drawing/painting that we did there. I remember though that I was terrible… and terribly uninterested in it at a time. I was going to learn totally from scratch then.</p>
<h2 id="course-choice-drawabox">Course choice: Drawabox</h2>
<p>Because of my lack of any grounding, I was looking for an opinionated course that would teach me the fundamentals well, with particular examples as opposed to showing a variety of styles I would have hard time to wrap my head around. Other than that, I wanted the course to have relatively little reading/watching compared to the actual exercising: I knew that a big chunk of my struggles will be with physically executing the exercises, so I wanted to prioritize getting a lot of practice over listening/reading a lot.</p>
<p>After a short internet search, I decided to try <a href="https://drawabox.com">Drawabox</a> out, which teaches constructional drawing from beginning. It requires you to use fineliners to not have to worry with line width (and to not be able to erase the lines to learn confidence in markmaking) and ignores matters of shading and many others. It is divided into 7 lessons with different topics, each of which has a bit of reading/videos and tons of exercises to do.</p>
<h2 id="what-i-didnt-follow">What I didn’t follow</h2>
<p>While I generally tried to closely follow the course and understand all the concepts before moving on, I made some small deviations to the course assumptions to keep my motivation up.</p>
<p>First, Uncomfortable (the Drawabox’ author) encourages to dedicate half of the drawing time for drawing “for pleasure” and the other half for his exercises themselves. Being totally lost at the beginning and looking for a set of fixed, well-grounded exercises I didn’t want to occupy myself and wonder what (and how!) to draw apart from it. Also, being able to devote only an hour or two per week for drawing, if I also spent a big chunk of time drawing “other things” I’d quickly lose momentum, especially given how many regular “in-course” exercises there are in Drawabox.</p>
<p>Other than that, one of the guidelines was to start each drawing session with 10 minutes of exercises from lessons 1 &amp; 2 (after finishing them). I also didn’t do this due to lack of time, up until the last lesson (vehicles), where the inability to draw good ellipses was really hurting me.</p>
<h2 id="particular-lessons">Particular lessons</h2>
<p>Below I quickly review each of the lessons and attach my pictures for posterity.</p>
<h3 id="lesson-1-lines-ellipses-and-boxes">Lesson 1: Lines, Ellipses, and Boxes</h3>
<p>The first lesson lays a foundation on the mechanical aspect of drawing and perspective theory. Being green most of the information here: drawing from the shoulder, vanishing points, etc. was new to me.</p>
<p>The course firmly stipulates to draw from the shoulder, to make the lines smooth and confident. While I mostly followed this advice, I have to admit using an elbow in some of the later lessons for details.</p>
<p><a href="../images/drawabox/lesson1/A.jpg" target="_blank">Pictures<img
        data-bilderrahmen="lesson1"/></a></p>
<h3 id="box-challenge">250 box challenge</h3>
<p>After the first lesson, Uncomfortable proposes an optional “250 boxes challenge”. Initially, I decided to skip it, as it felt boring and too much. I only got back to it before lesson 6, which was a good choice, as the ability to draw boxes only starts to be useful there (for lessons 3–5 I drew natural shapes, which are more “sausage-shaped”).</p>
<p>After actually doing 100 of the 250 boxes, I felt I already knew how to draw the boxes (although not always being able to perfectly execute it), so I decided to not waste more time on it.</p>
<p>On the other hand, even after doing that many boxes, there were still some issues that were not clear to me:</p>
<ol type="1">
<li>not all of the boxes drawn with the proposed “Y method” correspond to a real box. For example, the box in the Drawabox logo is not :P</li>
<li>Not all of the boxes can be drawn with the method: if the line of the horizon is cutting the front face, all of the “Y”s will have the shape of an arrow instead</li>
</ol>
<figure>
<img src="https://d15v304a6xpq4b.cloudfront.net/assets/images/drawabox-logo.png" style="width:40.0%" alt="Drawabox logo: if we see the front face straight, the side should be hidden!" />
<figcaption aria-hidden="true">Drawabox logo: if we see the front face straight, the side should be hidden!</figcaption>
</figure>
<p>In retrospect, after doing a couple (tens of?) boxes, it would have been more useful to follow up with the following exercises:</p>
<ol type="a">
<li>try to imagine the orientation of the box and draw it,</li>
<li>start drawing the box with one side and fill the rest.</li>
</ol>
<h3 id="lesson-2-contour-lines-texture-and-construction">Lesson 2: Contour Lines, Texture and Construction</h3>
<p>The second lesson introduces the basics of 3D. I remember the ribbon-made arrows were very hard at the beginning (which you can see on the pictures), but I think I improved a lot while doing the exercises.</p>
<p>Apart from 3D, this lesson contains exercises on texture, which felt out of place in there (and in the course in general, as it is focused on constructional drawing, ie. making sure that the things have the correct general form). On the other hand, texture exercises were the first exercises that looked nice, which was a useful motivation boost after doing tens of boring exercises in the first two lessons.</p>
<p><a href="../images/drawabox/lesson2/A.jpg" target="_blank">Pictures<img data-bilderrahmen="lesson2"/></a></p>
<h3 id="lesson-3-applying-construction-to-plants">Lesson 3: Applying Construction to Plants</h3>
<p>Lesson 3 starts the second part of the course, where the students apply the learned concepts to different types of things. In all of them, it is encouraged to draw from reference, to concentrate on construction as opposed to imagining the object itself.</p>
<p>This lesson’s topic is plants, which are a nice start, as most of the parts here (namely petals) are still 2D, so the perspective is not as hard.</p>
<p><a href="../images/drawabox/lesson3/A.jpg" target="_blank">Pictures<img data-bilderrahmen="lesson3"/></a></p>
<h3 id="lesson-4-applying-construction-to-insects-and-arachnids">Lesson 4: Applying Construction to Insects and Arachnids</h3>
<p>The next lesson is about insects. One observation I made here is that I tended to draw small pictures which made the line too thick (as it is fixed to 0.5mm with the fineliner).</p>
<p>Apart from trying to make bigger pictures (which made them nicer independently from the thinner lines), I also moved to a 0.4mm fineliner (and 0.1mm one for doing construction lines in lessons 6 &amp; 7).</p>
<p><a href="../images/drawabox/lesson4/A.jpg" target="_blank">Pictures<img data-bilderrahmen="lesson4"/></a></p>
<h3 id="lesson-5-applying-construction-to-animals">Lesson 5: Applying Construction to Animals</h3>
<p>Animals had a surprisingly high good-look to hardness to create ratios, i.e. despite not being comfortable with my skills at all at this point, I have to say some of the pictures are actually not bad :)</p>
<p>Also, I observed that re-doing a given picture makes it easier (and better), but also teaches you less.</p>
<p><a href="../images/drawabox/lesson5/A.jpg" target="_blank">Pictures<img data-bilderrahmen="lesson5"/></a></p>
<h3 id="cylinders-and-wheels">250 cylinders and wheels</h3>
<p>Between lesson 6 and lesson 7, Uncomfortable encourages doing the 250 cylinders challenge. Similarly to the boxes’ ones, I did only a part of them: around 50–100.</p>
<p>One missing thing from the cylinders challenge instructions is that the checking method works only if you drew a perfect cube: otherwise, not all of the lines should converge to the VP. On the other hand, drawing perfect cubes is tricky: there is a guide on the construction in lesson 7, but it’s based on guessing a bit anyway.</p>
<p>Similarly, for the wheels challenge, I only did a small subset of them: they were not perfect, but I doubt I would be able to make them more round with (the boring!) practice.</p>
<h3 id="lesson-6-applying-construction-to-everyday-objects">Lesson 6: Applying Construction to Everyday Objects</h3>
<p>This is where you understand why you drew so many boxes before. On the other hand, drawing the boxes with the required shape/rotation and with a ruler is slightly different than drawing hundreds free-hand. As I’ve written above, I feel like trying to imagine a box with a given rotation and then drawing it free-hand would prepare me better for this.</p>
<p>Anyway, being able to use the ruler was nice: pictures immediately start to look more professional, just because the things converge to VPs, proportions are correct, etc. At the same time, I still tried to draw the final, non-construction lines free-hand: no really sure why, but using the ruler for the final lines felt like cheating.</p>
<p><a href="../images/drawabox/lesson6/A.jpg" target="_blank">Pictures<img data-bilderrahmen="lesson6"/></a></p>
<h3 id="lesson-7-applying-construction-to-vehicles">Lesson 7: Applying Construction to Vehicles</h3>
<p>As the author of the course said, vehicles are the boss fight of the RPG that Drawabox would be. Even though it was allowed to use ballpoint pens (which would make it easier with their thin line width), I stuck to fineliners (with a 0.1mm fineliner for construction lines).</p>
<p>There were a lot of tricky elements, guessing the 3D shape of things, not being clear where to put the horizon line, etc. but I think that after some pictures the patterns started to appear and next pictures seem to be easier/better.</p>
<p>Using an ellipse guide would be super useful, but I lost access to mine due to pandemic.</p>
<p>One construction which I needed but was not presented in the course was how to move “one wheel” deeper from an arbitrary point (so we are at X and want to be at X + 1), which is useful when the first wheel doesn’t start at the equal number of wheels starting from the front of the box. After some trial and error, it was possible to figure this construction out, though.</p>
<p><a href="../images/drawabox/lesson7/A.jpg" target="_blank">Pictures<img data-bilderrahmen="lesson7"/></a></p>
<h2 id="final-words">Final words</h2>
<p>As mentioned in the title, it took me 14 months to finish the course, which required making around 100 pictures. I was doing it mostly during the weekends, at most a couple of hours per week—more after the pandemics started and I got to have a lot of time :).</p>
<p>It was a great (and free!) course, I definitely feel like I grew from 0 to a place where I can start diving into more advanced topics. I plan to give
<a href="https://www.proko.com/">Proko</a> a try next.
<a href="../images/drawabox/lesson1/B.jpg" target="_blank"><img data-bilderrahmen="lesson1"/></a><a href="../images/drawabox/lesson1/C.jpg" target="_blank"><img data-bilderrahmen="lesson1"/></a><a href="../images/drawabox/lesson1/D.jpg" target="_blank"><img data-bilderrahmen="lesson1"/></a><a href="../images/drawabox/lesson1/E.jpg" target="_blank"><img data-bilderrahmen="lesson1"/></a><a href="../images/drawabox/lesson1/F.jpg" target="_blank"><img data-bilderrahmen="lesson1"/></a><a href="../images/drawabox/lesson1/G.jpg" target="_blank"><img data-bilderrahmen="lesson1"/></a><a href="../images/drawabox/lesson1/H.jpg" target="_blank"><img data-bilderrahmen="lesson1"/></a><a href="../images/drawabox/lesson1/I.jpg" target="_blank"><img data-bilderrahmen="lesson1"/></a><a href="../images/drawabox/lesson1/J.jpg" target="_blank"><img data-bilderrahmen="lesson1"/></a><a href="../images/drawabox/lesson1/K.jpg" target="_blank"><img data-bilderrahmen="lesson1"/></a><a href="../images/drawabox/lesson1/L.jpg" target="_blank"><img data-bilderrahmen="lesson1"/></a><a href="../images/drawabox/lesson1/M.jpg" target="_blank"><img data-bilderrahmen="lesson1"/></a><a href="../images/drawabox/lesson1/N.jpg" target="_blank"><img data-bilderrahmen="lesson1"/></a>
<a href="../images/drawabox/lesson2/B.jpg" target="_blank"><img data-bilderrahmen="lesson2"/></a><a href="../images/drawabox/lesson2/C.jpg" target="_blank"><img data-bilderrahmen="lesson2"/></a><a href="../images/drawabox/lesson2/D.jpg" target="_blank"><img data-bilderrahmen="lesson2"/></a><a href="../images/drawabox/lesson2/E.jpg" target="_blank"><img data-bilderrahmen="lesson2"/></a><a href="../images/drawabox/lesson2/F.jpg" target="_blank"><img data-bilderrahmen="lesson2"/></a><a href="../images/drawabox/lesson2/G.jpg" target="_blank"><img data-bilderrahmen="lesson2"/></a><a href="../images/drawabox/lesson2/H.jpg" target="_blank"><img data-bilderrahmen="lesson2"/></a><a href="../images/drawabox/lesson2/I.jpg" target="_blank"><img data-bilderrahmen="lesson2"/></a><a href="../images/drawabox/lesson2/J.jpg" target="_blank"><img data-bilderrahmen="lesson2"/></a><a href="../images/drawabox/lesson2/K.jpg" target="_blank"><img data-bilderrahmen="lesson2"/></a>
<a href="../images/drawabox/lesson3/B.jpg" target="_blank"><img data-bilderrahmen="lesson3"/></a><a href="../images/drawabox/lesson3/C.jpg" target="_blank"><img data-bilderrahmen="lesson3"/></a><a href="../images/drawabox/lesson3/D.jpg" target="_blank"><img data-bilderrahmen="lesson3"/></a><a href="../images/drawabox/lesson3/E.jpg" target="_blank"><img data-bilderrahmen="lesson3"/></a><a href="../images/drawabox/lesson3/F.jpg" target="_blank"><img data-bilderrahmen="lesson3"/></a><a href="../images/drawabox/lesson3/G.jpg" target="_blank"><img data-bilderrahmen="lesson3"/></a><a href="../images/drawabox/lesson3/H.jpg" target="_blank"><img data-bilderrahmen="lesson3"/></a><a href="../images/drawabox/lesson3/I.jpg" target="_blank"><img data-bilderrahmen="lesson3"/></a><a href="../images/drawabox/lesson3/J.jpg" target="_blank"><img data-bilderrahmen="lesson3"/></a><a href="../images/drawabox/lesson3/K.jpg" target="_blank"><img data-bilderrahmen="lesson3"/></a><a href="../images/drawabox/lesson3/L.jpg" target="_blank"><img data-bilderrahmen="lesson3"/></a><a href="../images/drawabox/lesson3/M.jpg" target="_blank"><img data-bilderrahmen="lesson3"/></a>
<a href="../images/drawabox/lesson4/B.jpg" target="_blank"><img data-bilderrahmen="lesson4"/></a><a href="../images/drawabox/lesson4/C.jpg" target="_blank"><img data-bilderrahmen="lesson4"/></a><a href="../images/drawabox/lesson4/D.jpg" target="_blank"><img data-bilderrahmen="lesson4"/></a><a href="../images/drawabox/lesson4/E.jpg" target="_blank"><img data-bilderrahmen="lesson4"/></a><a href="../images/drawabox/lesson4/F.jpg" target="_blank"><img data-bilderrahmen="lesson4"/></a><a href="../images/drawabox/lesson4/G.jpg" target="_blank"><img data-bilderrahmen="lesson4"/></a><a href="../images/drawabox/lesson4/H.jpg" target="_blank"><img data-bilderrahmen="lesson4"/></a><a href="../images/drawabox/lesson4/I.jpg" target="_blank"><img data-bilderrahmen="lesson4"/></a>
<a href="../images/drawabox/lesson5/B.jpg" target="_blank"><img data-bilderrahmen="lesson5"/></a><a href="../images/drawabox/lesson5/C.jpg" target="_blank"><img data-bilderrahmen="lesson5"/></a><a href="../images/drawabox/lesson5/Creal.jpg" target="_blank"><img data-bilderrahmen="lesson5"/></a><a href="../images/drawabox/lesson5/D.jpg" target="_blank"><img data-bilderrahmen="lesson5"/></a><a href="../images/drawabox/lesson5/E.jpg" target="_blank"><img data-bilderrahmen="lesson5"/></a><a href="../images/drawabox/lesson5/F.jpg" target="_blank"><img data-bilderrahmen="lesson5"/></a><a href="../images/drawabox/lesson5/Freal.jpg" target="_blank"><img data-bilderrahmen="lesson5"/></a><a href="../images/drawabox/lesson5/G.jpg" target="_blank"><img data-bilderrahmen="lesson5"/></a><a href="../images/drawabox/lesson5/H.jpg" target="_blank"><img data-bilderrahmen="lesson5"/></a><a href="../images/drawabox/lesson5/I.jpg" target="_blank"><img data-bilderrahmen="lesson5"/></a><a href="../images/drawabox/lesson5/J.jpg" target="_blank"><img data-bilderrahmen="lesson5"/></a><a href="../images/drawabox/lesson5/Jreal.jpg" target="_blank"><img data-bilderrahmen="lesson5"/></a><a href="../images/drawabox/lesson5/K.jpg" target="_blank"><img data-bilderrahmen="lesson5"/></a><a href="../images/drawabox/lesson5/L.jpg" target="_blank"><img data-bilderrahmen="lesson5"/></a><a href="../images/drawabox/lesson5/Lreal.jpg" target="_blank"><img data-bilderrahmen="lesson5"/></a><a href="../images/drawabox/lesson5/M.jpg" target="_blank"><img data-bilderrahmen="lesson5"/></a><a href="../images/drawabox/lesson5/Mreal2.jpg" target="_blank"><img data-bilderrahmen="lesson5"/></a><a href="../images/drawabox/lesson5/Mreal.jpg" target="_blank"><img data-bilderrahmen="lesson5"/></a><a href="../images/drawabox/lesson5/N.jpg" target="_blank"><img data-bilderrahmen="lesson5"/></a><a href="../images/drawabox/lesson5/Nreal.jpg" target="_blank"><img data-bilderrahmen="lesson5"/></a><a href="../images/drawabox/lesson5/O.jpg" target="_blank"><img data-bilderrahmen="lesson5"/></a><a href="../images/drawabox/lesson5/Oreal.jpg" target="_blank"><img data-bilderrahmen="lesson5"/></a><a href="../images/drawabox/lesson5/P.jpg" target="_blank"><img data-bilderrahmen="lesson5"/></a><a href="../images/drawabox/lesson5/Preal.jpg" target="_blank"><img data-bilderrahmen="lesson5"/></a>
<a href="../images/drawabox/lesson6/B.jpg" target="_blank"><img data-bilderrahmen="lesson6"/></a><a href="../images/drawabox/lesson6/C.jpg" target="_blank"><img data-bilderrahmen="lesson6"/></a><a href="../images/drawabox/lesson6/D.jpg" target="_blank"><img data-bilderrahmen="lesson6"/></a><a href="../images/drawabox/lesson6/E.jpg" target="_blank"><img data-bilderrahmen="lesson6"/></a><a href="../images/drawabox/lesson6/F.jpg" target="_blank"><img data-bilderrahmen="lesson6"/></a><a href="../images/drawabox/lesson6/G.jpg" target="_blank"><img data-bilderrahmen="lesson6"/></a><a href="../images/drawabox/lesson6/H.jpg" target="_blank"><img data-bilderrahmen="lesson6"/></a><a href="../images/drawabox/lesson6/I.jpg" target="_blank"><img data-bilderrahmen="lesson6"/></a>
<a href="../images/drawabox/lesson7/Areal.jpg" target="_blank"><img data-bilderrahmen="lesson7"/></a><a href="../images/drawabox/lesson7/Areal2.jpg" target="_blank"><img data-bilderrahmen="lesson7"/></a><a href="../images/drawabox/lesson7/Areal3.jpg" target="_blank"><img data-bilderrahmen="lesson7"/></a><a href="../images/drawabox/lesson7/B.jpg" target="_blank"><img data-bilderrahmen="lesson7"/></a><a href="../images/drawabox/lesson7/Breal.jpg" target="_blank"><img data-bilderrahmen="lesson7"/></a><a href="../images/drawabox/lesson7/Breal2.jpg" target="_blank"><img data-bilderrahmen="lesson7"/></a><a href="../images/drawabox/lesson7/C.jpg" target="_blank"><img data-bilderrahmen="lesson7"/></a><a href="../images/drawabox/lesson7/Creal.jpg" target="_blank"><img data-bilderrahmen="lesson7"/></a><a href="../images/drawabox/lesson7/D.jpg" target="_blank"><img data-bilderrahmen="lesson7"/></a><a href="../images/drawabox/lesson7/Dreal.jpg" target="_blank"><img data-bilderrahmen="lesson7"/></a><a href="../images/drawabox/lesson7/E.jpg" target="_blank"><img data-bilderrahmen="lesson7"/></a><a href="../images/drawabox/lesson7/Ereal.jpg" target="_blank"><img data-bilderrahmen="lesson7"/></a><a href="../images/drawabox/lesson7/F.jpg" target="_blank"><img data-bilderrahmen="lesson7"/></a><a href="../images/drawabox/lesson7/G.jpg" target="_blank"><img data-bilderrahmen="lesson7"/></a><a href="../images/drawabox/lesson7/H.jpg" target="_blank"><img data-bilderrahmen="lesson7"/></a><a href="../images/drawabox/lesson7/I.jpg" target="_blank"><img data-bilderrahmen="lesson7"/></a><a href="../images/drawabox/lesson7/Ireal.jpg" target="_blank"><img data-bilderrahmen="lesson7"/></a><a href="../images/drawabox/lesson7/J.jpg" target="_blank"><img data-bilderrahmen="lesson7"/></a><a href="../images/drawabox/lesson7/Jreal.jpg" target="_blank"><img data-bilderrahmen="lesson7"/></a><a href="../images/drawabox/lesson7/K.jpg" target="_blank"><img data-bilderrahmen="lesson7"/></a><a href="../images/drawabox/lesson7/L.jpg" target="_blank"><img data-bilderrahmen="lesson7"/></a><a href="../images/drawabox/lesson7/Lreal.jpg" target="_blank"><img data-bilderrahmen="lesson7"/></a><a href="../images/drawabox/lesson7/M.jpg" target="_blank"><img data-bilderrahmen="lesson7"/></a><a href="../images/drawabox/lesson7/Mreal.jpg" target="_blank"><img data-bilderrahmen="lesson7"/></a></p>
]]></description>
    <pubDate>Sat, 11 Jul 2020 00:00:00 UT</pubDate>
    <guid>https://sygnowski.ml/posts/2020-07-11-drawabox.html</guid>
    <dc:creator>Jakub Sygnowski</dc:creator>
</item>
<item>
    <title>Digital version of T.I.M.E stories</title>
    <link>https://sygnowski.ml/posts/2020-06-19-time-stories.html</link>
    <description><![CDATA[<div class="info">
    Posted on June 19, 2020
    
</div>
<h1>Digital version of T.I.M.E stories</h1>
<p><a href="https://boardgamegeek.com/boardgame/146508/time-stories">T.I.M.E stories</a> is a cooperative board game where the players take the role of agents, who remotely control the avatars which are put in some turbulent moment of history to prevent spacetime disintegration due to events not going according to the historical record. Together with my girlfriend: <a href="https://micha7a.github.io">Michalina Pacholska</a> we decided to make a digital version of it.</p>
<h2 id="mechanics">Mechanics</h2>
<p>Despite being a <em>board</em> game, T.I.M.E stories is mostly a decksploration game: each mission has an associated deck of 100–200 cards which define the game world to be explored by the players.</p>
<p>The deck consists of <em>location</em> cards, which describe the venue of the game, hero cards depicting avatars which players can impersonate during a mission, <em>items</em> which can be used by avatars, and others.</p>
<p>The location cards are grouped into bigger locations (like “windmill” or “the forest”). Agents explore a location by each choosing one card to see and following its instructions. On the cards there are NPCs to talk to, challenges to perform and items to collect.</p>
<p>An effect of visiting a given location card can be unlocking a new location, introduce some scenario-specific mechanic (eg. you can choose a random card and look at it for 5s or make it easier to convince an NPC to do something) or spending some precious time talking to an NPC which does not have any clue for you whatsoever.</p>
<p>Each agent has a unique set of traits that are described on its player card and collects the items separately (but the players can exchange items when they are on the same location card).</p>
<p>The change in the state of the world is implemented through receiving <em>state tokens</em>, which block some location cards or dialogue options (eg. you can’t ask an NPC about his brother if you don’t know him yet). The game tracks resources using a number of regular <em>tokens</em>.</p>
<p>The challenges with NPCs are resolved using a dice-based combat system, and the passage of time is measured in <em>Time Units</em>, passing whenever agents do actions or change locations. If the time allocated for a given <em>run</em> of the scenario passes, the run is considered failed and the agents need to start from scratch (but armed with the gained knowledge about the world).</p>
<h2 id="moving-digital">Moving digital</h2>
<p>We really like playing the game, but, being far from our usual gaming groups, we didn’t expect to be able to play it in real life in the foreseeable future (not to mention the raging pandemic). Because of this, we decided to build a digital version of the game, to be played in the browser.</p>
<p>At the beginning of this project, we had a rudimentary experience in web development, so the project was mostly a learning journey for us. Because of that, we have made many questionable engineering decisions along the way (and many we don’t know yet that are questionable), mostly trying to deliver the features without trying too hard to do everything <em>right</em> on the first try.</p>
<h2 id="assumptions">Assumptions</h2>
<p>We have made the following design assumptions:</p>
<ol type="1">
<li>We should be able to play a scenario without a need for a human to look over the cards. The scenario cards are available in non-structured formats (more on that later). Without looking at the cards we are not e.g., able to split cards into items and locations, but we also don’t spoil the game for the person preparing the scenario.</li>
<li>We want the app to be a game simulator, ie. we assume that players will enforce the game rules (subtracting TUs, having access to a subset of locations, etc.) themselves. The scenarios often provide various innovative deviations from the basic rules, e.g. vehicles making it cheaper to move locations, resetting state tokens mid-scenario, a future-teller talisman allowing to see a random card in the deck, etc. We decided we want to give the players the final control over following the rules.</li>
<li>We will not distinguish between different user connections to the server. In other words, everyone will be able to do the same actions, see the same cards, etc. As in the physical game, the users will choose the color of their pawn (which is used to mark the location card the agent is in), but nothing will prevent the users from changing it mid-game, peeking at other players’ cards, etc.</li>
</ol>
<h2 id="preparing-scenarios">Preparing scenarios</h2>
<p>The original game has many extensions to buy, containing different scenarios. There are also dozens of fan-made scenarios that are available to download for free (a list of them can be found <a href="https://boardgamegeek.com/thread/1533236/overview-scenarios-authors-countries">here</a>). In our app, we will be using the fan-made scenarios, which are available in two formats:</p>
<ol type="1">
<li>as a pdf with 4 cards/page (ready to print)</li>
<li>as a set of png files that contain a sheet of around 5x6 cards, either front or back.</li>
</ol>
<p>In either of these, each card has a small number at the bottom, to be able to sort the cards and match fronts with backs.</p>
<figure>
<img src="../images/ts/card.png" style="width:50.0%" alt="Back of an item card. Note the small number 20 in bottom-left corner." />
<figcaption aria-hidden="true">Back of an item card. Note the small number 20 in bottom-left corner.</figcaption>
</figure>
<p>To be able to use a scenario, we need to divide the cards into separate files. Ideally, the filename/directory structure would describe which card it is (eg. location X), but that is not possible without looking at the cards, so we decided to store the files under <code>scenario/card_number</code> with the file names <code>(front|back).webp</code>. We used webp as an image format because it offered great compression rates (and is supported by nearly all browsers).</p>
<p>We made two separate tools for cutting pdf- and png-based scenarios.</p>
<h3 id="pdf-based">Pdf-based</h3>
<p>Most pdf-based scenarios followed the same ordering of cards withing the scenario, so we were able to hard-code. A bigger problem here were different sizes of margins in different files.</p>
<figure>
<img src="../images/ts/page.png" style="width:90.0%" alt="An example page to be cut into single cards." />
<figcaption aria-hidden="true">An example page to be cut into single cards.</figcaption>
</figure>
<p>We went for the human-in-the-loop approach for detecting the size of margins, where the user was choosing an appropriate margin for the first page of the scenario and we assumed that the margins don’t change within one file.</p>
<p>To make it convenient for the user to do so, the console-based tool was increasing a margin by a fixed amount when the user pressed <code>+</code> and decreasing it when they pressed <code>-</code>. Whenever the user changed the direction of change (i.e. going from a margin that’s too big to a one that’s too small), we decreased the amount of change, allowing the user to quickly get to a margin that’s nearly-perfect and finetuning it.</p>
<p>For example, if the user pressed <code>---+-</code>, the first three signals would decrease margin by 16px each, the next one will increase it by 8px, and the next one will decrease it by 4px.</p>
<p>We observed that while pdf files opened in Chrome don’t refresh on change, the Evince viewer does refresh them, which made it more convenient to change margins without changing window focus.</p>
<p>We used PyPDF2, pdf2image, and curses to make this tool.</p>
<h3 id="png-based">Png-based</h3>
<p>When cutting the sheets of card images, there were no problems with margins (there are none), but instead, with the order of cards. Here, different scenarios had both different orderings: either first cards in each sheet are numbered 1, 2, 3, …, or consecutive cards in the first sheet and sometimes the order of a couple of cards was just mixed up without any (apparent) structure.</p>
<p>Initially, I decided to try out cutting the small part of the image where the card number was present and use an off-the-shelf OCR tool to read it. I used an apparently popular OCR tool: tesseract.</p>
<p>I suspected it will work out-of-the-box, but couldn’t be more wrong. I ended up:
a) converting the image to black-and-white and increasing contrast to make it easier for tesseract to detect the numbers
b) providing <a href="https://tesseract-ocr.github.io/tessdoc/Data-Files.html">separate data files</a> to tesseract, specially trained for mathematics, as I suspected the language bias would make it harder to decipher single numbers
c) conditioned tesseract to detect only digits.</p>
<p>Even with these changes, I only managed to detect around half of the card numbers in every second scenario.</p>
<p>Obviously, this wasn’t good enough, so I again resorted to using human assistance. Having dissected the card numbers from the cards, I found it acceptable to show them to the user for detection, as they would not be spoilery.</p>
<p>Providing the readings for all the missing numbers would be a lot of work, so instead, we used the following algorithm:</p>
<ol type="1">
<li>We try to predict the next number to be used using OCR (<code>OCR_t</code>).</li>
<li>We predict the next number to be used as a linear extrapolation from previous two number: <code>diff_t := x_{t-1} - x_{t-2}</code>. <code>pred_t := x_{t-1} + diff_t</code>.</li>
<li>If the two predictions match (<code>pred_t = OCR_t</code>), we use it without asking the user: <code>x_t := OCR_t</code>.</li>
<li>If not, we ask the user who can either accept the second prediction or provide the input on their own.</li>
</ol>
<p>Note that the second prediction would work for both orderings of the cards (assuming there are no errors in the ordering). In practice, as the number of places where the numbers would be mixed up was relatively small (say, &lt;5% numbers), it worked quite reliably, with the user just confirming that the predicted number is correct most of the time, and providing the actual number in a small number of cases only.</p>
<h2 id="choosing-a-framework">Choosing a framework</h2>
<p>We spent some time choosing which web development framework to use. Initially, we considered going for a minimal version in django where one person streams the “table” with all the cards and players just click appropriate links on their devices.</p>
<p>We decided to go for a more challenge (and learning) and go for a <a href="https://en.wikipedia.org/wiki/Single-page_application">single-page application</a> where the app would serve the game board to all players. After looking over the list of SPA-frameworks, we decided to give (Typescript-based) <a href="https://en.wikipedia.org/wiki/Angular_(web_framework)">Angular</a> and <a href="https://socket.io">socket.io</a> a go.</p>
<p>Only after hitting some first import errors and <a href="https://stackoverflow.com/questions/61192037/typescript-import-errors">asking for help on StackOverflow</a>, we realized that Angular itself is client-side only, and we need to have a separate server to be able to communicate between clients.</p>
<p>After taking another look at available options, we decided to go for <a href="https://en.wikipedia.org/wiki/Meteor_(web_framework)">MeteorJS</a>, which proved to make it easy to synchronize the common state. For the frontend, we still used Angular and helped ourselves with <a href="https://github.com/Urigo/meteor-rxjs">meteor-rxjs</a> to listen to changes in the data in a reactive way.</p>
<h2 id="the-database">The database</h2>
<p>Meteor uses a MongoDB database and a convenient interface to it in the form of <a href="https://guide.meteor.com/collections.html">Collections</a>. In our case, the main part of the data lived in two collections: Cards and Holders with basically the following schemas:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode javascript"><code class="sourceCode javascript"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>CardsSchema <span class="op">=</span> <span class="kw">new</span> <span class="fu">SimpleSchema</span>({</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">number</span><span class="op">:</span>  SimpleSchema<span class="op">.</span><span class="at">Integer</span><span class="op">,</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">holderId</span><span class="op">:</span> <span class="bu">String</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>})<span class="op">;</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>HoldersSchema <span class="op">=</span> <span class="kw">new</span> <span class="fu">SimpleSchema</span>({</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>  <span class="dt">name</span><span class="op">:</span> <span class="bu">String</span><span class="op">,</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>  <span class="dt">cards</span><span class="op">:</span> { <span class="dt">type</span><span class="op">:</span> <span class="bu">Array</span><span class="op">,</span> <span class="dt">defaultValue</span><span class="op">:</span> [] }<span class="op">,</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>  <span class="st">&quot;cards.$&quot;</span><span class="op">:</span> <span class="bu">String</span><span class="op">,</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>  <span class="dt">currentCardIndex</span><span class="op">:</span> {<span class="dt">type</span><span class="op">:</span> SimpleSchema<span class="op">.</span><span class="at">Integer</span><span class="op">,</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>                     <span class="dt">optional</span><span class="op">:</span> <span class="kw">true</span>}<span class="op">,</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>  <span class="dt">currentSide</span><span class="op">:</span> {<span class="dt">type</span><span class="op">:</span> <span class="bu">String</span><span class="op">,</span> <span class="dt">allowedValues</span><span class="op">:</span> [<span class="st">&quot;front&quot;</span><span class="op">,</span> <span class="st">&quot;back&quot;</span>]<span class="op">,</span> <span class="dt">optional</span><span class="op">:</span> <span class="kw">true</span>}</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>})<span class="op">;</span></span></code></pre></div>
<p>We assume the card number and side is enough info to find the URL of the card. In addition to the basic schema, we implemented a number of hooks when inserting or updating the data:</p>
<ol type="1">
<li>When inserting a new card or updating holderId, we insert/remove the id of the card to the appropriate holders.</li>
<li>After changing the set of cards in a given holder, we sort the cards by their numbers. When the cards are not sorted, finding a given card may be tedious in the deck of &gt;100 cards. At the same time, as the scenario creators expected the deck to be initially sorted, the cards with consecutive numbers are semantically connected (e.g. cards of a given location).</li>
<li>When we add a new card to a given holder, after sorting the cards, we make sure that the new card ends up on top of the deck. Otherwise, it was easy to move a card to a holder by mistake and lose track of it within tens of other cards.</li>
</ol>
<h3 id="the-order-of-insertion">The order of insertion</h3>
<p>At the start of the game, we needed to insert all cards to a deck holder. We found all card numbers by listing a given directory, and then needed to add them one by one to the database (as the <a href="https://docs.meteor.com/api/collections.html#Mongo-Collection-insert">insert</a> is only able to add one document to the database).</p>
<p>As the mentioned-above hooks are not atomic (for example to sort cards we need to first retrieve them from the database, sort them and insert them back), we need to make sure that the insertions to the collections happen in a synchronous way to prevent race conditions.</p>
<p>One way to do so would be to use the default mechanism for insertion for collections, which is synchronous and wait for the results. This would not allow us to reactively observe changes using meteor-rxjs (as it uses its own, asynchronous wrapper over insert). This would not be a big problem if we were to initialize the database once, but we also wanted to have a button that would allow us to change the scenario (and reinitialize the database) while the app is running.</p>
<p>In theory, this should be possible to implement using <a href="https://rxjs-dev.firebaseapp.com/api/index/function/concat">rxjs’ concat</a>, which takes a number of Observables and subscribes to them one at a time, moving to the next one whenever the previous one finishes. The problem with that solution is that meteor-rxjs’ insert <a href="https://github.com/Urigo/meteor-rxjs/blob/master/src/ObservableCollection.ts#L127">doesn’t wait for the subscription</a> before inserting, so creating all the Observables for inserting all of the cards will already start calling the underlying inserts and their hooks, causing race conditions before we even start subscribing to any of them.</p>
<p>The solution we went for involved active waiting for when the subscription for insertion of card <code>N</code> is called before inserting the <code>N + 1</code>-th card. However, the simple:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode javascript"><code class="sourceCode javascript"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">let</span> inserted <span class="op">=</span> allCards<span class="op">.</span><span class="fu">map</span>(c <span class="kw">=&gt;</span> <span class="kw">false</span>)<span class="op">;</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(<span class="op">;</span> i <span class="op">&lt;</span> allCards<span class="op">.</span><span class="at">length</span><span class="op">;</span>) {</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (<span class="op">!</span>inserted[i]) {</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    inserted[i] <span class="op">=</span> <span class="kw">true</span><span class="op">;</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    Cards<span class="op">.</span><span class="fu">insert</span>(allCards[i])<span class="op">.</span><span class="fu">subscribe</span>(() <span class="kw">=&gt;</span> { i<span class="op">++;</span> })<span class="op">;</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>did not work. As Meteor methods are running in <a href="https://docs.meteor.com/api/methods.html#DDPCommon-MethodInvocation-unblock">one fiber per client</a>, running the code above will result in an endless loop which will never transfer execution context to the insert method.</p>
<p>Adding a simple <code>await sleep(10)</code>, solved the issue, as the execution could go to inserting the appropriate card and returning whenever insertion finished.</p>
<h2 id="pulling-cards">Pulling cards</h2>
<p>In the beginning, we implemented moving cards between holders as clicking one holder and then the other. It wasn’t very intuitive, so we decided to move them using drag&amp;drop.</p>
<p>We opted for Angular Material’s <a href="https://material.angular.io/cdk/drag-drop/overview">Drag and Drop</a> module. While the terminology suggests it’s made to move <em>items</em> between <em>lists</em>, we find it fine to consider a single card a <code>cdkDrag</code> and one holder to be a single-element <code>cdkDropList</code>.</p>
<figure>
<img src="../images/ts/moving-card.gif" style="width:100.0%" alt="Moving card from one holder to another." />
<figcaption aria-hidden="true">Moving card from one holder to another.</figcaption>
</figure>
<h3 id="rotating-cards">Rotating cards</h3>
<p>While most of the cards are oriented vertically, some of them present horizontally best. One popular example is the cards of the map, which show different locations available to the agents.</p>
<p>One problem which appears when starting to drag and drop cards is the behavior of placeholders which are shown in the to-be-dropped-into holders when the source and sink holder are of different orientation. The default behavior with the Drag and Drop module is to make the placeholder to be the same as the element being dragged. In our case that meant trying to show a vertical placeholder in the horizontal holder and vice versa, which looked wrong.</p>
<figure>
<img src="../images/ts/rotated-cards-before.gif" style="width:40.0%" alt="Moving rotated cards: before." />
<img src="../images/ts/rotated-cards-after.gif" style="width:39.0%" alt="Moving rotated cards: after." />
<figcaption>
Moving cards to holders with a different orientation: before and after.
</figcaption>
</figure>
<p>We fixed it with <a href="https://angular.io/api/core/Renderer2">Renderer2</a>, which is able to read and change the CSS class of any html element. We called it when processing the <a href="https://material.angular.io/cdk/drag-drop/api#CdkDropList">cdkDropListEntered</a> event, which is called whenever we hover an item above a container.</p>
<h2 id="rolling-dice">Rolling dice</h2>
<p>One part of the mechanics of the game is the use of dice to:</p>
<ol type="1">
<li>Measure the passage of time which the agents have left</li>
<li>Resolve the results of the challenges.</li>
</ol>
<p>In the second case, the players throw a number of dice equal to the value of their statistic (like strength or eloquence) and count the number of “hits” which help the player succeed in the test and “skulls” which hurt the player.</p>
<p>Initially, we just made a panel where the player chose the number of players and received the total results, but while playtesting one of the scenarios, we stumbled upon a practice of re-rolling a die (e.g. one character was allowed to choose one of the dice after a throw and roll it once more, hoping for a better result). It wasn’t possible to do with our dice, as the player didn’t get the division of the total result into particular dice (was this score of 4 a 2 + 2 + 0 + 0 or 1 + 1 + 1 + 1?). Because of that, we decided to show separate dice with results (apart from the total) and allowed the user to click on any die to reroll it.</p>
<p>However, this wasn’t working perfectly from the UI perspective; the die could get the same result as the previous one after the reroll, and if this happened, the user doesn’t get any feedback that the system indeed registered the reroll. Because of that, we added an animation based on <a href="https://material.io/develop/web/components/animation/">material</a> where the previous die fades away and the new one slowly appears, so that the users see the change.</p>
<figure>
<img src="../images/ts/dice.gif" style="width:70.0%" alt="Animation when rerolling challenge dice." />
<figcaption aria-hidden="true">Animation when rerolling challenge dice.</figcaption>
</figure>
<p>Another problem with the interface was that every time the agents leave a location one of the players should throw the Time Unit die to spend TU. In the physical game, everyone sees whether the die has been thrown and the score subtracted from the tracker. In the digital version, players don’t see whether others have thrown the die if they don’t track the available TU closely.</p>
<p>To make sure people know whether the die has been thrown already, we send every player a notification once anyone throws the TU die.</p>
<h2 id="player-pawns">Player pawns</h2>
<p>In the physical version of the game, every round every player puts their pawn above the location card they want to explore. We made something similar, where each players’ pawn is placed above the card they chose for the given round. One problem, which exists for either version of the game, is spending Time Units every round. As each player performs their actions somewhat independently of others, sometimes it’s hard to make sure every player has done the same number of rounds, especially when some of the players wait for others to finish their challenges.</p>
<p>To make it easier to keep of track the number of rounds each player did, we added a small mark that we put next to a player token whenever that player moves to another location or throws a die (each of which constitutes an action).</p>
<figure>
<img src="../images/ts/pawn-mark.png" alt="The mark on the pawn after the player did the action." />
<figcaption aria-hidden="true">The mark on the pawn after the player did the action.</figcaption>
</figure>
<p>One facilitation we tried introducing to the gameplay was to automate the throwing of the TU die whenever players leave a location. We thought that leaving the location can be recorded whenever the first location card is removed, and we could throw the die without their action.</p>
<p>This had one problem though, as the fact of cards moving is independently observed by each of the players, so when we threw a die every time someone observed removal of the first card, the die was thrown 4 times. I didn’t want to send the request only from a player with a given color (e.g. only blue player), as there may be two (or none) players impersonating a blue player at a given moment. I tried to have a hacky version of the server refusing to accept the throw-die request for a short time after another player requested it, but as each player’s call is running a separate thread, it wasn’t reliable and we decided to abandon this feature.</p>
<h2 id="service">Service</h2>
<p>We decided to serve the application using Amazon Web Services, taking advantage of its global free tier, allowing us to use small amounts (way bigger than our needs) of compute and storage from a central-European cluster, which was useful for us as we didn’t want to spend time trying to speed up the request time/cache the requests.</p>
<h3 id="meteor-up">Meteor up</h3>
<p>To easily set the server up on the micro EC2 instance, we used <a href="http://meteor-up.com">Meteor Up</a>. I was surprised how plug-and-play the tool was: it checks the connection to the server, bundles the whole meteor code, node, nginx into a docker container, sends it to the server, and turns it on.</p>
<p>There even was an option to bundle together the mongoDB database, but we went for setting it up ourselves so that we could make changes to it that would preserve new deploys of the meteor code.</p>
<h3 id="storage">Storage</h3>
<p>For storing and serving the files we went for a standard S3 solution. One scenario is around 300 files and 30MB altogether, so the free tier allowance of 5GB of data was more than enough. The bigger problem was the number of requests: 20k GET requests and 2k PUT/COPY/LIST requests per month.</p>
<p>As during the scenario each client requests every card multiple times, I was worried we will get the limit on the number of GET requests, but after setting up <a href="https://docs.meteor.com/packages/appcache.html">appcache</a> as well as the <a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cache-Control">Cache-Control</a> headers, the number of GET requests per (4h-long) game run was 2k or 3k.</p>
<p>The bigger problem turn out to be the PUT/COPY/LIST requests. I didn’t expect problems here, even though each file needed to be inserted separately, as we need to insert each scenario only once, so there could be around 6 scenarios we could insert per month, and we only do one LIST request per run where we initially create a database.</p>
<p>However, when trying the app out, I observed a bigger number of requests than the number of files suggested. I tried using <a href="https://aws.amazon.com/cloudtrail/">AWS CloudTrial</a> as well as <a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerLogs.html">server access logging</a>, which are two tools for aggregating logs to show types and origins of the requests to S3 buckets, but didn’t find anything suspicious there, apart from the logs about inserting the logs themselves.</p>
<h3 id="dns-ssl">DNS, SSL</h3>
<p>I already had a free website domain claimed from <a href="https://freenom.com">freenom.com</a> and a DNS server set up at <a href="https://freedns.afraid.org/">freeDNS</a>, and I only wanted to set up another subdomain there.</p>
<p>Initially, I considered getting an <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html">Elastic IP</a> address, which would allow me to attach the fixed IP to the different EC2 instances, which I was planning to take up and down; mostly because I didn’t want to waste the resources it is running on, as I was only planning to occasionally play with friends.</p>
<p>As the Elastic IPs are only free while the instance they are attached to is running, instead of using them I decided to go for the dynamic IP on the EC2 and used dynamic DNS service on freedns, which allows to set up a cron job on EC2 to request a change of the IP to the current address whenever the instance is up.</p>
<h2 id="final-notes">Final notes</h2>
<p>This project was an experiment in web development, site hosting, and writing the blog post. Thanks for everyone who playtested the various versions with us.</p>
<p>The final state of the project can be seen here:
<a href="../images/ts/all.png"><img src="../images/ts/all.png" style="width:100.0%" alt="Full view of the final app." /></a></p>
]]></description>
    <pubDate>Fri, 19 Jun 2020 00:00:00 UT</pubDate>
    <guid>https://sygnowski.ml/posts/2020-06-19-time-stories.html</guid>
    <dc:creator>Jakub Sygnowski</dc:creator>
</item>
<item>
    <title>--init</title>
    <link>https://sygnowski.ml/posts/2016-03-24-init.html</link>
    <description><![CDATA[<div class="info">
    Posted on March 24, 2016
    
</div>
<h1>--init</h1>
<p>In this post I am just testing various features of the blog. There is nothing interesting here.</p>
<h2 id="syntax-highlighting">Syntax highlighting:</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> <span class="kw">lambda</span> x: <span class="bu">map</span>(y) <span class="op">+</span> <span class="dv">7</span></span></code></pre></div>
<h2 id="mathjax">MathJax</h2>
<p><span class="math inline">\(\frac{e^\pi}{2a}\)</span></p>
<h3 id="some-random-spams">Some random spams</h3>
<p>Vestibulum leo turpis, dignissim quis ultrices sit amet, iaculis ac ligula.
Pellentesque tristique, velit eget scelerisque scelerisque, est dolor ultrices
arcu, quis ullamcorper justo :smile: arcu luctus mauris. Integer congue molestie nisi id
posuere. Fusce pellentesque gravida tempus. Integer viverra tortor nec eros
mollis quis convallis sem laoreet. <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> Nulla id libero ac erat varius laoreet. Proin</p>
<details>
<summary>
See more content
</summary>
sed est est. Curabitur lacinia fermentum lorem, elementum malesuada ipsum
malesuada ut. Donec suscipit elit:x: id leo vehicula mattis non sed leo. Morbi
varius eleifend varius. Nulla vestibulum, neque vitae aliquam eleifend, nisi
tellus placerat nunc, quis suscipit elit turpis eu tortor. Etiam euismod
convallis lectus quis venenatis. Phasellus laoreet magna in nibh cursus eu
egestas nulla convallis. Aliquam vel ullamcorper risus. Fusce dictum, massa id
consequat viverra, nulla ante tristique est, a faucibus nisi enim nec dui. Donec
metus ligula, condimentum at porttitor eget, lobortis at quam.
</details>
<p>Aenean vel libero in magna ultricies congue in a odio. Donec faucibus rutrum
ornare. Fusce dictbum eleifend fermentum. Vestibulum vel nibh a metus porttitor
rhoncus. Pellentesque id quam neque, eget molestie arcu. Integer in elit vel
neque viverra ultricies in eget massa. Nam ut convallis est. Pellentesque eros
eros, sodales non vehicula et, tincidunt ut odio. Cras suscipit ultrices metus
sit amet molestie. Fusce enim leo, vehicula sed sodales quis, adipiscing at
ipsum.</p>
<details>
<summary>
Some code
</summary>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>  res <span class="op">=</span> process(argv)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> res</span></code></pre></div>
</details>
<p>Nulla adipiscing ultricies lobortis. Vivamus iaculis nisl vitae tellus laoreet
vitae aliquet lacus mollis. Phasellus ut lacus urna, sed sagittis ante. Etiam
consectetur pretium nisl sed dignissim. Pellentesque convallis, nisl eget
commodo mollis, sem magna consequat arcu, sed pretium ipsum arcu sit amet neque.
Aliquam erat volutpat. Morbi sed mi sed urna vestibulum placerat vitae vel
metus. Fusce ac ante at justo pharetra vehicula. Vivamus vel tortor eget augue
aliquet aliquet at vel odio. Nunc venenatis, magna quis facilisis fringilla,
augue tellus varius neque, in vulputate est eros ut tortor. Duis lorem neque,
aliquam congue posuere id, condimentum non dui. Phasellus ut dui massa,
porttitor suscipit augue. Praesent quis tellus quam, vel volutpat metus. Vivamus
enim est, aliquam in imperdiet et, sagittis eu ligula. Vestibulum hendrerit
placerat orci et aliquet. Cras pharetra, dolor placerat lobortis tempor, metus
odio cursus ligula, et posuere lacus ligula quis dui.</p>
<p>Donec a lectus eu nibh malesuada aliquam. Proin at metus quam, et tincidunt leo.
Quisque lacus justo, scelerisque sodales pulvinar sed, dignissim ut sapien.
Vivamus diam felis, adipiscing sollicitudin ultricies id, accumsan ac felis. In
eu posuere ligula. Suspendisse potenti. Donec porttitor dictum dui id vehicula.
Integer ante velit, congue id dictum et, adipiscing a tortor.</p>
<h2 id="page-links">Page links:</h2>
<p><a href="http://github.com/sygi/some-link">here</a>
<a href="#mathjax">local</a></p>
<section class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>Some footnote<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
]]></description>
    <pubDate>Thu, 24 Mar 2016 00:00:00 UT</pubDate>
    <guid>https://sygnowski.ml/posts/2016-03-24-init.html</guid>
    <dc:creator>Jakub Sygnowski</dc:creator>
</item>

    </channel>
</rss>
